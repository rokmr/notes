{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#notes","title":"Notes","text":""},{"location":"#explore-and-star-on-github","title":"Explore and star on GitHub","text":"<ul> <li> <p> rokmr/kick-start-ai</p> </li> <li> <p> rokmr/machine-learning</p> </li> <li> <p> rokmr/computer-vision</p> </li> <li> <p> rokmr/notes</p> </li> </ul>"},{"location":"basics/","title":"Basics","text":""},{"location":"cv/","title":"Computer Vision","text":""},{"location":"cv/#overview","title":"Overview","text":""},{"location":"cv/#core","title":"Core","text":"<ul> <li> Basics</li> <li> Image Operations</li> <li> Image Annotations</li> <li> Image Enhancement</li> <li> Image Filtering</li> <li> Image Feature</li> <li> Image Alignment</li> <li> Panorama</li> <li> HDR</li> <li> Image Classification</li> <li> Image Segmentation</li> <li> Edge Detection &amp; Contours</li> <li> Object Detection</li> <li> Face Detection</li> <li> Optical Flow</li> <li> Object Tracking</li> <li> Pose Estimation</li> <li> OCR</li> </ul>"},{"location":"cv/#advance","title":"Advance","text":"<ul> <li> AutoEncoder</li> <li> ViT</li> <li> NeRF</li> </ul>"},{"location":"cv/#generative","title":"Generative","text":"<ul> <li> Variational Encoder</li> </ul>"},{"location":"cv/detection/","title":"Object Detection","text":""},{"location":"cv/detection/#object-detection","title":"Object Detection","text":""},{"location":"cv/detection/#introduction","title":"Introduction","text":"<p>The goal of object detection is to predict a set of bounding boxes(x,y,w,h) and category labels for each object of interest.</p>"},{"location":"cv/detection/#traditional","title":"Traditional","text":""},{"location":"cv/detection/#template-matching-sliding-window","title":"Template Matching + Sliding Window","text":"<p>For every position you evaluate how much do the pixels in the image and template correlate.</p> <p>Cons</p> <ol> <li>Does not handle occlusions.</li> <li>Works with instance of object but not with class of it.</li> <li>Does not work if pose changes.</li> <li>Does not work if position, scale and aspect ratio changes.</li> </ol>"},{"location":"cv/detection/#feature-extraction-and-classification","title":"Feature Extraction and Classification","text":"<p>Learn multiple weak classifier to build a strong final decision.</p>"},{"location":"cv/detection/#feature-extraction","title":"Feature Extraction","text":"<p>Viola-Jones Detector</p> <p>Haar Features</p> <p>Histogram of Oriented Gradients(HOGs) Compute gradients in dense grids, compute gradients and create a histogram based on gradient direction</p> <p>Deformable Part Model (DPM) Based on HOG features but based on body part detection. More robust to different body poses.</p>"},{"location":"cv/detection/#classification","title":"Classification","text":"<p>It is done with the help of SVM.</p>"},{"location":"cv/detection/#general-object-detection","title":"General Object Detection","text":"<ul> <li>Class agnostic</li> <li>Object Proposals / Region of Intrest<ul> <li>Selective search</li> <li>Edge boxes</li> </ul> </li> </ul> <p>Localization</p>"},{"location":"cv/detection/#two-stage-detector","title":"Two-Stage Detector","text":"<ul> <li>R-CNN, Fast R-CNN, Faster R-CNN</li> <li>SPP-Net, R-FCN, FPN</li> </ul> <ul> <li>Overfeat</li> <li>R-CNN, Fast R-CNN, Faster R-CNN, SPP-Net</li> </ul>"},{"location":"cv/detection/#one-stage-detector","title":"One-Stage Detector","text":"<p>No need of Region Proposal Network</p> <p>They are very fast</p> <ul> <li>YOLO, SSD, RetinaNet</li> <li>CenterNet, CornerNet, ExtremeNet</li> </ul> <ul> <li>YOLO</li> <li>RetinaNet</li> <li>CornerNet</li> <li>CenterNet</li> <li>ExtremeNet</li> </ul>"},{"location":"cv/detection/#transformer-based-detector","title":"Transformer-Based Detector","text":"<ul> <li>DETR</li> </ul>"},{"location":"cv/detection/#methods","title":"Methods","text":"<ul> <li>Swin Transformer</li> <li>DINO</li> <li>InternImage</li> <li>OWL</li> </ul>"},{"location":"cv/detection/CenterNet/","title":"CenterNet","text":""},{"location":"cv/detection/CenterNet/#centernet","title":"CenterNet","text":"<ul> <li>Focus on the center of the object to infer its class.</li> <li>Use the corners as proposals, and the center to verify the class of the object and filter out outliers.</li> </ul>  CenterNet Architecture   Center Pooling Module"},{"location":"cv/detection/CornerNet/","title":"CornerNet","text":""},{"location":"cv/detection/CornerNet/#cornernet","title":"CornerNet","text":"<p>Bounding box cordinates as top-left and bottom-right corner.</p>  Hourglass network   Corner pooling"},{"location":"cv/detection/CornerNet/#issues","title":"Issues","text":"<ul> <li>Many incorrect bounding boxes (especially small) \\(\\rightarrow\\) too many False Positives</li> <li>Hypothesis: It is hard to infer the class of the box if the network is focused on the boundaries</li> </ul>"},{"location":"cv/detection/DETR/","title":"DETR","text":""},{"location":"cv/detection/DETR/#detr","title":"DETR","text":"<p>A direct set prediction approach to bypass the surrogate tasks (like proposals, anchors, window centers, non-maximum suppression).</p> <p>A encoder-decoder based architecure.</p> <p>It predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.</p>"},{"location":"cv/detection/DETR/#components","title":"Components","text":""},{"location":"cv/detection/DETR/#set-prediction","title":"Set Prediction","text":"<p><code>Bipartite matching loss</code></p> <p>The usual solution is to design a loss based on the Hungarian algorithm, to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match.</p>"},{"location":"cv/detection/DETR/#transformers-and-parallel-decoding","title":"Transformers and Parallel Decoding","text":"<p>Combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction.</p>"},{"location":"cv/detection/DETR/#object-detection","title":"Object detection","text":"<ol> <li>Two-Stage detectors</li> <li>One-Stage detectors</li> </ol> <p>The final performance of above systems heavily depends on the exact way these initial guesses are set.</p>"},{"location":"cv/detection/DETR/#model","title":"Model","text":""},{"location":"cv/detection/DETR/#object-detection-set-prediction-loss","title":"Object detection set prediction loss","text":"<p>DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image.</p> <p>\\(y:\\) ground-truth (\\(y_i = (c_i, b_i)\\), \\(c_i\\) rget class label and \\(b_i \\in [0,1]^4\\) is ground-truth box.)</p> <p>\\(\\hat{y} = \\{\\hat{y}_i\\}_{i=1}^{N}:\\) set of N predictions</p> <p>\\(y\\) also as a set of size \\(N\\) padded with \\(\\varnothing\\) (no object)</p> <p>Permutation of element with lowest cost:</p> <p>\\(\\hat{\\sigma} = \\arg\\min_{\\sigma\\in\\mathfrak{G}_N} \\sum_{i}^N \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)})\\)</p> <p>Pair-wise matching cost</p> <p>\\(\\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)}) = -\\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\hat{p}_{\\sigma(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)})\\)</p> <p>\\(\\hat{p}_{\\sigma_i}(c_i):\\) prediction with index \\(\\sigma(i)\\) we define probability of class \\(c_i\\)</p> <p>Hungarian loss</p> <p>\\(\\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^N \\left[-\\log \\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)})\\right]\\)</p> <p>Box-Loss</p> <p>\\(\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{\\text{L1}}\\|b_i - \\hat{b}_{\\sigma(i)}\\|_1\\)</p>"},{"location":"cv/detection/DETR/#detr-architecture","title":"DETR architecture","text":"<p>CNN Backbone</p> <p>As a feature extractor</p> <p>Transformer encoder</p> <p>Since the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings that are added to the input of each attention layer.</p> <p>Transformer decoder</p> <p>The difference with the original transformer is that our model decodes the N objects in parallel at each decoder layer, while Vaswani et al use an autoregressive model that predicts the output sequence one element at a time.</p> <p>Prediction feed-forward networks (FFNs)</p> <p>The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer.</p> <p>Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total N = 100 prediction slots in DETR decoder. </p> <p>This shows that the prediction box focuses on different part of the image.</p>"},{"location":"cv/detection/DETR/#rt-detr","title":"RT-DETR","text":""},{"location":"cv/detection/DETR/#fast-detr","title":"Fast-DETR","text":""},{"location":"cv/detection/DETR/#detrs-fastdetr","title":"DETRs-FastDETR","text":""},{"location":"cv/detection/DETR/#sam-det","title":"SAM-Det","text":""},{"location":"cv/detection/DETR/#ultra-detr","title":"ULTRA-DETR","text":""},{"location":"cv/detection/DINO/","title":"DINO","text":""},{"location":"cv/detection/DINO/#dino","title":"DINO","text":""},{"location":"cv/detection/DINO/#grounding-dino","title":"Grounding DINO","text":""},{"location":"cv/detection/ExtremeNet/","title":"ExtremeNet","text":""},{"location":"cv/detection/ExtremeNet/#extremenet","title":"ExtremeNet","text":"<ul> <li>Precticting the corner where object does not lies is hard for CNNs.</li> <li>Represent objects by their extreme points.</li> <li>No need to predict embeddings for the box computation.</li> </ul>  ExtremeNet Arch."},{"location":"cv/detection/ExtremeNet/#applications","title":"Applications","text":"<ul> <li>Extreme points are used commonly for annotation</li> </ul>"},{"location":"cv/detection/InternImage/","title":"InternImage","text":""},{"location":"cv/detection/OWL/","title":"OWLv2","text":""},{"location":"cv/detection/Overfeat/","title":"Overfeat","text":""},{"location":"cv/detection/Overfeat/#overfeat","title":"Overfeat","text":"<p>Slidingwindow + bbox regression + classification</p>"},{"location":"cv/detection/Overfeat/#sliding-window","title":"Sliding Window","text":"<p>Implicity encoded in the CNN architecture. Use sliding widow at different scale.</p>"},{"location":"cv/detection/Overfeat/#localization","title":"Localization","text":"<p>Regression</p>"},{"location":"cv/detection/Overfeat/#detection","title":"Detection","text":"<p>Classification</p>"},{"location":"cv/detection/Overfeat/#cons","title":"Cons","text":"<ul> <li>Needs fixed sized window as the fully-connected layer need to have fixed input.</li> <li>Expensive to try out all the possible positions, scales and aspect ratio. (Choose only the potential location)</li> </ul>"},{"location":"cv/detection/RCNN/","title":"RCNNs","text":""},{"location":"cv/detection/RCNN/#rcnn","title":"RCNN","text":""},{"location":"cv/detection/RCNN/#rcnn_1","title":"RCNN","text":"<p>Steps</p> <ol> <li> <p>Scan the input image for possible objects using an algorithm called Selective Search, generating ~2000 region proposals</p> </li> <li> <p>Warp to a fix size 227 x 227</p> </li> <li> <p>Run a convolutional neural net (CNN) on top of each of these region proposals</p> </li> <li> <p>Make the output of each CNN and feed it into:</p> <p>a) an SVM to classify the region and </p> <p>b) a linear regressor to tighten the bounding box of the object, if such an object exists.</p> </li> </ol>"},{"location":"cv/detection/RCNN/#training","title":"Training","text":"<ol> <li>Pre-train the CNN on ImageNet</li> <li>Finetune the CNN on the number of classes the detector is aiming to classify (softmax loss).</li> <li>Train a linear Support Vector Machine classifier to classify image regions. One SVM per class! (hinge loss)</li> <li>Train the bounding box regressor (L2 loss)</li> </ol> <p>Cons 1. If we have overlapping window then we will do ConvNet computation for each of the pixels more than 1 times. This increases extra computation.</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#spp-net","title":"SPP Net","text":"<p>Makes the RCNN fast at test time.</p> <p>Issues</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#fast-rcnn","title":"Fast RCNN","text":"<ol> <li>Performing feature extraction over the image before proposing regions, thus only running one CNN over the entire image instead of 2000 CNN\u2019s over 2000 overlapping regions.</li> <li>After conv5 there is FC layer we need to make all the deature size need to be of same size using RoI Pooling layer.</li> <li>Replacing the SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn","title":"Faster RCNN","text":"<ul> <li>Removes region proposal network Can we reuse our CNN feature and still be able to create this proposal.</li> </ul> <p>How to extract proposals.</p> <ul> <li> <p>How many proposals?</p> <ul> <li>Decide a fix number</li> <li>set of 9 anchor box per location (3 scales, 3 aspect ratio)</li> </ul> </li> <li> <p>Where are they placed?</p> <ul> <li>Densly</li> </ul> </li> <li> <p>For each of the location get the descriptor of 256-d by 3X3 filtermap</p> </li> <li> <p>Pass the descriptor to the classification layer and regression layer</p> </li> </ul>"},{"location":"cv/detection/RCNN/#rpn-training","title":"RPN Training","text":"Region Proposal Network  <p>Classification Ground-Truth</p> <p>\\(p^{*}\\) Amount of anchor box overlapping with the Ground-Truth.</p> <p>\\(p^{*} = 1\\) if IoU &gt; 0.7 (anchor is in foreground)</p> <p>\\(p^{*} = 0\\) if IoU &lt; 0.3 (anchor is in background)</p> <p>For training only consider above two case.</p> <ol> <li>Randomly sample 256 sample, form mini-batch.</li> <li>Calculate binary CE loss</li> <li>Anchor box containing object will go through regression box</li> </ol> <p>Anchor box \\((x_a, y_a, w_a, h_a)\\), \\(x_a, y_a\\) is center of box and rest width and height respectively.</p> <ol> <li>Network actually predicts are \\((t_x, t_y, t_w, t_h)\\) which are relative.</li> </ol> <p>\\(t_x = (x-x_a)/w_a\\)</p> <p>\\(t_y = (y-y_a)/h_a\\)</p> <p>\\(t_w = \\log(w/w_a)\\)</p> <p>\\(t_h = \\log(h/h_a)\\)</p> <ol> <li>Smooth L1 loss on regression targets</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn-training","title":"Faster RCNN Training","text":"<p>Can be train jointly. But in paper it is trained in following manner.     - RPN classification (object/non-object)     - RPN regression (anchor -&gt; proposal)     - Fast R-CNN classification (type of object)     - Fast R-CNN regression (proposal -&gt; box)</p> <p>Pros</p> <ol> <li>10x faster at test time wrt Fast R-CNN</li> <li>Trained end-to-end including feature extraction, region proposals, classifier and regressor.</li> <li>More accurate, since proposals are learned. RPN is fully convolutional.</li> </ol>"},{"location":"cv/detection/RCNN/#conclusion","title":"Conclusion","text":"R-CNN Fast RCNN Faster RCNN Test time per image (sec) 50 2 0.2 Speeed-Up 1X 25X 250X mAP (VOC 2007) 66.0 66.9 66.9"},{"location":"cv/detection/RetinaNet/","title":"RetinaNet","text":""},{"location":"cv/detection/RetinaNet/#retinanet","title":"RetinaNet","text":"<p>Since there are lots of anchor box in which there is no object and very few of them object. We need to incorporate this information in the loss function which is done with the weighting of the loss function.</p>  Focal Loss  <p>As \\(\\gamma\\) increases the easy sample weight decreases.</p>"},{"location":"cv/detection/RetinaNet/#key-point","title":"Key Point","text":"<ul> <li>Proposed: Focal loss</li> <li>Powerful feature extraction: ResNet</li> <li>Multi-scale prediction</li> <li>9 anchors per level, each one with a classification and regression target</li> </ul>"},{"location":"cv/detection/SSD/","title":"SSD","text":""},{"location":"cv/detection/SelectiveSearch/","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#selective-search","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#wip","title":"WIP","text":""},{"location":"cv/detection/SwinTransformer/","title":"Swin Transformer","text":""},{"location":"cv/detection/YOLO-World/","title":"YOLO-World","text":""},{"location":"cv/detection/YOLO-World/#yolo-world","title":"YOLO-World","text":"<ul> <li> <p>YOLO with openvocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets.</p> </li> <li> <p>Propose a new Re-parameterizable VisionLanguage Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information.</p> </li> <li> <p>\\(\\Omega = \\{B_i, t_i \\}_{i=1}^{N}\\) where \\(t_i\\) is the corresponding text for the region \\(B_i\\)</p> </li> </ul>  YOLO-World Architecture   YOLO-World Architecture  <p>Architecture 1. YOLO Detector      - YOLOv8 2. Text Encoder      - CLIP 3. Text Contrastive Head      - object-text similarity     - \\(s_{k,j} = \\alpha \\cdot \\text{L2-Norm}(e_k) \\cdot \\text{L2-Norm}(w_j)^T + \\beta,\\)     - \\(e_k\\) object embedding and \\(w_j\\) word embedding     - \\(\\alpha\\) and \\(\\beta\\) are learnable.</p>"},{"location":"cv/detection/YOLO-World/#re-parameterizable-vision-language-pan","title":"Re-parameterizable Vision-Language PAN","text":"<ul> <li>establish the feature pyramids {P3,P4,P5} with the multi-scale image features {C3,C4,C5}</li> <li>propose the Text-guided CSPLayer (T-CSPLayer) and Image-Pooling Attention (I-Pooling Attention) to further enhance the interaction between image features and text features, which can improve the visual-semantic representation for open-vocabulary capability</li> </ul>  RepVL-PAN"},{"location":"cv/detection/YOLO-World/#text-guided-csplayer","title":"Text-guided CSPLayer","text":"<p>Text-guided cross-stage partial layers</p> <ul> <li>text embeddings W</li> <li>Image features \\(X_l \\in R^{H \\times W \\times D} (l \\in \\{3, 4, 5 \\})\\)</li> <li>\\(\\delta\\) sigmoid function \\(X_l' = X_l \\cdot \\delta(\\max_{j \\in \\{1..C\\}} (X_lW_j^{\\top}))\\)</li> </ul>"},{"location":"cv/detection/YOLO-World/#image-pooling-attention","title":"Image-Pooling Attention","text":"<ul> <li>Max-Pooling output \\(\\tilde{X}\\)</li> </ul> <p>\\(W' = W + \\text{MultiHead-Attention}(W, \\tilde{X}, \\tilde{X})\\)</p>"},{"location":"cv/detection/YOLO-World/#training","title":"Training","text":"<p>\\(\\mathcal{L}(I) = \\mathcal{L}_{con} + \\lambda_I \\cdot (\\mathcal{L}_{iou} + \\mathcal{L}_{dfl})\\)</p> <ul> <li>\\(\\mathcal{L}_{con}\\) : region-text contrastive loss</li> <li>\\(\\mathcal{L}_{dfl}\\) : distributed focal loss</li> <li>\\(\\lambda_I \\in \\{0, 1\\}\\) <ul> <li>1 if \\(I\\) is from detection or grounding data</li> <li>0 if \\(I\\) is from the image-text data</li> </ul> </li> </ul>"},{"location":"cv/detection/YOLO/","title":"YOLO","text":""},{"location":"cv/detection/YOLO/#yolo","title":"YOLO","text":"<p>It does not have region proposal network and also it does not have the fully-connected layer. </p>  You Only Look Once"},{"location":"cv/detection/YOLO/#process","title":"Process","text":"<ol> <li>Divide the image into grid (SxS cells).</li> <li>Predict B anchor box at the center of each box along with the confidence score</li> <li>Predict C classes for each grid cell.</li> </ol> <p>YOLO-Tensor : SxS(Bx5 + C) </p> <p>where, - SxS : number of grid</p> <ul> <li> <p>B : Number of bbox \\((P_c, b_x, b_y, b_h, b_w)\\)</p> </li> <li> <p>C : Number of classes</p> </li> </ul>  Predicting bounding box and confidence score for each cell.   Predicting class probability for each cell."},{"location":"cv/detection/YOLO/#loss-function","title":"Loss Function","text":"<p>\\(L_{total} = L_{localization} + L_{confidence} + L_{classification}\\)</p> <p>$ L_{localization} = \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 + (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2]$</p> <p>\\(L_{confidence} =  \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 + \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} (C_i - \\hat{C}_i)^2\\)</p> <p>\\(L_{classification} = \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum_{c\\in classes} (p_i(c) - \\hat{p}_i(c))^2\\)</p> <p>\\(\\lambda_{coord} = 5\\)</p> <p>\\(\\lambda_{noobj} = 0.5\\)</p>"},{"location":"cv/detection/YOLO/#yolo-papers","title":"YOLO Papers","text":""},{"location":"cv/detection/YOLO/#yolo-survey","title":"YOLO Survey","text":""},{"location":"cv/detection/YOLO/#yolo-v1-slides","title":"YOLO-v1 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v2-slides","title":"YOLO-v2 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v3","title":"YOLO-v3","text":""},{"location":"cv/detection/YOLO/#yolo-v4","title":"YOLO-v4","text":""},{"location":"cv/detection/YOLO/#yolo-v5-colab","title":"YOLO-v5 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v6-colab","title":"YOLO-v6 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v7-colab","title":"YOLO-v7 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v8","title":"YOLO-v8","text":""},{"location":"cv/detection/YOLO/#yolo-nas","title":"YOLO-NAS","text":""},{"location":"cv/detection/YOLOE/","title":"YOLOE","text":""},{"location":"cv/faceDetection/","title":"Face Detection","text":""},{"location":"cv/faceDetection/#face-detection","title":"Face Detection","text":""},{"location":"cv/faceDetection/#haar-cascade","title":"Haar Cascade","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#hog-svm","title":"HoG + SVM","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#facenet","title":"FaceNet","text":"FaceNet   Triplet Loss"},{"location":"cv/faceDetection/#loss","title":"Loss","text":"<p>\\(L = \\sum_{i}^{N} \\left[\\|f(x_i^a) - f(x_i^p)\\|_2^2 - \\|f(x_i^a) - f(x_i^n)\\|_2^2 + \\alpha\\right]_+\\)</p>"},{"location":"cv/nerf/","title":"NeRF","text":""},{"location":"cv/nerf/#nerf-video","title":"NeRF Video","text":""},{"location":"cv/nerf/#code","title":"Code","text":""},{"location":"cv/objectTracking/","title":"Object Tracking","text":""},{"location":"cv/objectTracking/#object-tracking","title":"Object Tracking","text":"<p>To continuously locate and maintain the identity of target objects as they move through video frames.</p> <p>Tracking is similarity measurement, correlation, correspondence, Matching/retrieval &amp; data association.</p>"},{"location":"cv/objectTracking/#learining","title":"Learining","text":""},{"location":"cv/objectTracking/#appearance","title":"Appearance","text":"<p>we need to know how the target looks like - Single object tracking - Re-identification</p>"},{"location":"cv/objectTracking/#motion","title":"Motion","text":"<p>To make predictions of where the targets goes - Trajectory prediction</p>"},{"location":"cv/objectTracking/#single-target-tracking","title":"Single Target Tracking","text":""},{"location":"cv/objectTracking/#as-a-matchingcorrespondence-problem","title":"As a matching/correspondence problem","text":"<ul> <li>GOTURN: no online appearance modeling</li> </ul> <p>Input: what to track?</p> <p>Architecture: conv + concatenate + FC</p> <p>Pros: - No training, very fast as it is juct template matching problem</p> <p>Cons: - Does not work if the object moves very fast and goes out of search window.</p>"},{"location":"cv/objectTracking/#as-an-appearance-learning-problem","title":"As an appearance learning problem","text":"<ul> <li>MDNet: quick online finetuning of the network</li> <li>Slow: not suitable for real-time applications</li> <li>Solution: train as few layers as possible</li> </ul> <p>At test time, we need to train fc6 (up to fc4 if wanted)</p> <p>Pros: - No previous location assumption, the object can move anywhere in the image</p> <p>Cons: - Not as fast as GOTURN</p>"},{"location":"cv/objectTracking/#as-a-temporal-prediction-problem","title":"As a (temporal) prediction problem","text":"<ul> <li>ROLO = CNN + LSTM</li> </ul> <p>LSTM receives the heatmap for the object\u2019s position and the 4096 descriptor of the image</p>"},{"location":"cv/objectTracking/#challanges","title":"Challanges","text":"<ul> <li>Occlusions</li> <li>Viewpoint/pose/blur/illumination variations (in a few frames of a sequence)</li> <li>Background clutter</li> </ul>"},{"location":"cv/objectTracking/#multiple-object-tracing","title":"Multiple Object Tracing","text":""},{"location":"cv/objectTracking/#online-tracking","title":"Online Tracking","text":"<ul> <li>Processes two frames at a time</li> <li>For real-time applications</li> <li>Prone to drifting \u00e0 hard to recover from errors or occlusions</li> </ul> <p>Process - Track initialization (e.g. using a detector) - Prediction of the next position (motion model)     - Kalman filter     - Recurrent architecture     - constant velocity model (works really well at high framerates and without occlusions!)</p> <ul> <li>Matching predictions with detections (appearance model)<ul> <li>Bipartite matching</li> </ul> </li> </ul>"},{"location":"cv/objectTracking/#track-initialization","title":"Track initialization","text":"<p>Making a detector into a tracktor - Tracktor: a method trained as a detector but with tracking capabilities. - Where did the detection with ID1 go in the next frame? </p> <p>Two-Step Detector - Region Proposal - Regression</p> <p>Pros - Tracktor are online - We can train our model on still images - We can reuse an extremely well-trained regressor</p> <p>Cons - Confusion in crowded places as there is no notion of identification. - The track is killed if the target becomes occluded. - Will not work if the object or the camera has large motions.</p> <p>1st &amp; 2nd can be solved using ReID (Re-Identification). While 2nd &amp; 3rd can be solved using Motion model.</p> <p>Modeling Appearence : Re-ID Modeling Motion : Model Motion </p>"},{"location":"cv/objectTracking/#prediction-of-the-next-position","title":"Prediction of the next position","text":""},{"location":"cv/objectTracking/#matching-predictions-with-detections-appearance-model","title":"Matching predictions with detections (appearance model)","text":""},{"location":"cv/objectTracking/#offline-tracking","title":"Offline Tracking","text":"<ul> <li>Processes a batch of frames</li> <li>Good to recover from occlusions</li> <li>Not suitable for real-time applications</li> <li>Suitable for video analysis</li> </ul>"},{"location":"cv/objectTracking/#challanges_1","title":"Challanges","text":"<ul> <li>Multiple objects of the same type</li> <li>Heavy occlusions</li> <li>Appearance is often very similar</li> </ul>"},{"location":"cv/objectTracking/#applications","title":"Applications","text":"<ul> <li>Surveillance &amp; Security</li> <li>Traffic Monitoring</li> <li>Autonomous Vehicles</li> <li>Sports Analytics</li> <li>Human-Computer Interaction</li> <li>Medical Imaging</li> <li>Robotics</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/","title":"Bipartite Matching","text":""},{"location":"cv/objectTracking/BipartiteMatching/#bipartite-matching","title":"Bipartite Matching","text":"<ul> <li>Links detections with predictions using distance metrics</li> <li>Uses IoU, Pixel, or 3D distances between boxes</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#process","title":"Process","text":"<ul> <li>Calculate distances between boxes</li> <li>Apply Hungarian algorithm for optimal matching</li> <li>Set thresholds to handle missing/unsuitable matches</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#edge-cases","title":"Edge Cases","text":"<ul> <li>Missing prediction: Add dummy nodes</li> <li>No suitable match: Use cost threshold</li> <li>Unmatched boxes: Create new tracks</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#output","title":"Output","text":"<ul> <li>Matched pairs</li> <li>New tracks from unmatched detections</li> <li>Lost tracks from unmatched predictions</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#hungarian-algorithm","title":"Hungarian algorithm","text":"<p>Demo</p>"},{"location":"cv/objectTracking/KalmanFilter/","title":"Kalman Filter","text":""},{"location":"cv/opticalFlow/","title":"Optical Flow","text":""},{"location":"cv/opticalFlow/#optical-flow","title":"Optical Flow","text":"<p>Optical flow refers to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer (camera) and the scene. It helps understand how objects move in a sequence of images.</p> <p>The optical flow problem involves estimating a dense vector field where each vector represents the displacement of a pixel from one frame to the next. This displacement field captures both the magnitude and direction of motion for each point in the image.</p> <p>Key characteristics of optical flow: 1. Dense Motion Field: Unlike feature tracking which follows specific points, optical flow estimates motion for every pixel in the image 2. Temporal Consistency: Assumes that pixel intensities remain constant between consecutive frames (brightness constancy assumption) 3. Spatial Smoothness: Nearby pixels tend to move in similar ways (smoothness constraint)</p>"},{"location":"cv/opticalFlow/#methods","title":"Methods:","text":"<ul> <li>Patched Based<ol> <li>Lucas-Kanade</li> <li>Horn-Shunck</li> </ol> </li> <li>NN Based<ol> <li>FlowNet</li> </ol> </li> </ul>"},{"location":"cv/opticalFlow/#perception-of-motion","title":"Perception of Motion","text":"Figure 1: Illustration of optical flow and motion perception. The top left image shows a person running with a static camera (no camera motion), while the top right image shows the same person running but with the camera moving in the opposite direction. The bottom diagrams visualize the resulting optical flow vectors for each scenario. <p>Assuming Image intensity is constant.</p> <p>Brightness Constancy Equation:</p> <p>\\(I(x,y,t) \\approx I(x+dx,y+dy,t+dt)\\)</p> <p>Using Taylor Series Expansion:</p> <p>\\(I(x(t)+u.\\Delta t,y+v.\\Delta t) - I(x(t),y(t),t) \\approx 0\\)</p> <p>\\(I_x \\cdot u +  I_y \\cdot v + I_t = 0\\) (Brightness Constancy Constraint)</p> <p>\\([u, v]\\) is the optical flow.</p>"},{"location":"cv/opticalFlow/#lucas-and-kanade","title":"Lucas and Kanade","text":"<p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\(2(I_xu+ I_yv+ I_t)I_x = 2(I_xu+ I_yv+ I_t)I_y = 0\\)</p> <p>\\(\\begin{bmatrix} \\sum I_{x}^2 &amp; \\sum I_{x}I_{y} \\\\ \\sum I_{x}I_{y} &amp; \\sum I_{y}^2 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} \\sum I_{x}I_{t} \\\\ \\sum I_{y}I_{t} \\end{bmatrix}\\)</p> <p>Structural Tensor representation:</p> <p>\\(\\begin{bmatrix} T_{xx} &amp; T_{xy} \\\\ T_{xy} &amp; T_{yy} \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} T_{xt} \\\\ T_{yt} \\end{bmatrix}\\)</p> <p>$u = \\frac{T_{yt}T_{xy} - T_{xt}T_{yy}}{T_{xx}T_{yy} - T_{xy}^2} \\text{ and } v = \\frac{T_{xt}T_{xy} - T_{yt}T_{xx}}{T_{xx}T_{yy} - T_{xy}^2} $</p>"},{"location":"cv/opticalFlow/#issues","title":"Issues","text":"<ul> <li>Brightness constancy is not satisfied (Correlation based method could be used)</li> <li>A point may not move like its neighbors (Regularization based methods)</li> <li>The motion may not be small (Taylor does not hold!) (Multi-scale estimation could be used)</li> </ul>"},{"location":"cv/opticalFlow/#horn-schunck","title":"Horn &amp; Schunck","text":"<p>Global method with smoothness constraint to solve aperture problem</p> <p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 + \\alpha^2(|\\nabla u|^2 + |\\nabla v|^2) dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\((I_xu+ I_yv+ I_t)I_x - \\alpha^2(|\\nabla u|)= (I_xu+ I_yv+ I_t)I_y + \\alpha^2(|\\nabla u|) = 0\\)</p>"},{"location":"cv/opticalFlow/#flownet","title":"FlowNet","text":"<p>End-to-end frame work to for optical flow prediction.</p>"},{"location":"cv/opticalFlow/#simplenet","title":"SimpleNet","text":"<p>Both input images together and feed them through a rather generic network, allowing the network to decide itself how to process the image pair to extract the motion information.</p>"},{"location":"cv/opticalFlow/#flownetcorr","title":"FlowNetCorr","text":"<p>First produce meaningful representations of the two images separately and then combine them on a higher level. <code>correlation layer</code> performs multiplicative patch comparisons between two feature maps.</p> <p>Correlation of 2 patches of size \\(K \\times K\\) is given by :</p> <p>\\(c(\\mathbf{x}_1, \\mathbf{x}_2) = \\sum_{\\mathbf{o} \\in [-k,k] \\times [-k,k]} \\langle \\mathbf{f}_1(\\mathbf{x}_1 + \\mathbf{o}), \\mathbf{f}_2(\\mathbf{x}_2 + \\mathbf{o}) \\rangle\\)</p> <p>where \\(K = 2k+1\\). Above equation is similar to convolution but it is convolution of one data with another instead of filter.</p>"},{"location":"cv/opticalFlow/#flownetrefine","title":"FlowNetRefine","text":"<p>The main ingredient are \u2018upconvolutional\u2019 layers, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. To perform the refinement, we apply the \u2018upconvolution\u2019 to feature maps, and concatenate it with corresponding feature maps from the \u2019contractive\u2019 part of the network and an upsampled coarser flow prediction (if available). </p> <p>This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps.</p>"},{"location":"cv/opticalFlow/#application","title":"Application","text":"<ol> <li>Motion based segmentation</li> <li>SfM</li> <li>Alignment (e.g., UAV analysis)</li> <li>Video Compression</li> <li>Object Tracking</li> <li>Deformation Analysis</li> </ol>"},{"location":"cv/poseEstimation/","title":"Pose Estimation","text":""},{"location":"cv/poseEstimation/#pose-estimation","title":"Pose Estimation","text":"<ul> <li>Estimate a 2D pose (x,y) coordinates for each joint from a RGB image.</li> <li>17 joints</li> <li>Challanges<ul> <li>occlusions</li> <li>clothing</li> <li>extreme poses</li> <li>viewpoint changes etc.</li> </ul> </li> </ul>"},{"location":"cv/poseEstimation/#direct-regression","title":"Direct Regression","text":"DeepPose"},{"location":"cv/poseEstimation/#heatmap-predicion","title":"HeatMap Predicion","text":"<ul> <li>Instead of prediction by regression, for each joint one predicts a full image with a heatmap of the joint location</li> <li>Powerful representation, easier to predict a confidence per location, rather than regress a value for the position</li> <li>Ground truth (GT) heatmap is constructed by placing a 2D Gaussian around the joint position (e.g. variance 1.5 pixels)</li> <li>Loss: MSE between predicted and GT heatmap</li> </ul>  Newell predicted heatmap  <p>Bringing the structure of the problem - Body parts are linked to each other - Body symmetries - Joint limits, e.g., elbow cannot bend backwards - Physical connectivity: elbow connected to wrist</p> <p>Using graphical models also allows us to find the pose of several targets</p>  DeepCut  <p>Alternatively one ca do two stage process</p> <ol> <li>Object Detection</li> <li>Pose Estimation</li> </ol>"},{"location":"cv/segmentation/","title":"Image Segmentatio","text":""},{"location":"cv/segmentation/#semantic-segmentation","title":"Semantic Segmentation","text":""},{"location":"cv/segmentation/#instace-based-segmentation","title":"Instace-Based Segmentation","text":""},{"location":"cv/segmentation/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Region-based<ul> <li>e.g., IoU or Jaccard index, F-measure  or Dice\u2019s coefficient,  weighted F-measure,</li> </ul> </li> <li>Boundary-based <ul> <li>CM, boundary F-measure, boundary IoU, boundary displacement error (BDE), Hausdorff distances, </li> </ul> </li> <li>Structure-based<ul> <li>S-measure, E-measure, </li> </ul> </li> <li>Confidence-based <ul> <li>MAE </li> </ul> </li> </ul>"},{"location":"cv/segmentation/#dichotomous-image-segmentation","title":"Dichotomous Image Segmentation","text":""},{"location":"cv/segmentation/#dis","title":"DIS","text":""},{"location":"cv/segmentation/#birefnet","title":"BiRefNet","text":""},{"location":"cv/segmentation/#methods","title":"Methods","text":""},{"location":"cv/segmentation/#pdfnet","title":"PDFNet","text":""},{"location":"cv/segmentation/#ben","title":"BEN","text":""},{"location":"cv/segmentation/BiRefNet/","title":"BiRefNet","text":""},{"location":"cv/segmentation/BiRefNet/#birefnet","title":"BiRefNet","text":"<p>Bilateral Reference for High-Resolution Dichotomous Image Segmentation</p> <ul> <li>Swin-Transformer Based</li> <li>inward reference(InRef) and an outward reference (OutRef)</li> </ul> <p>Two essential Module:</p> <ol> <li>Localization module (LM) : <ul> <li>object localization using global semantic information</li> <li>extract hierarchical features from vision transformer backbone, which are combined and squeezed to obtain corase predictions in low resolution in deep layers.</li> </ul> </li> <li>Reconstruction module (RM) : <ul> <li>hierarchical patches of images provide the source reference, and gradient maps serve as the target reference.</li> <li>the inward and outward references as bilateral references (BiRef), in which the source image and the gradient map are fed into the decoder at different stages.</li> </ul> </li> </ol>  BiRefNet Comparison"},{"location":"cv/segmentation/BiRefNet/#localization-module","title":"Localization Module","text":"BiRefNet Architecture  <ul> <li>Transformer Encoder extract the fearures at different stages i.e., \\(F_1^e, F_2^e, F_3^e\\) with resolution at 4,8,16,32. </li> <li>The features of the first four \\(\\{F_i^e\\}_{i=1}^3\\) are transferred to the corresponding decoder stages with lateral connections (1\u00d71 convolution layers).</li> <li>These features are stacked and concatenated in the last encoder block to generate \\(F^e\\) then fed into a classification module.</li> </ul> <p>To enlarge the receptive fields to cover features of large objects and focus on local features for high precision simultaneously Atrous Spatial Pyramid Pooling (ASPP)  is used for multi-context fusion.</p>"},{"location":"cv/segmentation/BiRefNet/#reconstruction-module","title":"Reconstruction Module","text":"BiRef Blocks  <ul> <li> <p>Small receptive field (RFs) lead to inadequate context information to locate the right target on a large background, whereas large RFs often result in insufficient feature extraction in detailed areas.</p> </li> <li> <p>To achieve balance, using reconstruction block (RB) in each BiRef block as a replacement for the vanilla residual blocks.</p> </li> <li> <p>In RB, we employ deformable convolutions with hierarchical receptive fields (i.e., 1\u00d71, 3\u00d73, 7\u00d77) and an adaptive average pooling layer to extract features with RFs of various scales.</p> </li> <li> <p>These features extracted by different RFs are then concatenated as \\(F_i^{\\theta}\\), followed by a 1\u00d71 convolution layer and a batch normalization layer to generate the output feature of RM \\(F_i^{d'}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#bilateral-reference","title":"Bilateral Reference","text":"<ul> <li> <p>inward reference(InRef) and an outward reference (OutRef)</p> </li> <li> <p>In InRef, images \\(I\\) with original high resolution are cropped to patches \\(\\{P_{k=1}^N\\}\\) of consistent size with the output features of the corresponding decoder stage.</p> </li> <li> <p>These patches are stacked with the original feature \\(F_i^{d+}\\) to be fed into the RM.</p> </li> <li> <p>In OutRef, we use gradient labels to draw more attention to areas of richer gradient information which is essential for the segmentation of fine structures.</p> </li> <li> <p>First, we extract the gradient maps of the input images as \\(G_i^{gt}\\). Meanwhile, \\(F_i^{\\theta}\\) is used to generate the feature \\(F_i^G\\) to produce the predicted gradient maps \\(\\hat{G}^i\\)</p> </li> <li> <p>It passes through a conv and a sigmoid layer and is used to generate the gradient referring attention \\(A_i^G\\), which is then multiplied by \\(F_i^{d'}\\) to generate output of the BiRef block as \\(F_{i\u22121}^{d}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#loss","title":"Loss","text":"<p>\\(L = L_{pixel} + L_{region} + L_{boundary} + L_{semantic} \\\\ = \\lambda_1 L_{BCE} + \\lambda_2 L_{IoU} + \\lambda_3 L_{SSIM} + \\lambda_4 L_{CE}\\)</p> <p>\\(L_{BCE} = -\\sum_{(i,j)} [G(i,j) \\log(M(i,j)) + (1-G(i,j)) \\log(1-M(i,j))]\\)</p> <p>\\(L_{IoU} = 1 - \\frac{\\sum_{r=1}^H \\sum_{c=1}^W M(i,j)G(i,j)}{\\sum_{r=1}^H \\sum_{c=1}^W [M(i,j)+G(i,j)-M(i,j)G(i,j)]}\\)</p> <p>\\(L_{SSIM} = 1 - \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\\)</p> <p>\\(L_{CE} = -\\sum_{c=1}^N y_{o,c}\\log(p_{o,c})\\)</p>"},{"location":"cv/segmentation/DIS/","title":"DIS","text":""},{"location":"cv/segmentation/DIS/#dis","title":"DIS","text":"<p>Dichotomous Image Segmentation (DIS) proposed IS-Net. IS-Net as 3 components</p> <ol> <li>ground truth (GT) encoder,</li> <li>image segmentation component (\\(U^2\\)-Net with an input convolution layer before its first encoder stage.)</li> <li>intermediate supervision strategy</li> </ol>"},{"location":"cv/segmentation/DIS/#1st-stage","title":"1st Stage","text":"<p>Self-supervised training of the GT-encoder</p> <p>\\(L_{gt} = \\sum_{d=1}^{D} \\lambda_{d}^{gt} BCE(F_{gt}(\\theta_{gt}, G)_d, G)\\)</p> <p>GT encoder will be frozen.</p>"},{"location":"cv/segmentation/DIS/#2nd-stage","title":"2nd Stage","text":"<p>High dimensional intermediate features</p> <p>\\(f_{D}^{G} = F_{gt}^{-}(\\theta_{gt}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <ul> <li> <p>\\(F_{gt}^{-}\\) represents the \\(F_{gt}\\)  without the last convolution layers for generating the probability maps.</p> </li> <li> <p>\\(F_{gt}^{-}\\) is to supervise those corresponding features \\(f_{D}^{I}\\) from the segmentation model \\(F_{sg}\\)</p> </li> </ul> <p>High dimensional intermediate features from image segmentation component</p> <p>\\(f_{I}^{G} = F_{sg}^{-}(\\theta_{sg}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <p>Feature Consistency Loss (intermediate supervision)</p> <p>\\(L_{fs} = \\sum_{d=1}^{D} \\lambda_{d}^{fs} ||f_{d}^{I} - f_{d}^{G}||^2\\)</p> <p>\\(L_{sg} = \\sum_{d=1}^{D} \\lambda_{d}^{sg} BCE(F_{sg}(\\theta_{sg}, I), G)\\)</p> <p>Loss for \\(F_{sg}\\) is  \\(L = L_{fs} + L_{sg}\\)</p>  IS-Net"},{"location":"cv/segmentation/DIS/#results","title":"Results","text":"Result"},{"location":"cv/segmentation/DIS/#meric-human-correction-efforts-hce","title":"Meric : Human Correction Efforts (HCE)","text":""},{"location":"ml/","title":"Machine Learning","text":""},{"location":"ml/#machine-learning","title":"Machine Learning","text":"<ul> <li> Linear Regression</li> <li> Logistic Regression</li> <li> Naive Bayes</li> <li> Principal Component Analysis (PCA)</li> <li> Linear Discriminant Analysis (LDA)</li> <li> k-Nearest Neighbors (k-NN)</li> <li> k-means Clustering</li> <li> Support Vector Machine (SVM)</li> <li> Decision Tree</li> <li> Random Forest</li> <li> Boosting Trees</li> <li> Adaboost</li> <li> Perceptron</li> <li> Neural Network</li> <li> Convolutional Neural Network (CNN)</li> </ul>"},{"location":"ml/CNN/","title":"Convolutional Neural Network","text":""},{"location":"ml/CNN/#convolutional-neural-network","title":"Convolutional Neural Network","text":"<p>\\(W_{new} = \\frac{W_{old} - F + 2P}{S} +1\\)</p> <p>\\(H_{new} = \\frac{H_{old} - F + 2P}{S} +1\\)</p> <p>How is learning kernel is different from regular feed-forward NN? - Weight Sharing - Sparse Network</p> <p>CNN tries to learn ecific characterstics of inputs, different neurons can fire for different characterstics.</p>"},{"location":"ml/DecisionTree/","title":"Decision Tree","text":""},{"location":"ml/GENERAL/","title":"Performance","text":""},{"location":"ml/GENERAL/#performance","title":"Performance","text":"<p>Higher degree polynomial linear regression model may result in overfitting. <code>Model complexity increase may result in overfitting</code></p> <p>Case of overfitting : 1. Low loss for training and high for the test. 2. Low accuracy for training and high for the test.</p> <p>Case of underfitting : 1. High loss for training and high for the test. 2. Low accuracy for training and low for the test.</p>"},{"location":"ml/GENERAL/#prevention","title":"Prevention","text":"<ol> <li> <p>Regularization: will reduce overfitting by penalizing the large coefficients which would lead to generalization. </p> </li> <li> <p>Train with more data: by having more data the model can better detect the signal and it reduces the chances of the model memorizing the data. </p> </li> <li> <p>Cross-Validation: split the data into k groups and let one group be the validation set while the others are used for training the model (we interchange which group becomes the validation set). </p> </li> <li> <p>Reduce the number of features: We can use some feature selection methods \u2013 filter-based (chi-square), wrapper based (Recursive Feature Elimination) or embedded like Lasso regularization. </p> </li> <li> <p>Change the model for e.g. ensemble learning techniques: by having multiple weak models instead of one model, we hope to capture the signal in the data better and so  we can generalize to unseen datasets. </p> </li> </ol>"},{"location":"ml/GENERAL/#regularization","title":"Regularization","text":"<ol> <li>Lasso (L1 regularization): Lasso has \u201cbuilt-in\u201d feature selection since it shrinks the least important features\u2019 coefficient to zero, creating sparse outputs.  \\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda |\\sum_{i=0}^{M}w_j|\\)</li> <li>Ridge (L2 regularization): Ridge regression is more computationally efficient due to being differentiable at 0 (can be used easily with gradient descent) while lasso is undefined at 0. \\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda \\sum_{i=0}^{M}w_j^2\\)</li> <li>Elastic-net(L1+L2 regularization): </li> </ol>"},{"location":"ml/GENERAL/#bias-variance-tradeoff","title":"Bias-Variance tradeoff","text":"<ul> <li>Bias is the difference between the predicted value and the actual value. Increasing the complexity of model, increases the model performance (bias decreases and variance increases). </li> <li>Variance is the sensitivity of the model to new data.</li> </ul> <p>The Bias-variance tradeoff is the relationship between bias and variance as you try to minimize each but by minimizing one the other increases, so there is  inherently a tradeoff. </p> \\[\\text{Expected Err. = Variance + $\\text{Bias}^2$  + Irreducible Error} \\]"},{"location":"ml/GENERAL/#handling-data-imbalance","title":"Handling Data Imbalance","text":"<ul> <li>over-sampling: sample the minority class significantly more such that the dataset is more balanced </li> <li>under-sampling: sample the majority class significantly less such that the dataset is more balanced </li> <li>SMOTE (synthetic minority oversampling technique) creates examples of the minority class by randomly sampling feature values from the current features </li> <li>Use another ML model: tree-based models perform well on imbalanced datasets</li> </ul>"},{"location":"ml/GENERAL/#curse-of-dimensionality","title":"Curse of Dimensionality","text":"<p>The curse of dimensionality describes the phenomenon where the feature space becomes  increasingly sparse for an increasing number of dimensions of a fixed-size training dataset.</p>"},{"location":"ml/GENERAL/#distance-metric","title":"Distance Metric","text":"<ol> <li>Euclidean Distance:</li> <li>Straight-line distance between two points</li> <li> <p>Formula: \\(d(p,q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}\\)</p> </li> <li> <p>Manhattan Distance:</p> </li> <li>Sum of absolute differences between coordinates</li> <li> <p>Formula: \\(d(p,q) = \\sum_{i=1}^{n}|p_i - q_i|\\)</p> </li> <li> <p>Cosine Similarity:</p> </li> <li>Measures cosine of angle between two vectors</li> <li>Range: [-1, 1] where 1 means vectors are identical</li> <li>Formula: \\(cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\\)</li> <li>Often converted to distance: \\(d(p,q) = 1 - cos(\\theta)\\)</li> </ol>"},{"location":"ml/KMeans/","title":"K-means Clustering","text":""},{"location":"ml/KMeans/#k-means-clustering","title":"K-means Clustering","text":""},{"location":"ml/KMeans/#process","title":"Process","text":"<ul> <li>Initialize number of k</li> <li>Randomly choose k points in the data as centroids </li> <li>While points do not change: (centroids do not change or max number of iter)<ul> <li>Assign each data point to its nearest centroid.</li> <li>Recompute the centroids of each cluster </li> </ul> </li> </ul>"},{"location":"ml/KMeans/#k","title":"K","text":""},{"location":"ml/KMeans/#optimal-k","title":"Optimal K","text":"<p>To find the optimal number of clusters (k), we use the Elbow Method: 1. Calculate the Within-Cluster Sum of Squares (WSS) for different values of k 2. Plot WSS vs k 3. Look for the \"elbow\" point - where increasing k starts giving diminishing returns 4. Choose k at this elbow point</p>"},{"location":"ml/KMeans/#advantages","title":"Advantages","text":"<ol> <li> <p>Simple and Intuitive: Easy to understand and implement; works well for basic clustering tasks; widely used in practice.</p> </li> <li> <p>Fast and Efficient: Linear time complexity O(nkd) where n is samples, k is clusters, d is dimensions; scales well with large datasets.</p> </li> <li> <p>Memory Efficient: Only stores centroids and cluster assignments; minimal memory requirements compared to other clustering methods.</p> </li> </ol>"},{"location":"ml/KMeans/#drawbacks","title":"Drawbacks","text":"<ol> <li> <p>Sensitive to Initialization: Results vary based on initial centroid positions, may get stuck in local optima; solution is to run multiple times with different initializations.</p> </li> <li> <p>Assumes Spherical Clusters: Works best with spherical, similarly sized clusters; struggles with elongated or irregular shapes; not suitable for varying densities.</p> </li> <li> <p>Requires Pre-specification of k: Number of clusters must be known beforehand; elbow method is subjective; different metrics may suggest different optimal k values.</p> </li> <li> <p>Sensitive to Outliers: Outliers can significantly influence centroid positions; may create clusters just for outliers; solution is to pre-process data.</p> </li> </ol>"},{"location":"ml/KMeans/#dbscan","title":"DBSCAN","text":"<p>DBSCAN is a density-based clustering algorithm.</p>"},{"location":"ml/KMeans/#process_1","title":"Process:","text":"<p>Parameters: \u03b5 (epsilon), n (min points)</p> <ol> <li>Start with an arbitrary point p from the dataset</li> <li>Find all points within \u03b5 radius of point p (\u03b5-neighbors)</li> <li>If number of \u03b5-neighbors \u2265 n:</li> <li>Create a new cluster</li> <li>Add point p and its \u03b5-neighbors to the cluster</li> <li>For each neighbor point:<ul> <li>Find its \u03b5-neighbors</li> <li>If \u2265 n neighbors, add them to the cluster</li> <li>Continue expanding until no more points can be added</li> </ul> </li> <li>Mark processed points as visited</li> <li>Repeat steps 1-4 with unvisited points until all points are processed</li> </ol> <p>Pros: Arbitrary cluster shapes</p> <p>Cons: Two parameters to tune and fixed \u03b5 can't handle varying densities</p>"},{"location":"ml/KMeans/#hdbscan","title":"HDBSCAN:","text":"<p>Only needs n parameter Handles varying densities Slower than k-means but more versatile Recommended as first clustering approach</p>"},{"location":"ml/KMeans/#hierarchical-clustering","title":"Hierarchical clustering","text":"<ul> <li>Agglomerative (bottom-up): Similar to flood filling, starts with individual points and merges closest pairs</li> <li>Divisive (top-down): Starts with all points in one cluster and recursively splits</li> </ul>"},{"location":"ml/KMeans/#gaussian-mixture-models-gmm","title":"Gaussian Mixture Models (GMM)","text":"<ul> <li>Soft clustering algorithm where data points can belong to multiple clusters with probability scores</li> <li>Models data as a mixture of K Gaussian distributions</li> <li>Each cluster is represented by a Gaussian distribution with its own parameters</li> </ul>"},{"location":"ml/KMeans/#key-components","title":"Key Components","text":"<ol> <li>Parameters for each Gaussian:</li> <li>Mean (\u03bc)</li> <li>Covariance matrix (\u03a3)</li> <li>Weight/mixing coefficient (\u03c0)</li> </ol>"},{"location":"ml/KMeans/#process_2","title":"Process","text":"<ol> <li>Initialization:</li> <li>Randomly initialize parameters for K Gaussians</li> <li> <p>Set initial weights, means, and covariance matrices</p> </li> <li> <p>Expectation-Maximization (EM):</p> </li> </ol> <p>a) Expectation Step (E-step):    - Calculate probability of each data point belonging to each cluster    - Compute posterior probabilities (responsibilities)</p> <p>b) Maximization Step (M-step):    - Update Gaussian parameters using weighted averages    - Recalculate means, covariances, and mixing coefficients</p> <ol> <li>Convergence:</li> <li>Repeat E-step and M-step until parameters converge</li> <li>Monitor log-likelihood for convergence criteria</li> </ol>"},{"location":"ml/KMeans/#advantages_1","title":"Advantages","text":"<ul> <li>Provides soft assignments (probabilities)</li> <li>Can model elliptical clusters</li> <li>More flexible than K-means</li> <li>Handles overlapping clusters well</li> </ul>"},{"location":"ml/KMeans/#limitations","title":"Limitations","text":"<ul> <li>Computationally more expensive than K-means</li> <li>Sensitive to initialization</li> <li>May converge to local optima</li> </ul>"},{"location":"ml/KNN/","title":"k-nearest neighbor (KNN)","text":""},{"location":"ml/KNN/#k-nearest-neighbor-knn","title":"k-nearest neighbor (KNN)","text":"<p>It is non-parametric learning algorithm. It is mainly used for classification but also can be used for regression by averaging out the nearest value based on distance.</p>"},{"location":"ml/KNN/#process","title":"Process","text":"<ol> <li>Choose the number of k and a distance metric(Euclidean, Manhattan, Cosine etc.)</li> <li>Find the k-nearest neighbors of the data record that we want to classify</li> <li>Assign the class label by majority vote</li> </ol> <p>The right choice of k is crucial to finding a good balance between overfitting and underfitting.</p>"},{"location":"ml/KNN/#k","title":"k","text":"<p><code>Effect of k:</code> As k increases, variance decreases while bias increases. Conversely, as k decreases, variance increases while bias decreases.</p> <p><code>Choice of k:</code> Choosig based on validation error:</p>"},{"location":"ml/KNN/#key-points","title":"Key Points","text":"<ul> <li>It is a memory-based approach immediately adapts as we collect new training data. </li> <li>The computational complexity for classifying new examples grows linearly with the number of examples in the training dataset in the worst-case scenario.</li> <li>KNN is very susceptible to overfitting due to the curse of dimensionality (the closest neighbors as being too far away in a high-dimensional space to give a good estimate.). Regularization method cannot be applied here.</li> <li>All the features should be scaled as we will be taking distnace based on features.</li> <li>Optimization can be done through the dimensionality reduction by using method like PCA, LDA etc.</li> </ul>"},{"location":"ml/LDA/","title":"Linear Discriminant Analysis (LDA)","text":""},{"location":"ml/LDA/#linear-discriminant-analysis-lda","title":"Linear Discriminant Analysis (LDA)","text":"<p>LDA is a supervised dimensionality reduction and classification algorithm. Unlike PCA which focuses on maximizing variance, LDA aims to find a linear combination of features that maximizes class separation while minimizing within-class variance.</p>"},{"location":"ml/LDA/#within-class-scatter-matrix-sw","title":"Within-Class Scatter Matrix (SW)","text":"<p>Measures the variance within each class: \\(S_W = \\sum_c S_c\\)</p> <p>Where for each class c: \\(S_c = \\sum_{i \\in c} (x_i - \\bar{x}_c) \\cdot (x_i - \\bar{x}_c)^T\\)</p>"},{"location":"ml/LDA/#between-class-scatter-matrix-sb","title":"Between-Class Scatter Matrix (SB)","text":"<p>Measures the variance between different classes: \\(S_B = \\sum_{c} n_c \\cdot (\\bar{x}_c - \\bar{x}) \\cdot (\\bar{x}_c - \\bar{x})^T\\)</p> <p>Where: - \\(n_c\\) is the number of samples in class c - \\(\\bar{x}_c\\) is the mean of class c - \\(\\bar{x}\\) is the overall mean</p>"},{"location":"ml/LDA/#fishers-linear-discriminant","title":"Fisher's Linear Discriminant","text":""},{"location":"ml/LDA/#objective-function","title":"Objective Function","text":"<p>To maximize class separation while minimizing within-class variance, we use Fisher's criterion: \\(J(w) = \\frac{w^TS_Bw}{w^TS_Ww}\\)</p> <p>Taking the derivative with respect to w and setting it to zero: \\(\\frac{\\partial}{\\partial w}J(w) = \\frac{2S_B w(w^T S_W w) - 2S_W w(w^T S_B w)}{(w^T S_W w)^2} = 0\\)</p> <p>Simplifying: \\(S_B w(w^T S_W w) = S_W w(w^T S_B w)\\)</p> <p>\\(S_W^{-1}S_B w = \\frac{(w^T S_B w)}{(w^T S_W w)} w\\)</p> <p>\\(S_W^{-1}S_B w = \\lambda w\\)</p>"},{"location":"ml/LDA/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Calculate the within-class scatter matrix (\\(S_W\\))</li> <li>Calculate the between-class scatter matrix (\\(S_B\\))</li> <li>Compute eigenvalues and eigenvectors of \\(S_W^{-1}S_B\\)</li> <li>Select top k eigenvectors based on eigenvalues</li> <li>Transform the data using selected eigenvectors</li> </ol>"},{"location":"ml/LDA/#advantages","title":"Advantages","text":"<ul> <li>Preserves class discriminatory information</li> <li>Reduces dimensionality while maintaining class separation</li> <li>Works well for normally distributed classes</li> </ul>"},{"location":"ml/LDA/#limitations","title":"Limitations","text":"<ul> <li>Assumes normal distribution of features</li> <li>May fail if within-class covariance is not equal across classes</li> <li>Limited by the number of classes (max components = number of classes - 1)</li> </ul>"},{"location":"ml/LinearRegression/","title":"Linear Regression","text":""},{"location":"ml/LinearRegression/#linear-regression","title":"Linear Regression","text":"<p>Linear Regression would be appropriate since we are predicting a continuous value.</p> <p>Linear Regression works when these 4 assumtion being followed:</p> <ol> <li> <p>Linearity: this means that the relationship must be linear between the independent variables and dependent variables.  \\(y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) or \\(y= \\beta_0 + \\beta_1 sin(x) + \\beta_2 cosx(x)\\)</p> </li> <li> <p>Homoscedasticity: Constant variance of residuals. </p> </li> <li> <p>Independence: independent variables (observations) are not highly correlated.</p> </li> <li> <p>Normality: Residuals are normally distributed for any fixed value of our observations </p> </li> </ol> <p>Note</p> <ul> <li> <p>Find the collinearity by using Variance Inflation Factors (VIF). VIF &gt; 5 variable are dependent.</p> </li> <li> <p>Solve collinearity by either removing one of the features or linearly combine both features. </p> </li> </ul>"},{"location":"ml/LinearRegression/#metrics","title":"Metrics","text":"<ul> <li> <p>Root Mean Square Error (RMSE) : Calculates the average of the squared difference between the predicted and actual values. Thus, larger errors (outliers or poor prediction) are flagged  more than when using MAE due to squaring errors. \\(RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2}{N}}\\)</p> </li> <li> <p>Mean Absolute Error (MAE) : Calculates the average of the absolute difference between the predicted and actual values. As a result, it does not punish large errors as much as RMSE. \\(MAE = \\frac{\\sum_{i=1}^{N}|y_i - \\hat{y_i}|}{N}\\)</p> </li> </ul>"},{"location":"ml/LinearRegression/#methods","title":"Methods","text":"<ol> <li> <p>Closed form solution</p> <ul> <li>\\(XW=y\\)</li> <li>\\(X^TXW=X^Ty\\)</li> <li>\\(w = (X^TX)^{-1}X^Ty\\)</li> <li>Useful, when optimal solution is needed. Issue when inverse does notexist and computationally expensive when data is too large.</li> </ul> </li> <li> <p>Optimization algorithm, typically Gradient Descent (GD) or Stochastic Gradient Descent (SGD).</p> <ul> <li>\\(\\text{L} = \\frac{1}{2} ||\\hat{y} - y||^2\\) where \\(\\hat{y} = X*W + b\\) </li> <li>\\(\\frac{\\partial L}{\\partial W} = X*(\\hat{y}- y)\\)</li> <li>\\(\\frac{\\partial L}{\\partial W} = \\hat{y}- y\\)</li> </ul> </li> </ol>"},{"location":"ml/LinearRegression/#feature-importance","title":"Feature Importance","text":"<p>If the features are normalized then the coefficients are an indication of feature importance, i.e. features with higher coefficients are more useful for  prediction.</p>"},{"location":"ml/LinearRegression/#prediction","title":"Prediction","text":"<p>\\(y = \\sum_{i}w_ix_i + b\\)</p>"},{"location":"ml/LinearRegression/#code","title":"Code","text":"<p>Numpy</p> Closed FormGradient Form <pre><code>class LinearRegressionClosedForm:\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        bias = np.ones((n_samples, 1))\n        X_new = np.column_stack((X, bias))\n        W = np.linalg.inv(X_new.T @ X_new) @ X_new.T @ y\n        self.weights = W[:-1]\n        self.bias = W[-1]\n\n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n</code></pre> <pre><code>class LinearRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # gradient descent\n        for _ in range(self.n_iters):\n            y_predicted = np.dot(X, self.weights) + self.bias\n            # compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n\n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n</code></pre> <p>Pytorch</p> Parameter BasedLinear Layer Based <pre><code>class LinearRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegression, self).__init__()\n        self.weights = nn.Parameter(torch.randn(input_dim, output_dim, requires_grad=True))\n        self.bias = nn.Parameter(torch.randn(1, output_dim, requires_grad=True))\n\n    def forward(self, x):\n        return  torch.matmul(x, self.weights) + self.bias\n</code></pre> <pre><code>class LinearRegressionV2(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionV2, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return  self.linear(x)\n</code></pre>"},{"location":"ml/LogisticRegression/","title":"Logistic Regression","text":""},{"location":"ml/LogisticRegression/#logistic-regression","title":"Logistic Regression","text":"<p>It is used for te classification problem. It uses linear regressing equaition to predict the class probabilities.</p> <p>Equation:</p> <p>\\(y = wx+b\\)</p> <p>This \\(y\\) is feed to the sigmoid function to get the output between 0 and 1 as probailities. So,</p> <p>\\(y = \\frac{1}{1+ e^{-(wx+b)}}\\)</p> <p>Logistic regression doesn't require: - Normality of residuals - Homoscedasticity</p> <p>Logistic regression specifically requires: - Binary/categorical outcome - Linear relationship with log odds (not the outcome itself)</p>"},{"location":"ml/LogisticRegression/#effect-of-outlier","title":"Effect of Outlier","text":"<p>Since here we focus on finding the decision boundry that linearly seperate the classes. So we mostly focus on the points which are closer to the boundry. Therefore, outlier will have very less effect here.</p>"},{"location":"ml/LogisticRegression/#logistic-regression-as-maximum-likelihood-estimationmle","title":"Logistic Regression as Maximum Likelihood Estimation(MLE)","text":"<p>Assuming the Bernoulli distribution (i.e., binary classification). Let, \\(y \\in \\{0,1\\}\\) if \\(p\\) is the probability of class as 1. Then according to MLE we need to maximize \\(p^y\\) if class is 1 and \\((1-p)^{(1-y)}\\) if class is 0.</p> <p>\\(L = \\Pi_{i=1}^{N} p_i^{y_i} (1-p_i)^{(1-y_i)}\\)</p> <p>Multipying such large number may result in the overflow. So take <code>log</code> on both side.</p> <p>\\(L = \\sum_{i=1}^{N} (y_i \\ln p_i + (1 - y_i) \\ln (1 - p_i))\\)</p> <p>\\(Loss = -Likelihood\\)</p> <p>This loss penelizes much more than MSE when prediciton is wrong.</p>"},{"location":"ml/LogisticRegression/#prediction","title":"Prediction","text":"<p>Here, \\(pred = \\frac{1}{1+ e^{-(wx+b)}}\\), if \\(pred &gt; \\tau\\) then class 1 else class 0.</p> <p>\\(\\tau\\) is decided according to problem statement.</p>"},{"location":"ml/LogisticRegression/#multi-class-n","title":"Multi-Class (N)","text":"<ol> <li> <p>One-vs-all: We need to have N models </p> <p>\\(pred = \\text{argmax}_{i} f_{i}(x)\\)</p> </li> <li> <p>One-vs-one: We need to have \\(\\binom{N}{2}\\) models, where each model is trained to distinguish between a pair of classes. For N classes, this results in \\(\\frac{N(N-1)}{2}\\) binary classifiers. The prediction is made by majority voting across all pairwise comparisons:</p> <p>\\(pred = argmax_{i} \\sum_{j \\neq i} \\mathbb{I}(f_{ij}(x) = i)\\)</p> <p>where \\(f_{ij}(x)\\) is the binary classifier for classes i and j, and \\(\\mathbb{I}\\) is the indicator function.</p> </li> <li> <p>Mathematical: Use softmax instead of sigmoid and use cross-entropy loss.</p> </li> </ol>"},{"location":"ml/LogisticRegression/#code","title":"Code","text":"NumpyPyTorch <pre><code>class LogisticRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # gradient descent\n        for _ in range(self.n_iters):\n            # approximate y with linear combination of weights and x, plus bias\n            linear_model = np.dot(X, self.weights) + self.bias\n            # apply sigmoid function\n            y_predicted = self._sigmoid(linear_model)\n\n            # compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights) + self.bias\n        y_predicted = self._sigmoid(linear_model)\n        y_predicted_cls = [1 if i &gt; 0.5 else 0 for i in y_predicted]\n        return np.array(y_predicted_cls)\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n</code></pre> <pre><code>class LogisticRegression(nn.Module):\n    def __init__(self, input_features):\n        super(LogisticRegression, self).__init__()\n        self.layer1 = nn.Linear(input_features, 8)\n        self.layer2 = nn.Linear(8, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.bn = nn.BatchNorm1d(8)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = torch.sigmoid(self.layer2(x))\n        return x\n</code></pre>"},{"location":"ml/MATH/","title":"Co-Variance:","text":""},{"location":"ml/MATH/#co-variance","title":"Co-Variance:","text":"<p>Covariance is a measures the extent two features either increase or decrease with each other (range ( \u2212 \u221e , \u221e ) ). A covariance score of 0 indicates that both features are not related. If the covariance score is positive it means that both features increase in the same direction and a negative score indicates an inverse relationship between the two features. </p> <p>A Covariance matrix is used in PCA, Gaussian mixture models (GMMs) and Mahalanobis Distance. </p>"},{"location":"ml/MATH/#correlation","title":"Correlation","text":"<p>Correlation lets us know the strength and direction of the two features (range ( \u2212 1 , 1 ) ). If we say two features are correlated, then we can say that a change in one feature creates an impact/change in another variable. A positive correlation indicates that as one feature increases the other will increase, a correlation score of 0 indicates no relationship between variables and a negative correlation indicates that as one feature increases the other will  decrease. </p> <p>Correlation is just in large amounts of data is to find patterns, e.g. correlated features within a dataset. </p>"},{"location":"ml/MATH/#bayes-equation","title":"Bayes Equation","text":""},{"location":"ml/MATH/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":""},{"location":"ml/Metrices/","title":"METRICS","text":""},{"location":"ml/Metrices/#metrics","title":"METRICS","text":""},{"location":"ml/Metrices/#content","title":"Content","text":"<ol> <li>Basics</li> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>PR-Curve</li> <li>F1-Score</li> <li>ROC</li> <li>Pixel Accuracy</li> <li>IoU</li> <li>Dice Coefficient</li> <li>NMS</li> </ol>"},{"location":"ml/Metrices/#basics","title":"Basics","text":"<ul> <li>True Positives (TP): actual observation is 1 (True) and prediction is 1 (True) </li> <li>True Negative (TN): actual observation is 0 (False) and prediction is 0 (False) </li> <li>False Positive (FP): actual observation is 0 (False) and prediction is 1 (True) </li> <li>False Negative (FN): actual observation is 1 (True) and prediction is 0 (False) </li> </ul>"},{"location":"ml/Metrices/#accuracy","title":"Accuracy","text":"<p><code>Balanced</code></p>"},{"location":"ml/Metrices/#precision","title":"Precision","text":"<p><code>Balanced, Imbalanced</code></p> <p>Out of all the True <code>predictions</code> how many were actually True.</p> <p>\\(Precision = \\frac{TP}{TP+FP}\\)</p>"},{"location":"ml/Metrices/#recall","title":"Recall","text":"<p><code>Balanced, Imbalanced</code></p> <p>Out of all the True <code>observations</code> how many were actually True. \\(Recall = \\frac{TP}{TP+FN}\\)</p>"},{"location":"ml/Metrices/#pr-curve","title":"PR Curve","text":""},{"location":"ml/Metrices/#f1-score","title":"F1-score","text":"<p><code>Balanced, Imbalanced</code></p> <p>Harmonic Mean of Precision(P) and Recall(R)</p> <p>\\(F1-score = \\frac{2*PR}{P+R}\\)</p>"},{"location":"ml/Metrices/#receiver-operating-characteristic-roc","title":"Receiver Operating Characteristic (ROC)","text":"<p>The ROC is a probability curve and the area under  the curve can be thought of as the degree of separability between the two classes. </p>"},{"location":"ml/Metrices/#pixel-accuracy","title":"Pixel Accuracy","text":"<p>A metric that calculates the percentage of correctly classified pixels in an image.</p> <p>\\(Pixel Accuracy = \\frac{Number\\ of\\ Correctly\\ Classified\\ Pixels}{Total\\ Number\\ of\\ Pixels}\\)</p> <p>Limitation: Can be misleading for imbalanced datasets. For example, if an image has only 2 pixels of interest out of 100 pixels, predicting no objects would still result in 98% accuracy.</p>"},{"location":"ml/Metrices/#intersection-over-union-iou","title":"Intersection over Union (IoU)","text":"<p>A more robust metric that measures the overlap between predicted and ground truth segments.</p> <p>\\(IoU = \\frac{Area\\ of\\ Overlap}{Area\\ of\\ Union} = \\frac{Intersection}{Union}\\)</p> <p>Where: - Intersection = Area shared between prediction and ground truth - Union = Total area covered by both prediction and ground truth</p> <p>If two bbox ahve both Intersection and union high then the IoU will be high. Assume two case :</p> <ol> <li>Intersion is large but the bbox are also very large bbox this leads to very high union depicting same same object.</li> <li>Intersion is large but the bbox are small bbox then possibly they are dipicting different object.</li> </ol>"},{"location":"ml/Metrices/#dice-coefficient","title":"Dice Coefficient","text":"<p>Similar to IoU but less harsh on misclassifications. It has a monotonic relationship with IoU.</p> <p>\\(Dice = \\frac{2 \\times Area\\ of\\ Overlap}{Sum\\ of\\ Areas} = \\frac{2 \\times Intersection}{Area\\ of\\ Prediction + Area\\ of\\ Ground\\ Truth}\\)</p> <p>Relationship with IoU: \\(Dice = \\frac{2 \\times IoU}{1 + IoU}\\)</p>"},{"location":"ml/Metrices/#non-maxium-supression-nms","title":"Non-Maxium Supression (NMS)","text":"<p>We  don't need all the object proposals. We only want to keep the best one.</p>"},{"location":"ml/Metrices/#nms","title":"\u03bb<sub>NMS</sub>","text":"<p>Do not allow the bbos if they are overlapping more than \\(\\lambda_{NMS}\\) threshold.</p> <p>Narrrow Threshold (High IoU) : Low Precision (More False Positive) Wide Threshold (Low IoU): Low Recall (More False Negative)</p>"},{"location":"ml/NaiveBayes/","title":"Naive Bayes","text":""},{"location":"ml/NaiveBayes/#naive-bayes","title":"Naive Bayes","text":"<p>Naive Bayes classifiers have a general assumption that the effect of an attribute value on a given class in independent of the values of the other attributes. This assumption is called class-conditional independence.</p> <p>\\(P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\\)</p> <p>\\(\\text{posterior} = \\frac{\\text{likelihood prob} * \\text{prior}}{\\text{marginal}}\\)</p> <p>In the context of Naive Bayes classification, we often work with log probabilities to avoid numerical underflow. The log posterior probability is calculated as:</p> <p>\\(\\log P(A|B) = \\log P(B|A) + \\log P(A) - \\log P(B)\\)</p> <p>The denominator \\(P(B)\\) is omitted since it's constant across classes and doesn't affect the \\(\\text{argmax}\\)</p> <p>\\(\\log P(A|B) = \\log P(B|A) + \\log P(A)\\)</p> <p>\\(posterior = posterior + prior\\)</p> <p>This matches the implementation shown in the code where: - \\(\\log P(A)\\) is the prior probability - \\(\\log P(B|A)\\) is calculated as the sum of log probabilities from the PDF</p>"},{"location":"ml/NaiveBayes/#additional-details","title":"Additional Details","text":"<p>Prior Probability[P(A)]: The prior probability represents our initial belief about the probability of each class before seeing any evidence. It's calculated by dividing the number of instances of a particular class by the total number of instances in the training dataset.</p> <p>Likelihood Probability[P(B|A)]: This represents the probability of observing the features given a particular class. It measures how likely we are to see these features if the class is true.</p> <p>Marginal Probability[P(B)]: This is the probability of observing the features regardless of the class. It acts as a normalizing constant to ensure our probabilities sum to 1.</p> <p>Posterior Probability[P(A|B)]: This is our final probability of a class given the observed features. It's calculated by multiplying the likelihood by the prior and dividing by the marginal probability.</p> <p>\\(P(x|c) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}} e^{-\\frac{(x-\\mu_c)^2}{2\\sigma_c^2}}\\)</p> <p>we assume that the data follows Gaussian distribution, due  to which the probability of an item, given a class label c, can be defined as</p>"},{"location":"ml/NaiveBayes/#advantages","title":"Advantages","text":"<ol> <li>Simple and fast to train and predict</li> <li>Works well with small datasets</li> <li>Can handle both continuous and discrete features</li> <li>Less prone to overfitting</li> </ol>"},{"location":"ml/NaiveBayes/#disadvantages","title":"Disadvantages","text":"<ol> <li>Assumes features are independent (naive assumption)</li> <li>May not perform well when features are correlated</li> <li>Requires features to be normally distributed for optimal performance</li> <li>Sensitive to feature scaling</li> </ol>"},{"location":"ml/NaiveBayes/#laplacian-smoothing","title":"Laplacian Smoothing","text":"<p>Laplacian smoothing (also known as additive smoothing) is a technique used to handle zero probability problems in Naive Bayes classification. It adds a small constant \u03b1 to the count of each feature-class combination to prevent zero probabilities.</p> <p>For a feature value x and class c, the smoothed probability is calculated as:</p> <p>\\(P(x|c) = \\frac{count(x,c) + \\alpha}{count(c) + \\alpha|V|}\\)</p> <p>Where</p> <ul> <li>count(x,c) is the number of times feature x appears with class c</li> <li>count(c) is the number of instances of class c</li> <li>\u03b1 is the smoothing parameter (typically 1)</li> <li>|V| is the size of the vocabulary (number of unique feature values)</li> </ul>"},{"location":"ml/NaiveBayes/#question","title":"Question","text":"Why Naive Bayes is called Naive? <ul> <li>Simplified assumption that all features in a dataset are independent of each other, even though in real-world scenarios this is rarely true.</li> <li>\\(P(y|x\u2081,x\u2082,...,x\u2099) = \\frac{P(x\u2081|y) \u00d7 P(x\u2082|y) \u00d7 ... \u00d7 P(x\u2099|y) \u00d7 P(y)}{P(X)} = \\frac{P(y) \\prod_{i=1}^{n} P(x_i|y)}{P(X)}\\)</li> </ul>"},{"location":"ml/NaiveBayes/#code","title":"Code","text":"NumpyPyTorch <pre><code>class NaiveBayes:\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self._classes = np.unique(y)\n        n_classes = len(self._classes)\n\n        # calculate mean, var, and prior for each class\n        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._priors = np.zeros(n_classes, dtype=np.float64)\n\n        for idx, c in enumerate(self._classes):\n            X_c = X[y == c]\n            self._mean[idx, :] = X_c.mean(axis=0)\n            self._var[idx, :] = X_c.var(axis=0)\n            self._priors[idx] = X_c.shape[0] / float(n_samples)\n\n    def predict(self, X):\n        y_pred = [self._predict(x) for x in X]\n        return np.array(y_pred)\n\n    def _predict(self, x):\n        posteriors = []\n\n        # calculate posterior probability for each class\n        for idx, c in enumerate(self._classes):\n            prior = np.log(self._priors[idx])\n            posterior = np.sum(np.log(self._pdf(idx, x)))\n            posterior = prior + posterior\n            posteriors.append(posterior)\n\n        # return class with highest posterior probability\n        return self._classes[np.argmax(posteriors)]\n\n    def _pdf(self, class_idx, x):\n        mean = self._mean[class_idx]\n        var = self._var[class_idx]\n        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n        denominator = np.sqrt(2 * np.pi * var)\n        return numerator / denominator\n</code></pre> <pre><code>\n</code></pre>"},{"location":"ml/PCA/","title":"PCA","text":""},{"location":"ml/PCA/#pca","title":"PCA","text":"<p>High dimensional data could be a problem:  - An obvious reason would be more computing power/time needed to build a model  - The curse of dimensionality: When the number of dimensions (features) increases our model starts to become more complex and dependent on the data it was trained on and so it overfits thus reducing our model performance  - For models that use distance metrics, when the number of dimensions is too high, then each datapoint seems very similar to each other. This is because, with very large features, the distance between two data points are almost equal since they are all very far from each other. </p> <p>Method to resolve : PCA, LDA, t-SNE(non-linear correlation), AutoEncoder</p>"},{"location":"ml/PCA/#pca_1","title":"PCA","text":"<p>It is <code>unspervised machine learning</code></p> <p>Assumption:</p> <p>PCA needs a linear correlation  between all variables  (It should not have  non-linear correlations )</p> <p>It is a linear combination of variables that results in a line or axis/axes that explain a maximal amount of variance from the original dataset. More formally, the eigenvectors of the covariance matrix (of the data) are the principal components and the eigenvalues represent the amount of variance carried in each principal component. </p> <p>If the eigenvectors are, all the same, PCA would not be able to select which principal component since we select the top n eigenvectors and there would be no top n since they are all equal. </p> <p>It is not necessary to remove variables that are highly correlated because PCA would project all the correlated variables onto the same principal  component. </p> <p>Choose number of principle component such that it captures 95-99% or the variance.</p>"},{"location":"ml/PCA/#process","title":"Process","text":"<ol> <li>Standardize our data (since, PCA is sensitive to the variance within features.)</li> <li>Calculate the covariance matrix </li> <li>Then using this matrix we calculate eigenvectors and eigenvalues and thus the principal components from the eigenvectors</li> </ol>"},{"location":"ml/PCA/#drawback","title":"Drawback","text":"<ol> <li>Computationally expensive </li> <li>Information is always lost </li> <li>Explainability becomes much more difficult.</li> </ol>"},{"location":"ml/SVM/","title":"Support Vector Machine (SVM)","text":""},{"location":"ml/SVM/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<ul> <li>SVM is a supervised learning algorithm used for classification and regression tasks. </li> <li>SVM finds an optimal hyperplane that maximizes the margin between different classes in an N-dimensional space. While there may be multiple hyperplanes that can separate two classes of data points, SVM specifically aims to find the one with the maximum distance (margin) between the data points of both classes.</li> </ul>"},{"location":"ml/SVM/#mathematical-formulation","title":"Mathematical Formulation","text":""},{"location":"ml/SVM/#linear-model","title":"Linear Model","text":"<p>Let it be a binary classification with \\(y \\in \\{1, -1\\}\\)</p> <p>\\(w \\cdot x_i - b = 0\\) (optimal hyperplane)</p> <p>\\(w \\cdot x_i - b \\ge 1\\) if \\(y_i = 1\\)    - (i)</p> <p>\\(w \\cdot x_i - b \\le 1\\) if \\(y_i = -1\\)   - (ii)</p> <p>Combining (i) &amp; (ii) we get</p> <p>\\(y_i (w \\cdot x_i - b) \\ge 1\\)</p> <p>Hinge Loss</p> <p>\\(L(y, \\hat{y}) = max(0, 1 - y \\cdot \\hat{y})\\)</p> <ul> <li>The loss is 0 when prediction is correct (same sign as true label)</li> <li>The loss increases linearly when prediction is incorrect.</li> </ul> <p>Now adding regularization to above, the final loss function becomes:</p> <p>\\(J(w,b) = \\frac{1}{n}\\sum_{i=1}^n max(0, 1 - y_i(w^T x_i + b)) + \\frac{\\lambda}{2}||w||^2\\)</p> <p>\\(\\lambda\\) is the regularization parameter</p> <pre><code>Above equation can be seen as contrained optimization:\n    - minimize: $\\frac{1}{2}||w||^2$\n    - subject to: $y_i(w^T x_i + b) \\geq 1$ for all $i$\n</code></pre>"},{"location":"ml/SVM/#optimization","title":"Optimization","text":"<p>To optimize for above loss we need to compute gradient wrt to parameters and then update them.</p> <p>\\(\\frac{\\partial J}{\\partial w} = \\lambda w - \\frac{1}{n}\\sum_{i=1}^n y_i x_i \\cdot I(y_i(w^T x_i + b) &lt; 1)\\)</p> <p>\\(\\frac{\\partial J}{\\partial b} = -\\frac{1}{n}\\sum_{i=1}^n y_i \\cdot I(y_i(w^T x_i + b) &lt; 1)\\)</p> <p>where,  \\(I()\\) is the indicator function.</p> <p>Gradient Updates:</p> <ol> <li> <p>No misclassification (\\(y_i(w^T x_i + b) \\geq 1\\)):    \\(w = w - \\eta \\lambda w\\)</p> </li> <li> <p>Misclassification (\\(y_i(w^T x_i + b) &lt; 1\\)):    \\(w = w - \\eta(\\lambda w - y_i x_i)\\) \\(b = b - \\eta(-y_i)\\)</p> </li> </ol> <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"ml/SVM/#dual-form","title":"Dual Form","text":"<p>The dual form of the optimization problem is:</p> <p>For classification: maximize: \\(\\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\)</p> <p>subject to:  - \\(\\sum_{i=1}^n \\alpha_i y_i = 0\\) - \\(\\alpha_i \\geq 0\\) for all \\(i\\)</p> <p>The dual form is particularly useful because: - It allows us to use the kernel trick - Only support vectors (points with \\(\\alpha_i &gt; 0\\)) contribute to the decision boundary - It's often more efficient to solve than the primal form</p>"},{"location":"ml/SVM/#kernel-trick","title":"Kernel Trick","text":"<p>For non-linear classification and regression, we can use kernel functions to implicitly map data to higher dimensions without explicitly computing the transformation:</p> <p>Common kernels: - Linear: \\(K(x,y) = x^T y\\) - Polynomial: \\(K(x,y) = (x^T y + 1)^d\\) where \\(d\\) is the degree of the polynomial kernel. - RBF: \\(K(x,y) = exp(-\\gamma ||x-y||^2)\\),  where \\(\\gamma = \\frac{1}{2\\sigma^2}\\) is the inverse of twice the variance of the Gaussian distribution. </p> <p>The kernel trick works because: - It allows us to compute dot products in high-dimensional space without explicitly mapping to that space - It's computationally efficient - It enables SVM to find non-linear decision boundaries and regression functions</p>"},{"location":"ml/SVM/#soft-margin-svm","title":"Soft Margin SVM","text":"<p>For non-linearly separable data, we introduce slack variables \\(\\xi_i\\) to allow some misclassification:</p> <p>minimize: \\(\\frac{1}{2}||w||^2 + C\\sum_{i=1}^n \\xi_i\\)</p> <p>subject to: \\(y_i(w^T x_i + b) \\geq 1 - \\xi_i\\) for all \\(i\\)</p> <p>where: - \\(\\xi_i\\) represents how far a point is from its correct margin</p>"},{"location":"ml/SVM/#advantages","title":"Advantages","text":"<ul> <li>Excellent for high-dimensional data where dimensions &gt; number of samples</li> <li>Creates clear decision boundaries through max-margin classification</li> <li>Memory efficient by only storing support vectors</li> <li>Versatile through kernel functions for non-linear classification and regression</li> <li>Good generalization due to margin maximization</li> <li>Robust to outliers in regression tasks</li> </ul>"},{"location":"ml/SVM/#disadvantages","title":"Disadvantages","text":"<ul> <li>Performs poorly with overlapping classes due to difficulty in finding clear margins</li> <li>Does not provide direct probability estimates (requires N-fold cross-validation)</li> <li>Computationally expensive for large datasets</li> <li>Requires careful feature scaling</li> <li>Memory requirements grow with the number of support vectors</li> <li>Sensitive to the choice of kernel parameters</li> </ul>"},{"location":"ml/TODO/","title":"TODO","text":"<ul> <li> Derivation for closed-form and SGD case linear Regression.</li> <li> Write Code for multiclass, SGD logistic regression.</li> <li> Variance Inflation Factors (VIF)</li> <li> (Overfitting) Reduce the number of features: We can use some feature selection methods \u2013 filter-based (chi-square), wrapper based (Recursive Feature Elimination) or embedded  like Lasso regularization. </li> <li> SMOTE(data imbalance)</li> <li>number of principle component (PCA)</li> <li> <p> GMM code</p> </li> <li> <p>Complete SVR in SVM etc</p> </li> </ul>"},{"location":"ml/Temp/","title":"Temp","text":""},{"location":"ml/Temp/#linear-svm-for-regression","title":"Linear SVM for Regression","text":"<p>For regression tasks, SVM aims to find a function that deviates from the actual values by at most \\(\\epsilon\\) while being as flat as possible. The optimization problem becomes:</p> <p>minimize: \\(\\frac{1}{2}||w||^2 + C\\sum_{i=1}^n (\\xi_i + \\xi_i^*)\\)</p> <p>subject to: - \\(y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i\\) - \\((w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i^*\\) - \\(\\xi_i, \\xi_i^* \\geq 0\\)</p> <p>where: - \\(\\epsilon\\) is the maximum allowed deviation - \\(\\xi_i, \\xi_i^*\\) are slack variables for points above and below the tube - \\(C\\) controls the trade-off between flatness and deviation tolerance</p>"},{"location":"ml/Temp/#hard-vs-soft-margin","title":"Hard vs Soft Margin","text":"<p>SVM can be implemented in two ways based on how strictly we enforce the classification constraints:</p> <ol> <li>Hard Margin SVM:</li> <li>Enforces strict classification with the constraint: \\(y_i(w^T x_i + b) - 1 \\geq 0\\)</li> <li>Requires perfect separation of classes</li> <li>More prone to overfitting</li> <li> <p>Only works with linearly separable data</p> </li> <li> <p>Soft Margin SVM:</p> </li> <li>Introduces slack variables \\(\\epsilon_i\\) to allow some misclassification</li> <li>Modified constraint: \\(y_i(w^T x_i + b) - 1 \\geq \\epsilon_i\\)</li> <li>\\(\\epsilon_i\\) indicates how much the constraint can be violated for the i-th point</li> <li>More robust to noise and outliers</li> <li>Works with non-linearly separable data</li> <li>Balances margin maximization with classification errors through regularization</li> </ol>"},{"location":"ml/Temp/#dual-form","title":"Dual Form","text":"<p>The dual form of the optimization problem is:</p> <p>For classification: maximize: \\(\\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\)</p> <p>subject to:  - \\(\\sum_{i=1}^n \\alpha_i y_i = 0\\) - \\(\\alpha_i \\geq 0\\) for all \\(i\\)</p> <p>For regression: maximize: \\(-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*)x_i^T x_j + \\sum_{i=1}^n y_i(\\alpha_i - \\alpha_i^*) - \\epsilon\\sum_{i=1}^n (\\alpha_i + \\alpha_i^*)\\)</p> <p>subject to: - \\(\\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) = 0\\) - \\(0 \\leq \\alpha_i, \\alpha_i^* \\leq C\\)</p> <p>The dual form is particularly useful because: - It allows us to use the kernel trick - Only support vectors (points with \\(\\alpha_i &gt; 0\\)) contribute to the decision boundary - It's often more efficient to solve than the primal form</p>"},{"location":"ml/Temp/#training-algorithm","title":"Training Algorithm","text":"<ol> <li>Initialize parameters:</li> <li>Set learning rate \\(\\eta\\)</li> <li>Set regularization parameter \\(C\\)</li> <li> <p>Initialize weights \\(w\\) and bias \\(b\\)</p> </li> <li> <p>For each training iteration:</p> </li> <li> <p>For each sample \\((x_i, y_i)\\):</p> <ul> <li>Calculate prediction: \\(\\hat{y} = w^T x_i + b\\)</li> <li>Update weights if margin violation:    \\(w = w - \\eta(2\\lambda w - y_i x_i)\\) \\(b = b - \\eta(-y_i)\\)  where \\(\\lambda = \\frac{1}{C}\\)</li> </ul> </li> <li> <p>Return final model parameters \\(w\\) and \\(b\\)</p> </li> </ol>"},{"location":"ml/Temp/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Use gradient descent or quadratic programming for optimization</li> <li>Consider using libraries like libsvm for large-scale problems</li> <li>Normalize features before training</li> <li>Cross-validate to find optimal parameters</li> <li>Consider using stochastic gradient descent for large datasets</li> <li>Monitor the number of support vectors to prevent overfitting</li> <li>For regression, tune \\(\\epsilon\\) and \\(C\\) parameters carefully</li> </ul>"},{"location":"nlp/","title":"Natural Language Procesing","text":""}]}