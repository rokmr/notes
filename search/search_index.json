{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#notes","title":"Notes","text":""},{"location":"#explore-and-star-on-github","title":"Explore and star on GitHub","text":"<ul> <li> <p> rokmr/kick-start-ai</p> </li> <li> <p> rokmr/machine-learning</p> </li> <li> <p> rokmr/computer-vision</p> </li> <li> <p> rokmr/notes</p> </li> </ul>"},{"location":"basics/","title":"Basic","text":""},{"location":"basics/#basics","title":"Basics","text":""},{"location":"basics/#convex-and-concave-function","title":"Convex and Concave function","text":"<p>Convex Functions</p> <p>A function \\(f(x)\\) is convex if the line segment between any two points on the graph lies above or on the graph.</p> <p>For all \\(x_1, x_2\\) in the domain and \\(t \\in [0,1]\\):</p> \\[f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2)\\] <p>The graph <code>curves upward</code></p> <p>Note</p> <ul> <li>Sum of convex functions is convex</li> <li>\\(f(E[X]) \\leq E[f(X)]\\)</li> <li>f is convex \u27fa f''(x) \u2265 0</li> <li>\\(g(f(x))\\) is convex if:<ul> <li>\\(g\\) is convex and non-decreasing, \\(f\\) is convex</li> <li>\\(g\\) is convex and non-increasing, \\(f\\) is concave</li> </ul> </li> <li>Examples: \\(x^2\\), \\(e^x\\), \\(|x|\\)</li> </ul> <p>Concave Functions</p> <p>A function \\(f(x)\\) is concave if the line segment between any two points on the graph lies below or on the graph.</p> <p>For all \\(x_1, x_2\\) in the domain and \\(t \\in [0,1]\\):</p> \\[f(tx_1 + (1-t)x_2) \\geq tf(x_1) + (1-t)f(x_2)\\] <p>The graph <code>curves downward</code></p> <p>Note</p> <ul> <li>Sum of concave functions is concave</li> <li>\\(f(E[X]) \\geq E[f(X)]\\)</li> <li>f is concave \u27fa f''(x) \u2264 0</li> <li>Examples: \\(\\log(x)\\), \\(\\sqrt{x}\\), \\(-x^2\\)</li> </ul>"},{"location":"basics/#parametric-and-non-parametric-model","title":"Parametric and Non-parametric model","text":"<p>Parametric</p> <ul> <li>Makes strong assumptions about the data distribution</li> <li>Has a fixed number of parameters</li> <li>Linear Regression, Logistic Regression, Neural Networks, LDA</li> </ul> <p>Non-parametric Model</p> <ul> <li>Makes fewer assumptions about data distribution</li> <li>Number of parameters grows with training data</li> <li>KNN, Decision Trees, Random Forests, SVM, Gaussian Processes</li> </ul>"},{"location":"basics/#generative-and-non-generative-model","title":"Generative and Non-generative model","text":"<p>Generative Model</p> <ul> <li>\\(P(Y|X) = P(X|Y)P(Y)/P(X)\\)</li> <li>\\(Y^{\\star} = argmax[P(X|Y)P(Y)]\\)<ul> <li>\\(X\\) : feature </li> <li>\\(Y\\) : class label</li> <li>\\(P(X|Y)\\) : Likelihood</li> </ul> </li> <li>Naive Bayes, GMM, LDA, GANs, HMM</li> </ul> <p>Non-generative Model</p> <ul> <li>Directly models \\(P(Y|X)\\)</li> <li>\\(Y^{\\star} = argmax[P(Y|X)]\\)</li> <li>Logistic Regression, SVM, Decision Tree, Random Forest, Neural Network </li> </ul>"},{"location":"basics/Activation/","title":"Activation","text":""},{"location":"basics/Activation/#activation","title":"Activation","text":""},{"location":"basics/Activation/#sigmoid","title":"Sigmoid","text":"<ul> <li> <p>\\(f(x) = \\frac{1}{1 + e^{-x}} = \\sigma(x)\\)</p> </li> <li> <p>\\(f'(x) = f(x)(1 - f(x)) = \\sigma(x)(1 - \\sigma(x))\\)</p> </li> </ul>"},{"location":"basics/Activation/#tanh","title":"Tanh","text":"<ul> <li> <p>\\(f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)</p> </li> <li> <p>\\(f'(x) = 1 - \\tanh^2(x) = 1 - f^2(x)\\)</p> </li> </ul>"},{"location":"basics/Activation/#relu","title":"ReLU","text":"<ul> <li> <p>\\(f(x) = \\max(0, x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul>"},{"location":"basics/Activation/#leaky-relu","title":"Leaky ReLU","text":"<ul> <li> <p>\\(f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ \\alpha &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul>"},{"location":"basics/Activation/#elu-exponential-linear-unit","title":"ELU (Exponential Linear Unit)","text":"<ul> <li> <p>\\(f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha(e^x - 1) &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ \\alpha e^x &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul>"},{"location":"basics/Activation/#swish","title":"Swish","text":"<ul> <li> <p>\\(f(x) = x \\cdot \\sigma(x)\\)</p> </li> <li> <p>\\(f'(x) = f(x) + \\sigma(x)(1 - f(x))\\)</p> </li> </ul>"},{"location":"basics/Activation/#prelu-parametric-relu","title":"PReLU (Parametric ReLU)","text":"<ul> <li> <p>\\(f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ \\alpha &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul> <p>\\(\\alpha :\\) Learnable Parameter</p>"},{"location":"basics/Activation/#gelu","title":"GeLU","text":"<ul> <li> <p>\\(f(x) = x \\cdot \\Phi(x)\\)</p> </li> <li> <p>\\(f'(x) = \\Phi(x) + x\\Phi(x)\\)</p> </li> <li> <p>\\(\\Phi(x) : \\text{Cumulative Distribution Function of Gaussian}\\)</p> </li> <li> <p>\\(\\Phi(x) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right] = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-\\frac{t^2}{2}}dt\\)</p> </li> <li> <p>\\(\\Phi(x) = 0.5 * (1 + \\tanh(\\sqrt{\\frac{2}{\\pi}} * (x + 0.044715 * x^3)))\\)</p> </li> </ul>"},{"location":"basics/Activation/#linear","title":"Linear","text":"<ul> <li> <p>\\(f(x) = x\\)</p> </li> <li> <p>\\(f'(x) = 1\\)</p> </li> </ul>"},{"location":"basics/Attention/","title":"Attention","text":""},{"location":"basics/BiasVar/","title":"Bias & Variance","text":""},{"location":"basics/BiasVar/#bias-variance-tradeoff","title":"Bias-Variance tradeoff","text":"<ul> <li>Let \\(f(x)\\) be true model and \\(\\hat{f}(x)\\) be estimate of our model</li> </ul>"},{"location":"basics/BiasVar/#bias","title":"Bias","text":"<ul> <li>Bias measures the difference between the model's average prediction and the true value</li> <li>Bias(\\(\\hat{f}(x)\\)) = \\(E[\\hat{f}(x)] - f(x)\\)</li> <li>\\(E[\\hat{f}(x)]:\\) Average value of the model.</li> <li>For simple models, the average predicted value is often very far from the true value</li> <li>Simple models have very high bias &amp; complex models have very low bias</li> <li>Simple model has very high bias &amp; complex model has very low bias.</li> </ul>"},{"location":"basics/BiasVar/#variance","title":"Variance","text":"<ul> <li>Variance measures the model's sensitivity to fluctuations in the training set</li> <li>Variance(\\(\\hat{f}(x)\\)) = \\(E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]\\)</li> <li>Measures how much the predictions vary from one training set to another</li> <li>Simple models have low variance as they are more stable</li> <li>Complex models have high variance as they are more sensitive to training data</li> </ul> <p>Note</p> <ul> <li>Simple Model: high bias, low variance, underfitting</li> <li>Complex Model: low bias, high variance, overfitting</li> </ul>"},{"location":"basics/BiasVar/#trade-off","title":"Trade-off","text":"<ul> <li>The Bias-variance tradeoff is the relationship between bias and variance as you try to minimize each but by minimizing one the other increases, so there is  inherently a tradeoff. </li> <li>\\(E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{variance} + \\sigma \\text{(irreducible error)}\\)</li> </ul>"},{"location":"basics/BiasVar/#case-of-underfitting","title":"Case of underfitting","text":"<ol> <li>High loss for training and high for the test.</li> <li>Low accuracy for training and low for the test.</li> </ol>"},{"location":"basics/BiasVar/#prevention","title":"Prevention","text":"<ul> <li>More complex models</li> <li>Reducing regularization</li> <li>Increase training time</li> </ul>"},{"location":"basics/BiasVar/#case-of-overfitting","title":"Case of overfitting","text":"<ol> <li>Low loss for training and high for the test.</li> <li>Low accuracy for training and high for the test.</li> </ol>"},{"location":"basics/BiasVar/#prevention_1","title":"Prevention","text":"<ul> <li>Regularization</li> <li>Train with more data</li> <li>Data augmentation</li> <li>K-fold cross-validation</li> <li>Reduce the number of feature</li> <li>Change the model</li> </ul>"},{"location":"basics/CICD/","title":"CI/CD","text":""},{"location":"basics/CICD/#cicd","title":"CI/CD","text":""},{"location":"basics/CICD/#continuous-deployment","title":"Continuous Deployment","text":"<ul> <li>GitHub repo</li> <li>Dev env</li> <li>QA env</li> <li>UAT env</li> <li>Production</li> </ul>"},{"location":"basics/CICD/#developer-workflow","title":"Developer Workflow","text":"<p>Key Stages</p> <ul> <li>Coding</li> <li>Version Control</li> <li>Code Review</li> <li>Testing</li> <li>Continuous Integration</li> <li>CD</li> <li>Monitoring</li> </ul> <p>Example</p> <ol> <li>Feature Development</li> <li>Push and Pull Request (PR)</li> <li>Automated CI Pipeline         - Once PR is opened, build the application and runs all the test cases.         - If pipeline pases then code will be approved then it will be merged to main</li> <li>Continuous Deployment         - After merging the PR a CD pipeline is triggered         - Application is deployed to staging environment</li> </ol>"},{"location":"basics/Distance/","title":"Distance","text":""},{"location":"basics/Distance/#distance","title":"Distance","text":""},{"location":"basics/Distance/#euclidean-distance","title":"Euclidean Distance","text":"<ul> <li>Straight-line distance between two points</li> <li>Formula: \\(d(p,q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}\\)</li> </ul>"},{"location":"basics/Distance/#manhattan-distance","title":"Manhattan Distance","text":"<ul> <li>Sum of absolute differences between coordinates</li> <li>Formula: \\(d(p,q) = \\sum_{i=1}^{n}|p_i - q_i|\\)</li> </ul>"},{"location":"basics/Distance/#cosine-similarity","title":"Cosine Similarity","text":"<ul> <li>Measures cosine of angle between two vectors</li> <li>Range: [-1, 1] where 1 means vectors are identical</li> <li>Formula: \\(cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\\)</li> <li>Often converted to distance: \\(d(p,q) = 1 - cos(\\theta)\\)</li> </ul>"},{"location":"basics/Extra/","title":"Extra","text":""},{"location":"basics/Extra/#extra","title":"Extra","text":""},{"location":"basics/Extra/#handling-data-imbalance","title":"Handling Data Imbalance","text":"<ul> <li>over-sampling: sample the minority class significantly more such that the dataset is more balanced </li> <li>under-sampling: sample the majority class significantly less such that the dataset is more balanced </li> <li>SMOTE (synthetic minority oversampling technique) creates examples of the minority class by randomly sampling feature values from the current features </li> <li>Use another ML model: tree-based models perform well on imbalanced datasets</li> </ul>"},{"location":"basics/Extra/#residual-loss","title":"Residual Loss","text":""},{"location":"basics/Extra/#ssim-loss","title":"SSIM Loss","text":""},{"location":"basics/Extra/#perceptual-loss","title":"Perceptual Loss","text":""},{"location":"basics/Extra/#style-loss","title":"Style Loss","text":""},{"location":"basics/Extra/#content-loss","title":"Content Loss","text":""},{"location":"basics/Extra/#iou-loss","title":"IoU Loss","text":""},{"location":"basics/Extra/#negative-log-likelihood","title":"Negative Log Likelihood","text":""},{"location":"basics/FastAPI/","title":"FastAPI","text":""},{"location":"basics/FastAPI/#fastapi","title":"FastAPI","text":"<p>Applicaiton Programming Interface (API) is a connection between computers or between computr programs.</p> <p>FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.</p> <ul> <li>uvicorn</li> <li>Pydantic</li> </ul> <pre><code>pip install fastapi uvicorn\npip freeze &gt; requrements.txt\nuvicorn main:app --reload\n</code></pre>"},{"location":"basics/FeatureEngg/","title":"Feature Engineering","text":""},{"location":"basics/FeatureEngg/#feature-engineering","title":"Feature Engineering","text":""},{"location":"basics/FeatureEngg/#feature-selection","title":"Feature Selection","text":"<ul> <li>Find a small subset of features that has best correlation with class labels.</li> <li>Transform the original feature vector into a new feature vector to improve the features (e.g., make them uncorrelated).</li> </ul>"},{"location":"basics/GithubAction/","title":"Macro Rendering Error","text":""},{"location":"basics/GithubAction/#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>basics/GithubAction.md</code></p> <p>UndefinedError: 'env' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/notes/lib/python3.13/site-packages/mkdocs_macros/plugin.py\", line 688, in render\n    return md_template.render(**page_variables)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/notes/lib/python3.13/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/opt/anaconda3/envs/notes/lib/python3.13/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 141, in top-level template code\n  File \"/opt/anaconda3/envs/notes/lib/python3.13/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'env' is undefined\n</code></pre>"},{"location":"basics/Inference/","title":"Inference","text":""},{"location":"basics/Inference/#inference","title":"Inference","text":""},{"location":"basics/Inference/#latency","title":"Latency","text":""},{"location":"basics/Inference/#bandwidth","title":"Bandwidth","text":""},{"location":"basics/Inference/#throughput","title":"Throughput","text":""},{"location":"basics/Inference/#model-compression","title":"Model Compression","text":""},{"location":"basics/Inference/#quantization","title":"Quantization","text":""},{"location":"basics/Inference/#pruning","title":"Pruning","text":""},{"location":"basics/Inference/#distillation","title":"Distillation","text":""},{"location":"basics/InfoTheory/","title":"Information Theory","text":""},{"location":"basics/InfoTheory/#information-theory","title":"Information Theory","text":""},{"location":"basics/InfoTheory/#entropy","title":"Entropy","text":"<p>\\(H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i)\\)</p>"},{"location":"basics/InfoTheory/#information-gain","title":"Information Gain","text":"<p>\\(IG(T,a) = H(T) - \\sum_{v \\in values(a)} \\frac{|T_v|}{|T|} H(T_v)\\)</p> <ul> <li>\\(IG(T,a)\\): Information Gain of dataset \\(T\\) when split on attribute \\(a\\)</li> <li>\\(H(T)\\): Entropy of the parent dataset</li> <li>\\(\\sum_{v \\in values(a)}\\): Sum over all possible values of attribute \\(a\\)</li> <li>\\(|T_v|\\) is the number of examples in subset \\(v\\)</li> <li>\\(|T|\\): is the total number of examples in the parent dataset</li> </ul>"},{"location":"basics/InfoTheory/#gini-impurity","title":"Gini Impurity","text":"<p>\\(Gini(T) = 1 - \\sum_{i=1}^{c} (p_i)^2\\)</p> <ul> <li>\\(c\\): Number of classes/categories in the dataset</li> <li>\\(p_i\\): Probability (proportion) of data points belonging to class \\(i\\)</li> </ul>"},{"location":"basics/InfoTheory/#cross-entropy","title":"Cross Entropy","text":"<p>\\(H(p,q) = -\\sum_{x} p(x) \\log q(x)\\)</p>"},{"location":"basics/InfoTheory/#kl-divergence","title":"KL-Divergence","text":"<p>\\(D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\\)</p>"},{"location":"basics/Loss/","title":"Loss","text":""},{"location":"basics/Loss/#loss","title":"Loss","text":""},{"location":"basics/Loss/#mean-square-loss","title":"Mean Square Loss","text":"<ul> <li><code>Regression problems</code></li> <li>\\(L_{MSE} = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2\\)</li> <li>Properties<ul> <li>Sensitive to outliers</li> </ul> </li> </ul>"},{"location":"basics/Loss/#binary-cross-entropy-loss","title":"Binary Cross Entropy Loss","text":"<ul> <li><code>Binary Classification</code></li> <li>\\(L_{BCE} = -\\sum_i y_i \\log(\\hat{y}_{i}) + (1 - y_i) \\log(1 - \\hat{y}_{i})\\)</li> <li>Properties<ul> <li>Penalizes confident wrong predictions heavily</li> </ul> </li> </ul>"},{"location":"basics/Loss/#cross-entropy-loss","title":"Cross Entropy Loss","text":"<ul> <li><code>Multi-class Classification</code></li> <li>\\(L_{CE} = -\\sum_i y_i \\log(\\hat{y}_i)\\)</li> <li>Properties<ul> <li>Good for mutually exclusive classes</li> <li>Penalizes wrong predictions proportionally</li> </ul> </li> </ul>"},{"location":"basics/Loss/#hinge-loss","title":"Hinge Loss","text":"<ul> <li><code>SVM Classification</code></li> <li>\\(L_{hinge}=\\max(0, 1-\\hat{y} \\cdot y)\\)</li> <li>Properties<ul> <li>Robust to outliers</li> <li>Used in maximum-margin classification</li> </ul> </li> </ul>"},{"location":"basics/Loss/#focal-loss","title":"Focal Loss","text":"<ul> <li><code>Object Detection/Imbalanced Classification</code></li> <li>\\(L_{focal} = - (1 - p_t)^{\\gamma} \\log(p_t)\\)</li> <li>\\(L_{CE} = - \\log(p_t)\\)</li> <li>Properties<ul> <li>\\(\\gamma\\) controls focus on hard examples</li> </ul> </li> </ul>"},{"location":"basics/Loss/#triplet-loss","title":"Triplet Loss","text":"<ul> <li><code>Similarity Learning/Embedding Learning</code></li> <li>\\(L_{triplet} = \\sum_i^N [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \\alpha]_{+}\\)</li> <li>Proeprties<ul> <li>Requires triplets (anchor, positive, negative)</li> <li>\\(\\alpha\\) : margin that is enforced between positive and negative pairs</li> <li>Used in face recognition</li> </ul> </li> </ul>"},{"location":"basics/Loss/#kl-divergence-loss","title":"KL Divergence Loss","text":"<ul> <li><code>Distribution Learning</code></li> <li>\\(KL(P||Q)=\\sum P(x) \\log \\frac{P(x)}{Q(x)}\\)</li> <li>Properties<ul> <li>Not symmetric</li> <li>Used in VAE</li> <li>Useful for probability distribution matching</li> </ul> </li> </ul>"},{"location":"basics/Loss/#contrastive-loss","title":"Contrastive Loss","text":"<ul> <li><code>Self-supervised/Multi-modal Learning</code></li> <li>\\(l_i^{(u \\rightarrow v)} = - \\log \\frac{\\exp(sim(u_i,v_i)/\\tau)}{\\sum_{k=1}^N \\exp(sim(u_i, v_k)/ \\tau)}\\)</li> <li>\\(L_{contrast} = \\frac{1}{N} \\sum_{i=1}^N (\\lambda l_i^{(v \\rightarrow u)} + (1-\\lambda) l_i^{(u \\rightarrow v)})\\)</li> <li>\\(\\lambda \\in [0,1]\\)</li> <li>Properties<ul> <li>Pulls similar pairs together</li> <li>Pushes dissimilar pairs apart</li> <li>Used in CLIP, SimCLR</li> </ul> </li> </ul>"},{"location":"basics/Loss/#question","title":"Question","text":"<ul> <li>Explain residual loss and how it can be generalized for training multiple hypotheses.</li> </ul> Draw graph of cross entropy."},{"location":"basics/MLEMAE/","title":"MAE &amp; MAE","text":""},{"location":"basics/MLEMAE/#mae-mae","title":"MAE &amp; MAE","text":""},{"location":"basics/MLEMAE/#what-is-mle-estimate-prior-and-map-estimate","title":"what is MLE estimate, prior and MAP estimate ?","text":""},{"location":"basics/Metric/","title":"Metric","text":""},{"location":"basics/Metric/#metric","title":"Metric","text":""},{"location":"basics/Metric/#basics","title":"Basics","text":"<ul> <li>True Positives (TP): actual observation is 1 (True) and prediction is 1 (True) </li> <li>True Negative (TN): actual observation is 0 (False) and prediction is 0 (False) </li> <li>False Positive (FP): actual observation is 0 (False) and prediction is 1 (True) </li> <li>False Negative (FN): actual observation is 1 (True) and prediction is 0 (False) </li> </ul>"},{"location":"basics/Metric/#accuracy","title":"Accuracy","text":"<p><code>Balanced</code></p> <p>\\(\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\)</p>"},{"location":"basics/Metric/#recall","title":"Recall","text":"<p><code>Balanced, Imbalanced</code></p> <p>Out of all the True <code>observations</code> how many were actually True. \\(Recall = \\frac{TP}{TP+FN}\\)</p> <ul> <li>When false positives are very costly</li> </ul> <p>Example</p> <ul> <li>Cancer Detection</li> <li>Criminal Detection</li> <li>Manufacturing Quality Control (catch all defects)</li> </ul>"},{"location":"basics/Metric/#precision","title":"Precision","text":"<p><code>Balanced, Imbalanced</code></p> <p>Out of all the True <code>predictions</code> how many were actually True.</p> <p>\\(Precision = \\frac{TP}{TP+FP}\\)</p> <p>Example</p> <ul> <li>Email Spam Filter (avoiding marking important emails as spam)</li> <li>Medical diagnostics  (avoiding unnecessary treatments)</li> <li>Fraud detection (avoiding blocking legitimate transactions)</li> </ul> <p>Info</p> <ul> <li>High threshold lead to high precision while recall decreases</li> </ul>"},{"location":"basics/Metric/#pr-curve","title":"PR Curve","text":""},{"location":"basics/Metric/#f1-score","title":"F1-score","text":"<p><code>Balanced, Imbalanced</code></p> <p>Harmonic Mean of Precision(P) and Recall(R)</p> <p>\\(F1-score = \\frac{2*PR}{P+R}\\)</p>"},{"location":"basics/Metric/#micro-f1-vs-macro-f1","title":"Micro F1 Vs macro F1","text":""},{"location":"basics/Metric/#mean-squared-error","title":"Mean Squared Error","text":""},{"location":"basics/Metric/#r2-score","title":"R2 Score","text":"<p>\\(R^2\\) Score</p> <ul> <li>range : [0, 1]</li> <li>Having 1 is good model and 0 is bad model </li> </ul>"},{"location":"basics/Metric/#receiver-operating-characteristic-roc","title":"Receiver Operating Characteristic (ROC)","text":"<p>The ROC is a probability curve and the area under  the curve can be thought of as the degree of separability between the two classes. </p>"},{"location":"basics/Metric/#pixel-accuracy","title":"Pixel Accuracy","text":"<p>A metric that calculates the percentage of correctly classified pixels in an image.</p> <p>\\(Pixel Accuracy = \\frac{Number\\ of\\ Correctly\\ Classified\\ Pixels}{Total\\ Number\\ of\\ Pixels}\\)</p> <p>Limitation: Can be misleading for imbalanced datasets. For example, if an image has only 2 pixels of interest out of 100 pixels, predicting no objects would still result in 98% accuracy.</p>"},{"location":"basics/Metric/#intersection-over-union-iou","title":"Intersection over Union (IoU)","text":"<p>A more robust metric that measures the overlap between predicted and ground truth segments.</p> <p>\\(IoU = \\frac{Area\\ of\\ Overlap}{Area\\ of\\ Union} = \\frac{Intersection}{Union}\\)</p> <p>Where,</p> <ul> <li>Intersection = Area shared between prediction and ground truth</li> <li>Union = Total area covered by both prediction and ground truth</li> </ul> <p>If two bbox ahve both Intersection and union high then the IoU will be high. Assume two case :</p> <ol> <li>Intersion is large but the bbox are also very large bbox this leads to very high union depicting same same object.</li> <li>Intersion is large but the bbox are small bbox then possibly they are dipicting different object.</li> </ol>"},{"location":"basics/Metric/#dice-coefficient","title":"Dice Coefficient","text":"<p>Similar to IoU but less harsh on misclassifications. It has a monotonic relationship with IoU.</p> <p>\\(Dice = \\frac{2 \\times Area\\ of\\ Overlap}{Sum\\ of\\ Areas} = \\frac{2 \\times Intersection}{Area\\ of\\ Prediction + Area\\ of\\ Ground\\ Truth}\\)</p> <p>Relationship with IoU: \\(Dice = \\frac{2 \\times IoU}{1 + IoU}\\)</p>"},{"location":"basics/Metric/#non-maxium-supression-nms","title":"Non-Maxium Supression (NMS)","text":"<p>We  don't need all the object proposals. We only want to keep the best one.</p>"},{"location":"basics/Metric/#nms","title":"\u03bb<sub>NMS</sub>","text":"<p>Do not allow the bbos if they are overlapping more than \\(\\lambda_{NMS}\\) threshold.</p> <ul> <li>Narrrow Threshold (High IoU) : Low Precision (More False Positive)</li> <li>Wide Threshold (Low IoU): Low Recall (More False Negative)</li> </ul>"},{"location":"basics/Metric/#question","title":"Question","text":"Metrics for multi-class classification? <ul> <li>Confusion Matrix</li> <li>Multi-class ROC-AUC<ul> <li>One-vs-Rest</li> <li>One-vs-One</li> </ul> </li> </ul>"},{"location":"basics/Optimization/","title":"Optimization","text":""},{"location":"basics/Optimization/#optimization","title":"Optimization","text":""},{"location":"basics/Optimization/#sgd","title":"SGD","text":"<p>\\(w_{t+1} = w_t - \\eta \\nabla w_t\\)</p>"},{"location":"basics/Optimization/#momentum-based-gd","title":"Momentum Based GD","text":"<p>\\(update_t = \\gamma \\cdot update_{t-1} + \\eta \\nabla w_t\\)</p> <p>\\(w_{t+1} = w_t - update_t\\)</p>"},{"location":"basics/Optimization/#nesterov-accelerated-gd","title":"Nesterov Accelerated GD","text":"<p>\\(update_t = \\gamma \\cdot update_{t-1} + \\eta \\nabla (w_t- \\gamma \\cdot update_{t-1})\\)</p> <p>\\(w_{t+1} = w_t - update_t\\)</p>"},{"location":"basics/Optimization/#adagrad","title":"AdaGrad","text":"<p>\\(v_t = v_{t-1} + (\\nabla w_t)^2\\)</p> <p>\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon }}* \\nabla w_t\\)</p> <p>effective_lr = initial_lr / sqrt(accumulated_squared_gradients + eps)</p>"},{"location":"basics/Optimization/#rmsprop","title":"RMSProp","text":"<p>\\(v_t = \\alpha * v_{t-1} + (1- \\alpha) *(\\nabla w_t)^2\\)</p> <p>\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon }}* \\nabla w_t\\)</p>"},{"location":"basics/Optimization/#adam","title":"Adam","text":"<p>\\(m_t = \\beta_1 * v_{t-1} + (1- \\beta_1) * \\nabla w_t\\)</p> <p>\\(v_t = \\beta_2 * v_{t-1} + (1- \\beta_2) *(\\nabla w_t)^2\\)</p> <p>Bias Correction</p> <p>\\(\\hat{m_t} = \\frac{m_t}{1-\\beta_1^t}\\)</p> <p>\\(\\hat{v_t} = \\frac{v_t}{1-\\beta_2^t}\\)</p> <p>\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v_t} + \\epsilon }} * \\nabla \\hat{m_t}\\)</p>"},{"location":"basics/Optimization/#question","title":"Question","text":"<ul> <li>Derive the backpropagation equations for a 3-layer neural network with the following specifications:<ul> <li>Each layer has exactly one neuron</li> <li>Network structure: Input \u2192 Hidden Layer 1 (1 neuron) \u2192 Hidden Layer 2 (1 neuron) \u2192 Output Layer (1 neuron)</li> </ul> </li> </ul> Despite using single samples for gradient estimation, how does Stochastic Gradient Descent (SGD) manage to converge? <ul> <li>Noisy but unbiased estimate of true gradient</li> <li>\\(E[\\nabla L_i] = \\nabla L\\)</li> <li>Converges in expectation</li> </ul>"},{"location":"basics/PositionEncoding/","title":"Position Encoding","text":""},{"location":"basics/Probability/","title":"Probability","text":""},{"location":"basics/Probability/#probability","title":"Probability","text":""},{"location":"basics/Probability/#notations","title":"Notations","text":"<p>\\(\\Omega\\) : Sample Space</p> <p>\\(\\mathcal{F}\\) : set of all possible event</p> <p>\\(P\\) : Probability</p>"},{"location":"basics/Probability/#basics","title":"Basics","text":""},{"location":"basics/Probability/#axioms-and-properties","title":"Axioms and Properties","text":"<p>Axioms</p> <p>\\(P : \\mathcal{F} \\rightarrow R\\) satisfying</p> <ul> <li>Non-negativity : \\(P(A)\\) \u2265 0, \\(\\forall A \\in \\mathcal{F}\\)</li> <li>Normalization: \\(P(\\Omega) =1\\)</li> <li>\\(\\sigma\\)-additivity: If \\(A_i \\cap A_j = \\phi\\), \\(\\forall i \\neq j\\) then \\(P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty}P(A_i)\\)</li> </ul> <p>Properties</p> <p>If \\(A \\subseteq B \\Longrightarrow P(A) \\le P(B).\\)</p> <ul> <li> <p>\\(P(A \\cap B) \\le min(P(A), P(B))\\)</p> </li> <li> <p>\\(P(A \\cup B) \\le P(A) + P(B)\\)</p> </li> <li> <p>\\(P( \\Omega / A) = 1- P(A)\\)</p> </li> </ul> <p>If \\(A_1, \\cdots, A_k\\) are a set of disjoint events such that \\(\\cup_{i=1}^{k} A_i = \\Omega\\), then \\(\\sum_{i=1}^{k} P(A_k) =1.\\)</p>"},{"location":"basics/Probability/#independent","title":"Independent","text":"<ul> <li> <p>\\(P(AB) = P(A) P(B)\\)</p> </li> <li> <p>\\(P(ABC) = P(A) P(B) P(C)\\)</p> </li> </ul>"},{"location":"basics/Probability/#conditional-independence","title":"Conditional Independence","text":"<ul> <li>\\(P(AB|C) = P(A|C)P(B|C)\\)</li> </ul> <p>Event may be conditionally independent but not independent.</p>"},{"location":"basics/Probability/#chain-rule","title":"Chain Rule","text":"\\[f(x_1, x_2, \\cdots , x_n) = f(x_1)f(x_2|x_1) f(x_3|x_1, x_2) \\cdots f(x_n| x_1,x_2, \\cdots , x_{n-1}) = f(x_1) \\Pi_{i=2}^{n} f(x_i | x_1, \\cdots , x_{i-1})\\]"},{"location":"basics/Probability/#conditional-probability","title":"Conditional Probability","text":"<ul> <li> <p>\\(P(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(AB)}{P(B)}\\)</p> </li> <li> <p>\\(P(AB) = P(A|B) P(B)\\)</p> </li> <li> <p>\\(P(A|B,C) = P(A|BC) = \\frac{P(ABC)}{P(BC)}\\)</p> </li> </ul>"},{"location":"basics/Probability/#total-probability-rule","title":"Total probability rule","text":"<ul> <li>\\(P(A) = \\sum_{i} P(A|B_i)P(B_i)\\)</li> </ul>"},{"location":"basics/Probability/#bayes-rule","title":"Bayes Rule","text":"<ul> <li>\\(P(A|B) = \\frac{P(AB)}{P(B)}= \\frac{P(B|A)P(A)}{P(B)}\\)</li> </ul>"},{"location":"basics/Probability/#random-variable","title":"Random Variable","text":"<p>\\(X:\\Omega \\rightarrow R\\), here \\(X(\\omega)\\) is denoted as \\(X\\).</p>"},{"location":"basics/Probability/#cdf","title":"CDF","text":"<p>\\(F_X : R \\rightarrow [0,1]\\)</p> <p>\\(F_X(x) = P(X &lt; x)\\)</p> <p>Properties</p> <ul> <li> <p>\\(0 \\le F_X(x) \\le 1.\\)</p> </li> <li> <p>\\(\\lim_{x \\to  - \\infty} F_X(x) = 0\\)</p> </li> <li> <p>\\(\\lim_{x \\to  \\infty} F_X(x) = 1\\)</p> </li> <li> <p>\\(x \\le y \\Longrightarrow  F_X(x) \\le F_X(y).\\)</p> </li> </ul>"},{"location":"basics/Probability/#pmf","title":"PMF","text":"<p>\\(X\\) is a discrete random variable</p> <p>\\(p_X : \\Omega \\rightarrow R\\)</p> <p>\\(p_X(x) = P(X=x)\\)</p> <p>\\(Val(X):\\) set of possible values of random variable \\(X\\).</p> <p>Properties:</p> <ul> <li> <p>\\(0 \\le p_X(x) \\le 1\\)</p> </li> <li> <p>\\(\\sum_{x \\in Val(X)} p_X(x) = 1\\)</p> </li> <li> <p>\\(\\sum_{x \\in A} p_X(x) = P(X \\in A)\\)</p> </li> </ul>"},{"location":"basics/Probability/#pdf","title":"PDF","text":"<p>For some continuous random variable, \\(F_x(x)\\) is differentiable everywhere. Then PDF can be defined as.</p> <p>\\(f_X(x) = \\frac{dF_X(x)}{dx}\\)</p> <p>So, PDF for continuous random variable may not always exits.</p> <p>PDF at any given point x is not the probability of that event, i.e., \\(f_X(x) \\neq P(X=x)\\) as it can take value larger than 1.</p> <p>Properties:</p> <ul> <li> <p>\\(f_X(x) \\ge 1\\)</p> </li> <li> <p>\\(\\int_{-\\infty}^{\\infty} f_X(x) =1\\)</p> </li> <li> <p>\\(\\int_{x \\in A} f_X(x) dx = P(X =A)\\)</p> </li> </ul>"},{"location":"basics/Probability/#expectation","title":"Expectation","text":"<ul> <li> <p>\\(E[g(X)] = \\sum_{x \\in Val(X)} g(x) p_X(x)\\), \\(X\\) is discrete random variable</p> </li> <li> <p>\\(E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx\\), \\(X\\) is continuous random variable</p> </li> </ul> <p>Properties</p> <ul> <li> <p>\\(E[a] = a\\)</p> </li> <li> <p>\\(E[af(X)]=aE[f(X)]\\), for any constant \\(a \\in R\\)</p> </li> <li> <p>\\(E[f(X) + g(X)] = E[f(X)] + E[g(X)]\\)</p> </li> </ul> <p>For discrete random variable \\(X, E[1\\{X=k\\}] = P(X=k)\\)</p>"},{"location":"basics/Probability/#variance","title":"Variance","text":"<ul> <li> <p>\\(Var(X) = E[(X - E(X))^2]\\)</p> </li> <li> <p>\\(E[(X - E(X))^2] = E[X^2] - E[X]^2\\)</p> </li> </ul> <p>Properties</p> <ul> <li> <p>\\(Var[a] = 0\\), for any constant \\(a \\in R\\).</p> </li> <li> <p>\\(Var[af(x)] = a^2 Var[f(x)]\\)</p> </li> </ul>"},{"location":"basics/Probability/#compilation","title":"Compilation","text":""},{"location":"basics/Probability/#two-random-variable","title":"Two Random Variable","text":""},{"location":"basics/Probability/#cdf_1","title":"CDF","text":"<ul> <li>\\(F_{XY}(x, y) = P(X \\le x, Y \\le y)\\)</li> </ul> <p>Marginal CDF</p> <ul> <li> <p>\\(F_X(x) = \\lim_{y \\to \\infty}F_{XY}(x, y) dy\\)</p> </li> <li> <p>\\(F_Y(y) = \\lim_{x \\to \\infty}F_{XY}(x, y) dx\\)</p> </li> </ul> <p>Properties</p> <ul> <li> <p>\\(0 \\le F_{XY}(x, y) \\le 1\\)</p> </li> <li> <p>\\(\\lim_{x,y \\to \\infty} F_{XY}(x, y) = 1\\)</p> </li> <li> <p>\\(\\lim_{x,y \\to -\\infty} F_{XY}(x, y) = 0\\)</p> </li> <li> <p>\\(F_X(x) = \\lim_{y \\to \\infty}F_{XY}(x, y)\\)</p> </li> </ul>"},{"location":"basics/Probability/#pmf_1","title":"PMF","text":"<ul> <li> <p>\\(p_{XY}:R \\times R \\rightarrow [0,1]\\)</p> </li> <li> <p>\\(p_{XY}(x,y) = P(X=x, Y=y)\\)</p> </li> <li> <p>\\(p_X(x) = \\sum_{y} p_{XY}(x,y)\\), marginal probability function.</p> </li> </ul>"},{"location":"basics/Probability/#expectation_1","title":"Expectation","text":"<ul> <li> <p>\\(g: R^2 \\rightarrow R\\)</p> </li> <li> <p>\\(E[g(X,Y)] =  \\sum_{x \\in Val(X)}\\sum_{y \\in Val(Y)} g(X,Y) p_{XY}(x,y)\\), \\(X, Y\\) are discrete rv.</p> </li> <li> <p>\\(E[g(X,Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y) f_{XY}(x,y)dxdy\\), \\(X, Y\\) are continuous  rv.</p> </li> </ul> <p>Properties</p> <ul> <li>\\(E[f(X,Y) +g(X,Y)] = E[f(X,Y)] + E[g(X,Y)]\\)</li> </ul> <p>If X and Y are independent, then \\(E[f(X)g(Y)] = E[f(X)] E[g(Y)]\\)</p>"},{"location":"basics/Probability/#covariance","title":"Covariance","text":"<ul> <li> <p>\\(Cov(X,Y) = E[(X - E[X]) (Y - E[Y])]\\)</p> </li> <li> <p>\\(Cov (X, Y) = E[XY] - E[X]E[Y]\\)</p> </li> </ul> <p>Properties</p> <ul> <li>\\(Var[X+Y] = Var[X] + Var[Y] + 2Cov[X,Y]\\)</li> </ul> <p>If X and Y are independent, then \\(Cov(X,Y) = 0\\)</p>"},{"location":"basics/Probability/#random-vectors","title":"Random Vectors","text":"<p>Consider data in the d-dimensional Euclidean Space</p> <ul> <li> <p>\\(x = (x^1, ..., x^d) \\in R^d\\)</p> </li> <li> <p>\\(\\langle x, y \\rangle = \\sum_{i=1}^{d} x^iy^i\\)</p> </li> <li> <p>\\(||x|| =\\sqrt{\\langle x,x \\rangle}\\)</p> </li> </ul> <p>Consider RV \\(X \\in R^d\\) with PDF \\(p_X : R^d \\rightarrow R_{\\ge 0}\\) providing event A with probability.</p> <ul> <li>\\(P(X\\in A) = \\int_{A} p_X(x)dx\\) where \\(\\int p_X(x)dx =1\\)</li> </ul> <p>To keep notation concise.</p> <ul> <li>\\(p_{X_t}\\) as \\(p_t\\)</li> <li>\\(X \\sim p(X)\\) as \\(X \\sim p\\)</li> </ul>"},{"location":"basics/Probability/#expectation_2","title":"Expectation","text":"<ul> <li> <p>Expected value minimizes the mean squared error.</p> </li> <li> <p>\\(E[X] = {\\arg \\min}_{z \\in R^d} \\int {||x-z ||}^2 p_X(x)dx = \\int x p_X(x)dx\\)</p> </li> <li> <p>\\(E[f(X)] = \\int f(x) p_X(x)dx\\) : Law of the Unconscious Statistician</p> </li> </ul> <p>Conditional densities and expectations</p> <ul> <li> <p>\\(X, Y \\in R^d\\)</p> </li> <li> <p>\\(p_{X,Y}(x,y)\\) :  Joint PDF</p> </li> <li> <p>\\(p_Y(y) = \\int p_{X,Y}(x,y) dx\\)  : Marginal PDF</p> </li> <li> <p>\\(p_X(x) = \\int p_{X,Y}(x,y) dy\\) : Marginal PDF</p> </li> <li> <p>\\(P_{X|Y}(x|y) := \\frac{p_{X,Y}(x,y)}{p_Y(y)}\\) : Conditional PDF that describes the PDF of the RV \\(X\\) when conditioned on \\(Y=y\\) with density \\(p_Y(y)&gt;0\\).</p> </li> </ul> <p>Using Bayes Rule</p> <ul> <li> <p>\\(P_{Y|X}(y|x) = \\frac{p_{X|Y}(x|y) p_Y(y)}{p_X(x)}\\) where \\(p_{X} &gt; 0\\).</p> </li> <li> <p>\\(E[X|Y=y] := g_{\\star}(y) = \\int x p_{X|Y}(x|y) dx\\) : It is function of y, \\(g_{\\star}(y) : R^d \\rightarrow R^d\\)</p> </li> <li> <p>\\(E[X|Y]\\) is a RV assuming values in \\(R^d\\)</p> </li> </ul>"},{"location":"basics/Probability/#tower-property","title":"Tower property","text":"<ul> <li> <p>\\(E[E[X|Y]] = E[X]\\) : Detail on variable \\(E_{Y}[E_{X}[X|Y]] = E_{X}[X]\\)</p> </li> <li> <p>\\(E[f(X,Y)|Y=y)] = \\int f(x,y) p_{X|Y}(x|y) dx\\) : Law of the Unconscious Statistician</p> </li> </ul>"},{"location":"basics/Probability/#covariance_1","title":"Covariance","text":"<ul> <li>\\(\\Sigma = E[(X-E[X])(X-E[X])^T]\\)</li> </ul>"},{"location":"basics/Probability/#change-of-variables","title":"Change of Variables","text":"<p>Jacobian matrix</p> <ul> <li> <p>\\(J =  \\frac{\\partial(x_1, x_2)}{\\partial(y_1, y_2)}=\\begin{bmatrix} \\frac{\\partial{x_1}}{\\partial{y_1}} &amp; \\frac{\\partial{x_1}}{\\partial{y_2}} \\\\ \\frac{\\partial{x_2}}{\\partial{y_1}} &amp; \\frac{\\partial{x_2}}{\\partial{y_2}} \\end{bmatrix}\\)</p> </li> <li> <p>\\(Y = g(X)\\) and \\(X = h(Y)\\). \\(h\\) is inverse of \\(g\\)</p> </li> <li> <p>\\(p_Y(y) =  |J| \\cdot p_X(h(y))\\)</p> </li> </ul> <p>Example use case ( Flow Matching )</p> <ul> <li> <p>\\(X \\sim p_X\\)</p> </li> <li> <p>\\(\\psi : R^d \\rightarrow R^d\\)</p> </li> <li> <p>\\(Y = \\psi (X)\\)</p> </li> </ul> <p>Calculate \\(p_Y(y)\\), \\(E[f(Y)]\\)</p> <p>Solution :</p> <ul> <li> <p>\\(p_Y(y) = |J| p_X(h(y))\\)</p> </li> <li> <p>\\(x = h(y) := \\psi^{-1}(y)\\)</p> </li> </ul> <p>\\(Here, J = \\begin{bmatrix} \\frac{\\partial{x}}{\\partial{y}}\\end{bmatrix} =\\begin{bmatrix} \\frac{\\partial{ \\psi^{-1}(y)}}{\\partial{y}}\\end{bmatrix}\\)</p> <p>Therefore, \\(p_Y(y) = | \\det \\partial_{y}\\psi^{-1}(y) | \\cdot p_X(\\psi^{-1}(y))\\)</p> <p>Now,</p> <ul> <li>\\(E[f(Y)] = \\int f(y) p_Y(y) dy = \\int f(y) | \\det \\partial_{y}\\psi^{-1}(y) | \\cdot p_X(\\psi^{-1}(y)) dy\\)</li> </ul>"},{"location":"basics/Regularization/","title":"Regularization","text":""},{"location":"basics/Regularization/#regularization","title":"Regularization","text":""},{"location":"basics/Regularization/#lasso","title":"Lasso","text":"<ul> <li>It is also called <code>L1 regularization</code></li> <li>Lasso has \u201cbuilt-in\u201d feature selection since it shrinks the least important features\u2019 coefficient to zero, creating sparse outputs. </li> <li>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda |\\sum_{i=0}^{M}w_j|\\)</li> </ul>"},{"location":"basics/Regularization/#ridge","title":"Ridge","text":"<ul> <li>It is also called <code>L2 regularization</code></li> <li>Ridge regression is more computationally efficient due to being differentiable at 0 (can be used easily with gradient descent) while lasso is undefined at 0. </li> <li>\\(\\tilde{\\mathcal{L}}(w) = \\mathcal{L}(w) + \\frac{\\lambda}{2} ||w||^2\\)<ul> <li>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\frac{\\lambda}{2} \\sum_{i=0}^{M}w_j^2\\)</li> </ul> </li> <li>\\(\\nabla \\tilde{\\mathcal{L}}(w) = \\nabla \\mathcal{L}(w) + \\lambda w\\)</li> <li>\\(w_{t+1} = w_t - \\eta \\cdot \\nabla \\tilde{\\mathcal{L}}(w_t)\\)</li> </ul>"},{"location":"basics/Regularization/#elastic-net","title":"Elastic-net","text":"<ul> <li>It is also called <code>L1+L2 regularization</code></li> <li>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda_1 |\\sum_{i=0}^{M}w_j| + \\lambda_2 \\sum_{i=0}^{M}w_j^2\\)</li> </ul>"},{"location":"basics/Regularization/#dataset-augmentation","title":"Dataset Augmentation","text":""},{"location":"basics/Regularization/#parameter-sharing-and-tying","title":"Parameter Sharing and tying","text":""},{"location":"basics/Regularization/#adding-noise-to-the-input","title":"Adding Noise to the input","text":""},{"location":"basics/Regularization/#adding-noise-to-the-output","title":"Adding Noise to the output","text":""},{"location":"basics/Regularization/#early-stopping","title":"Early Stopping","text":""},{"location":"basics/Regularization/#ensemble-methods","title":"Ensemble Methods","text":""},{"location":"basics/Regularization/#dorpout","title":"Dorpout","text":""},{"location":"basics/Regularization/#inverted-dropout","title":"Inverted Dropout","text":""},{"location":"basics/Regularization/#question","title":"Question","text":"How L1 helps in feature selection? <ul> <li>L1 regularization adds absolute value of weights (|w|) to loss function</li> <li>Pushes less important feature weights exactly to zero</li> <li>Creates sparse solutions where many features have zero coefficients</li> </ul> Why Exactly Zero? <ul> <li> <p>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda |\\sum_{i=0}^{M}w_j|\\)</p> </li> <li> <p>derivative of \\(|W|\\)</p> <ul> <li>+1  when w &gt; 0</li> <li>-1  when w &lt; 0</li> <li>[-1,1] at w = 0 (subgradient)</li> <li>Function is non-differentiable at w = 0</li> </ul> </li> <li> <p>update equation</p> <ul> <li> <p>w = w - \u03b1(\u2202L/\u2202w + \u03bb\u22c5sign(w)) where, sign(w) = {-1,0,1}</p> </li> <li> <p>\\(\\text{For } w &gt; 0:     \\begin{cases}     w_{t+1} = w_t - \\alpha(\\frac{\\partial L}{\\partial w} + \\lambda) \\     \\text{Term } +\\lambda \\text{ pushes w toward zero}     \\end{cases}\\)</p> <p>\\(\\text{For } w &lt; 0: \\begin{cases} w_{t+1} = w_t - \\alpha(\\frac{\\partial L}{\\partial w} - \\lambda) \\ \\text{Term } -\\lambda \\text{ pushes w toward zero} \\end{cases}\\)</p> <p>\\(\\text{At } w = 0: \\begin{cases} \\text{Stays at zero if } |\\frac{\\partial L}{\\partial w}| &lt; \\lambda \\ \\text{Escapes zero if } |\\frac{\\partial L}{\\partial w}| &gt; \\lambda \\end{cases}\\)</p> </li> </ul> </li> </ul> Dropout and how it can be used to obtain probability distribution of response variable. <ul> <li>During Inference:<ul> <li>Keep dropout active (don't disable)</li> <li>Run multiple forward passes (e.g., 100 times)</li> <li>Each pass gives different prediction</li> </ul> </li> <li>Use the prediction:<ul> <li>Mean prediction (average of all passes)</li> <li>Uncertainty estimate (variance of predictions)</li> <li>Approximate probability distribution</li> </ul> </li> </ul>"},{"location":"basics/Scheduler/","title":"Scheduler","text":""},{"location":"basics/Scheduler/#scheduler","title":"Scheduler","text":""},{"location":"basics/Scheduler/#steplr","title":"StepLR","text":"<p>\\(lr_{epoch} = \\begin{cases} \\gamma * lr_{epoch-1}, &amp; \\text{if epoch % step_size = 0} \\\\ lr_{epoch-1}, &amp; \\text{otherwise} \\end{cases}\\)</p>"},{"location":"basics/Scheduler/#multisteplr","title":"MultiStepLR","text":"<p>\\(lr_{epoch} = \\begin{cases} \\gamma * lr_{epoch-1}, &amp; \\text{if epoch in milestones} \\\\ lr_{epoch-1}, &amp; \\text{otherwise} \\end{cases}\\)</p>"},{"location":"basics/Scheduler/#exponentiallr","title":"ExponentialLR","text":"<p>\\(lr_{epoch} = \\gamma * lr_{epoch-1}\\)</p>"},{"location":"basics/VectorCalculus/","title":"Vector Calculus","text":""},{"location":"basics/VectorCalculus/#vector-calculus","title":"Vector Calculus","text":""},{"location":"basics/VectorCalculus/#basics","title":"Basics","text":""},{"location":"basics/VectorCalculus/#vector","title":"Vector","text":""},{"location":"basics/VectorCalculus/#notations","title":"Notations","text":"<ul> <li> <p>\\(\\bar{A} = A_x \\hat{i} + A_y \\hat{j} + A_z \\hat{k}\\)</p> </li> <li> <p>\\(\\bar{B} = B_x \\hat{i} + B_y \\hat{j} + B_z \\hat{k}\\)</p> </li> </ul> <p>Position vector for point \\(P(x,y,z) : \\bar{OP} = \\bar{r} = x \\hat{i} + y \\hat{j} + z \\hat{k}\\)</p> <ul> <li>\\(r = \\sqrt{x^2 + y^2 + z^2}\\)</li> </ul>"},{"location":"basics/VectorCalculus/#properties","title":"Properties","text":"<ol> <li>Magnitude</li> <li>Direction</li> <li>Follows vector law of addition</li> </ol>"},{"location":"basics/VectorCalculus/#representations","title":"Representations","text":"<ul> <li> <p>\\(\\bar{A} = |\\bar{A}| \\hat{a}_A\\)</p> </li> <li> <p>\\(\\hat{a}_A = \\frac{\\bar{A} }{ |\\bar{A}| }\\) : unit vector</p> </li> <li> <p>\\(\\bar{AB} = \\bar{B} - \\bar{A}\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#angle-between-two-vectors","title":"Angle between two vectors","text":"<ul> <li>\\(0 \\le \\theta_{AB} \\le 180^{\\circ}\\)</li> </ul>"},{"location":"basics/VectorCalculus/#dot-product","title":"Dot Product","text":"<ul> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = |\\bar{A}| |\\bar{B}| \\cos \\theta_{AB} \\rightarrow \\text{scalar}\\)</p> </li> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = 0 \\rightarrow \\text{perpendicular}\\)</p> </li> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = |\\bar{A}| |\\bar{B}|  \\rightarrow \\text{parallel}\\)</p> </li> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = - |\\bar{A}| |\\bar{B}| \\rightarrow \\text{anti-parallel}\\)</p> </li> </ul> <p>Example:</p> <ul> <li>\\(\\bar{A} \\cdot \\bar{B} = A_xB_x + A_yB_y +A_zB_z\\)</li> </ul> <p>Projection</p> <p>Projection of \\(\\bar{B}\\) in the direction of \\(\\bar{A} = \\bar{B} \\cdot \\hat{a}_A = |\\bar{B}| |\\hat{a}| \\cos \\theta_{AB} = |\\bar{B}| \\cos \\theta_{AB}\\)</p>"},{"location":"basics/VectorCalculus/#cross-product","title":"Cross Product","text":"<p>\\(\\bar{A} \\times \\bar{B} = |\\bar{A}| |\\bar{B}| \\sin \\theta_{AB} \\hat{a}_n\\)</p> <p>$\\hat{a}_n $$: normal perpendicular to plane AB while taking curl from \\(\\(\\bar{A}\\)\\) to towards $\\(\\bar{B}\\)</p> <p>Example:</p> <ul> <li>\\(\\bar{A} \\times \\bar{B} = \\begin{bmatrix} \\hat{i} &amp; \\hat{j} &amp; \\hat{k} \\\\ A_x &amp; A_y &amp; A_z \\\\ B_x &amp; B_y &amp;B_z \\end{bmatrix}\\)</li> </ul> <p>Physical Significance of curl:</p> <ul> <li> <p>\\(\\text{Area of }\\triangle ABC = \\frac{1}{2} |\\bar{AB} \\times \\bar{AC} |\\)</p> </li> <li> <p>\\(\\text{Area of }\\square ABCD = |\\bar{AB} \\times \\bar{AD} | = \\frac{1}{2} |\\bar{AC} \\times \\bar{BD} |\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#co-ordinate-system","title":"Co-ordinate System","text":""},{"location":"basics/VectorCalculus/#cartesian-coordinate-system","title":"Cartesian Coordinate System","text":"<p>Variables: \\((x, y, z)\\)</p> <p>Properties</p> <ul> <li>\\(-\\infty &lt; x &lt; \\infty\\)</li> <li>\\(-\\infty &lt; y &lt; \\infty\\)</li> <li>\\(-\\infty &lt; z &lt; \\infty\\)</li> </ul>"},{"location":"basics/VectorCalculus/#cylindrical-coordinate-system","title":"Cylindrical Coordinate System","text":"<p>Variables: \\((r, \\phi, z)\\)</p> <p>Properties</p> <ul> <li>\\(0 \\le r &lt; \\infty\\)</li> <li>\\(0 \\le \\phi &lt; 2\\pi\\)</li> <li>\\(-\\infty &lt; z &lt; \\infty\\)</li> </ul>"},{"location":"basics/VectorCalculus/#spherical-coordinate-system","title":"Spherical Coordinate System","text":"<p>Variables : \\((r, \\theta,  \\phi)\\)</p> <p>Properties</p> <ul> <li>\\(0 \\le r &lt; \\infty\\)</li> <li>\\(0 \\le \\phi &lt; 2\\pi\\)</li> <li>\\(0 \\le \\theta \\le \\pi\\)</li> </ul>"},{"location":"basics/VectorCalculus/#vector-operator","title":"Vector Operator","text":"<p>Del/Nabla operator</p> <ul> <li>\\(\\nabla = \\frac{\\partial{}}{\\partial{x}}\\bf{\\hat{i}} +  \\frac{\\partial{}}{\\partial{y}} \\bf{\\hat{j}} +  \\frac{\\partial{}}{\\partial{z}} \\bf{\\hat{k}}\\): vector</li> </ul> <p>Laplacian Operator</p> <ul> <li>\\(\\nabla \\cdot \\nabla = \\nabla^2 = \\frac{\\partial^2}{\\partial{x^2}}+  \\frac{\\partial^2}{\\partial{y^2}} +  \\frac{\\partial^2}{\\partial{z^2}}\\) : scalar</li> </ul> <p>Scalar-point function</p> <ul> <li>\\(f(x,y,z) : R^3 \\rightarrow R\\) (output scalar)</li> </ul> <p>Level-surface equation : \\(f(x,y,z) = c\\)</p> <p>Vector-point function</p> <ul> <li> <p>\\(\\bar{F}(x,y,z) : R^3 \\rightarrow R^3\\)(output vector)</p> </li> <li> <p>\\(\\bar{F} = F_x\\hat{i} + F_y\\hat{j} + F_z\\hat{k}\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#differential","title":"Differential","text":""},{"location":"basics/VectorCalculus/#gradient","title":"Gradient","text":"<p>Calculated for scalar-point function \\(f\\)</p> <p>Caution</p> <p>Convert function \\(f\\) into the form \\(f = 0\\) before calculating the gradient</p> <p>\\(\\text{grad} f  \\rightarrow \\text{vector}\\)</p> <p>\\(\\text{grad} f = \\nabla f = \\nabla f(x,y,z) = (\\frac{\\partial{f}}{\\partial{x}}, \\frac{\\partial{f}}{\\partial{y}}, \\frac{\\partial{f}}{\\partial{z}}) = \\frac{\\partial{f}}{\\partial{x}}\\bf{i} +  \\frac{\\partial{f}}{\\partial{y}} \\bf{j} +  \\frac{\\partial{f}}{\\partial{z}} \\bf{k}\\) ( direction is in the dir \\(f &gt; c\\) )</p> <p>Properties of gradient</p> <ul> <li>\\(|\\nabla f|\\): Maximum rate of change in the normal direction of \\(f\\)</li> <li>\\(\\nabla(r) = \\frac{\\bar{r}}{r}\\)</li> <li>\\(\\nabla(f(r)) = f'(r)\\frac{\\bar{r}}{r}\\)</li> <li>\\(\\nabla^2[f(r)] = f''(r) + \\frac{2}{r}f'(r)\\)</li> </ul> <p>Application of gradient</p> <ol> <li>Directional derivative (D.D.) of \\(f\\) at point \\(P\\) in the direction of \\(\\bar{M} = (\\nabla f)_{P} \\cdot \\hat{a}_M\\)</li> <li>Angle between to level surface \\(f_1\\) and \\(f_2\\) : \\(\\nabla f_1 \\cdot \\nabla f_2 = |\\nabla f_1| |\\nabla f_2| \\cos \\theta\\)</li> </ol>"},{"location":"basics/VectorCalculus/#divergence","title":"Divergence","text":"<p>Calculated for vector point function \\(\\bar{F}\\)</p> <ul> <li> <p>\\(\\nabla \\cdot \\bar{F} \\rightarrow \\text{Scalar}\\)</p> </li> <li> <p>\\(\\text{div} \\bar{F} =  \\nabla \\cdot \\bar{F} = (\\frac{\\partial{F_x}}{\\partial{x}} + \\frac{\\partial{F_y}}{\\partial{y}} + \\frac{\\partial{F_z}}{\\partial{z}})\\)</p> </li> </ul> <p>Physical significance of divergence</p> <p>1 . It is only understood at a point. i.e., \\((\\nabla \\cdot \\bar{F})_P\\)</p> <p>If \\((\\nabla \\cdot \\bar{F})_P\\)  := positive, \\(P\\) is acting as a source.</p> <p>If \\((\\nabla \\cdot \\bar{F})_P\\)  := negative, \\(P\\) is acting as a sink.</p> <ol> <li>Solenoidal Vector Field</li> </ol> <p>\\(\\nabla \\bar{F} = 0\\),  at all points \\((x,y,z)\\)</p> <p>Example: magnetic field, any field lines that makes closed loop</p> <p>Properties of divergence</p> <ul> <li>\\(\\nabla\\cdot \\bar{r} = 3\\)</li> </ul>"},{"location":"basics/VectorCalculus/#curl","title":"Curl","text":"<p>\\(\\nabla \\times \\bar{F} \\rightarrow \\text{Vector}\\)</p> <p>\\(\\nabla \\times \\bar{F} = \\begin{bmatrix} \\hat{i} &amp; \\hat{j} &amp; \\hat{k} \\\\ \\frac{\\partial}{\\partial{x}} &amp; \\frac{\\partial}{\\partial{y}} &amp;  \\frac{\\partial}{\\partial{z}} \\\\ F_x &amp; F_y &amp; F_z \\end{bmatrix}\\)</p> <p>Physical significance of curl</p> <ol> <li>It is only understood at a point. i.e., \\((\\nabla \\times \\bar{F})_P\\)</li> </ol> <p>It represents the capacity of vector field to rotate the point P</p> <ol> <li>Irrotational Vector Field/Conservative Vector Field : \\((\\nabla \\times \\bar{F})_P = \\bar{0}, \\forall P\\)</li> </ol> <p>\\((\\nabla \\times \\bar{E}) = \\bar{0}\\) , where \\(\\bar{E}\\) is static electric field.</p> <p>Properties of curl</p> <ul> <li>\\(\\nabla \\times  \\bar{r} = \\bar{0}\\), where \\(\\bar{r}\\) is positional vector.</li> </ul> <p>Null Identity</p> <ul> <li> <p>Scalar: \\(\\nabla \\cdot [\\nabla \\times \\bar{A}] = 0\\)</p> </li> <li> <p>Vector: \\(\\nabla \\times [\\nabla \\phi] = \\bar{0}\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#laplacian","title":"Laplacian","text":"<p>\\(\\nabla^2 f = \\nabla \\cdot (\\nabla f) = \\frac{\\partial^2 f}{\\partial{x}^2} + \\frac{\\partial^2 f}{\\partial{y}^2} + \\frac{\\partial^2 f}{\\partial{z}^2}\\)</p> <p>Properties of laplacian</p> <ul> <li>Rotation invariant</li> <li>\\(\\nabla^2(fg) = f \\nabla^2 g + 2(\\nabla f \\cdot \\nabla g) + g \\nabla^2 f\\)</li> <li>Laplace's equation: \\(\\nabla^2 f = 0\\)</li> <li>Poisson's equation: \\(\\nabla^2 f = - \\rho\\)</li> </ul>"},{"location":"basics/VectorCalculus/#integral","title":"Integral","text":""},{"location":"basics/VectorCalculus/#line-integral","title":"Line Integral","text":"<p>\\(\\int_C \\bar{F} \\cdot \\bar{d}l\\)</p> <p>Closed line encircling an Area.</p> <p>Non-conservative Vector Field</p> <ul> <li>Depends on endpoints</li> <li>Path dependent</li> <li>\\(\\oint_C \\bar{F} \\cdot \\bar{d}l \\neq 0\\)</li> </ul> <p>Conservative Vector Field</p> <ul> <li>Depends on endpoints</li> <li>Path independent</li> <li>\\(\\oint_C \\bar{F} \\cdot \\bar{d}l = 0\\)</li> <li>\\(F = \\nabla f\\) for some scalar field</li> <li>\\(\\int_A^B \\bar{F} \\cdot \\bar{d}l = f(B) - f(A)\\)</li> </ul>"},{"location":"basics/VectorCalculus/#surface-integral","title":"Surface Integral","text":"<p>\\(\\int_S \\bar{F} \\cdot \\bar{d}s\\) or \\(\\int \\int_S \\bar{F} \\cdot \\bar{d}s\\)</p> <p>Open Surface: Circle, Rectangle etc.</p> <p>Closed Surface: Closed Surface Enclosing a Volume (Hollow).</p> <p>Level Surface: \\(f(x,y,z) =constant\\)</p> <p>Area Vector: \\(\\bar{S} =|\\bar{S}|\\hat{a}_S\\)</p> <p>\\(\\hat{a}_S \\rightarrow\\) always normal to the surface</p> <p>Open Surface Integral of Vector Field</p> <p>\\(\\int \\int_S \\bar{F} \\cdot \\bar{d}s = \\psi\\)</p> <p>Measure of field lines of Vector \\(bar{F}\\) crossing surface \\(S\\), perpendicularly = Flux</p> <p>\\(\\psi\\) is flux of \\(\\bar{F}\\) through \\(S\\)</p>"},{"location":"basics/VectorCalculus/#volume-integral","title":"Volume Integral","text":"<p>\\(\\int \\int \\int_V V dv\\)</p>"},{"location":"basics/VectorCalculus/#stokes-theorem","title":"Stoke's Theorem","text":"<p>\\(\\oint_C \\bar{A} \\cdot \\bar{dl} = \\int\\int_S (\\nabla \\times \\bar{A}) \\cdot \\bar{ds}\\)</p> <p>A is continuous and differential at every point inside C</p> <p>If Vector Field is irrotational then it has to be Conservative vector field. But converse is not necessarily true.</p> <p>If : \\(\\nabla \\times \\bar{A} = \\bar{0}\\) then \\(\\oint_C \\bar{A} \\cdot \\bar{dl} = 0\\)</p> <p>Inverse Stoke's Theorem :</p> <p>\\(\\int\\int_S (\\nabla \\times \\bar{A}) \\cdot \\bar{ds} =\\oint_C \\bar{A} \\cdot \\bar{dl}\\)</p>"},{"location":"basics/VectorCalculus/#greens-theorem","title":"Green's Theorem","text":"<p>\\(\\oint_C M(x,y) dx + N(x,y) dy = \\oint M dx + N dy = \\int \\int_R (\\frac{\\partial N}{\\partial x} - \\frac{\\partial M}{\\partial y})dxdy\\) if C is in anti-clockwise direction and if C is in clock-wise direction \\(-\\int \\int_R (\\frac{\\partial N}{\\partial x} - \\frac{\\partial M}{\\partial y})dxdy\\)</p> <p>M, N are continuous and differential at every point C.</p>"},{"location":"basics/VectorCalculus/#divergence-theorem","title":"Divergence Theorem","text":"<p>\\(\\oint_S \\bar{A} \\cdot \\bar{ds} = \\int \\int \\int_V (\\nabla \\cdot \\bar{A}) dv\\)</p>"},{"location":"basics/bayesian/","title":"Bayesian","text":""},{"location":"basics/bayesian/#bayesian","title":"Bayesian","text":""},{"location":"basics/bayesian/#question","title":"Question","text":"<ul> <li>what is bayesian inference ?</li> </ul>"},{"location":"casestudy/","title":"Case Study","text":""},{"location":"casestudy/#case-study","title":"Case Study","text":"<ul> <li> <p>When a user adds an item to their cart, we need to calculate the probability that they will return the product (given that they complete the order). Using this probability, the company can adjust shipping charges accordingly. For example, if the probability of return is high, the company may increase shipping charges to discourage potential returns. How would you calculate this probability?</p> </li> <li> <p>Can you explain what A/B Testing is? How would you take a uniform sample set of 10M users from streaming data of approximately 10B users, ensuring that every user has equal probability of being in the final sampled set?</p> </li> <li> <p>In a scenario where you have a small sample size of approximately 20 observations, which statistical test would you use and why?</p> <ul> <li>This tests understanding of:<ul> <li>Small sample size statistics</li> <li>When to use parametric vs non-parametric tests</li> <li>Knowledge of t-test vs z-test vs other tests</li> </ul> </li> </ul> </li> <li> <p>In a credit card fraud detection system, how would you determine the optimal classification threshold? While ROC-AUC is one metric, what other considerations and metrics should be taken into account given that:</p> <ul> <li>False positives (legitimate transactions flagged as fraud) cause customer inconvenience</li> <li>False negatives (missed fraud) cause direct financial loss</li> </ul> </li> <li> <p>You need to design a recommendation system to replace an existing rule-based job search system. Provide a detailed plan</p> </li> </ul>"},{"location":"cv/","title":"Computer Vision","text":""},{"location":"cv/#overview","title":"Overview","text":""},{"location":"cv/#core","title":"Core","text":"<ul> <li> Basics</li> <li> Image Operations</li> <li> Image Annotations</li> <li> Image Enhancement</li> <li> Image Filtering</li> <li> Image Feature</li> <li> Image Alignment</li> <li> Panorama</li> <li> HDR</li> <li> Image Classification</li> <li> Image Segmentation</li> <li> Edge Detection &amp; Contours</li> <li> Object Detection</li> <li> Face Detection</li> <li> Optical Flow</li> <li> Object Tracking</li> <li> Pose Estimation</li> <li> OCR</li> </ul>"},{"location":"cv/#advance","title":"Advance","text":"<ul> <li> AutoEncoder</li> <li> ViT</li> <li> NeRF</li> </ul>"},{"location":"cv/#generative","title":"Generative","text":"<ul> <li> Variational Encoder</li> </ul>"},{"location":"cv/classification/","title":"Classification","text":""},{"location":"cv/classification/#classification","title":"Classification","text":""},{"location":"cv/classification/#question","title":"Question","text":"Given a lot of unlabelled data along with a small amount of labelled data, how will you use this unlabelled data to improve your classification model on labelled data. <ul> <li>TACLE</li> </ul>"},{"location":"cv/cnn/","title":"CNN","text":""},{"location":"cv/cnn/#convolutional-neural-network","title":"Convolutional Neural Network","text":"<p>\\(W_{new} = \\frac{W_{old} - F + 2P}{S} +1\\)</p> <p>\\(H_{new} = \\frac{H_{old} - F + 2P}{S} +1\\)</p> <p>How is learning kernel is different from regular feed-forward NN? - Weight Sharing - Sparse Network</p> <p>CNN tries to learn ecific characterstics of inputs, different neurons can fire for different characterstics.</p>"},{"location":"cv/cnn/#question","title":"Question","text":"<ul> <li>why cnn is more robust then dnn ? how is it translation invariant ?</li> </ul> Given two CNN with the same number of parameters, but one is of single layer while other is a 2-layer network. Which one you will choose and why? <ul> <li>2-layer CNN has better feature learning capability and expressiveness.<ul> <li>First layer learns basic features (edges, corners) and second layer combines these to learn more complex patterns</li> <li>Additional ReLU between layers adds more non-linearity</li> </ul> </li> </ul> Analyze the differentiability of the MaxPool function commonly used in CNNs? <ul> <li> <p>MaxPool Properties:</p> <ul> <li>Non-linear function</li> <li>Not differentiable at points where maximum changes</li> <li>Piece-wise continuous</li> <li>Only one input contributes to output</li> </ul> </li> <li> <p>Derivative Behavior:</p> <ul> <li>Gradient = 1 for max element</li> <li>Gradient = 0 for all other elements</li> <li>Not defined when two elements are equal</li> </ul> </li> </ul>"},{"location":"cv/detection/","title":"Object Detection","text":""},{"location":"cv/detection/#object-detection","title":"Object Detection","text":""},{"location":"cv/detection/#introduction","title":"Introduction","text":"<p>The goal of object detection is to predict a set of bounding boxes(x,y,w,h) and category labels for each object of interest.</p>"},{"location":"cv/detection/#traditional","title":"Traditional","text":""},{"location":"cv/detection/#template-matching-sliding-window","title":"Template Matching + Sliding Window","text":"<p>For every position you evaluate how much do the pixels in the image and template correlate.</p> <p>Cons</p> <ol> <li>Does not handle occlusions.</li> <li>Works with instance of object but not with class of it.</li> <li>Does not work if pose changes.</li> <li>Does not work if position, scale and aspect ratio changes.</li> </ol>"},{"location":"cv/detection/#feature-extraction-and-classification","title":"Feature Extraction and Classification","text":"<p>Learn multiple weak classifier to build a strong final decision.</p>"},{"location":"cv/detection/#feature-extraction","title":"Feature Extraction","text":"<p>Viola-Jones Detector</p> <p>Haar Features</p> <p>Histogram of Oriented Gradients(HOGs) Compute gradients in dense grids, compute gradients and create a histogram based on gradient direction</p> <p>Deformable Part Model (DPM) Based on HOG features but based on body part detection. More robust to different body poses.</p>"},{"location":"cv/detection/#classification","title":"Classification","text":"<p>It is done with the help of SVM.</p>"},{"location":"cv/detection/#general-object-detection","title":"General Object Detection","text":"<ul> <li>Class agnostic</li> <li>Object Proposals / Region of Intrest<ul> <li>Selective search</li> <li>Edge boxes</li> </ul> </li> </ul> <p>Localization</p>"},{"location":"cv/detection/#two-stage-detector","title":"Two-Stage Detector","text":"<ul> <li>R-CNN, Fast R-CNN, Faster R-CNN</li> <li>SPP-Net, R-FCN, FPN</li> </ul> <ul> <li>Overfeat</li> <li>R-CNN, Fast R-CNN, Faster R-CNN, SPP-Net</li> </ul>"},{"location":"cv/detection/#one-stage-detector","title":"One-Stage Detector","text":"<p>No need of Region Proposal Network</p> <p>They are very fast</p> <ul> <li>YOLO, SSD, RetinaNet</li> <li>CenterNet, CornerNet, ExtremeNet</li> </ul> <ul> <li>YOLO</li> <li>RetinaNet</li> <li>CornerNet</li> <li>CenterNet</li> <li>ExtremeNet</li> </ul>"},{"location":"cv/detection/#transformer-based-detector","title":"Transformer-Based Detector","text":"<ul> <li>DETR</li> </ul>"},{"location":"cv/detection/#methods","title":"Methods","text":"<ul> <li>Swin Transformer</li> <li>DINO</li> <li>InternImage</li> <li>OWL</li> </ul>"},{"location":"cv/detection/CenterNet/","title":"CenterNet","text":""},{"location":"cv/detection/CenterNet/#centernet","title":"CenterNet","text":"<ul> <li>Focus on the center of the object to infer its class.</li> <li>Use the corners as proposals, and the center to verify the class of the object and filter out outliers.</li> </ul>  CenterNet Architecture   Center Pooling Module"},{"location":"cv/detection/CornerNet/","title":"CornerNet","text":""},{"location":"cv/detection/CornerNet/#cornernet","title":"CornerNet","text":"<p>Bounding box cordinates as top-left and bottom-right corner.</p>  Hourglass network   Corner pooling"},{"location":"cv/detection/CornerNet/#issues","title":"Issues","text":"<ul> <li>Many incorrect bounding boxes (especially small) \\(\\rightarrow\\) too many False Positives</li> <li>Hypothesis: It is hard to infer the class of the box if the network is focused on the boundaries</li> </ul>"},{"location":"cv/detection/DETR/","title":"DETR","text":""},{"location":"cv/detection/DETR/#detr","title":"DETR","text":"<p>A direct set prediction approach to bypass the surrogate tasks (like proposals, anchors, window centers, non-maximum suppression).</p> <p>A encoder-decoder based architecure.</p> <p>It predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.</p>"},{"location":"cv/detection/DETR/#components","title":"Components","text":""},{"location":"cv/detection/DETR/#set-prediction","title":"Set Prediction","text":"<p><code>Bipartite matching loss</code></p> <p>The usual solution is to design a loss based on the Hungarian algorithm, to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match.</p>"},{"location":"cv/detection/DETR/#transformers-and-parallel-decoding","title":"Transformers and Parallel Decoding","text":"<p>Combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction.</p>"},{"location":"cv/detection/DETR/#object-detection","title":"Object detection","text":"<ol> <li>Two-Stage detectors</li> <li>One-Stage detectors</li> </ol> <p>The final performance of above systems heavily depends on the exact way these initial guesses are set.</p>"},{"location":"cv/detection/DETR/#model","title":"Model","text":""},{"location":"cv/detection/DETR/#object-detection-set-prediction-loss","title":"Object detection set prediction loss","text":"<p>DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image.</p> <ul> <li> <p>\\(y:\\) ground-truth (\\(y_i = (c_i, b_i)\\), \\(c_i\\) target class label and \\(b_i \\in [0,1]^4\\) is ground-truth box.)</p> </li> <li> <p>\\(\\hat{y} = \\{\\hat{y}_i\\}_{i=1}^{N}:\\) set of N predictions</p> </li> <li> <p>\\(y\\) also as a set of size \\(N\\) padded with \\(\\varnothing\\) (no object)</p> </li> </ul> <p>Permutation of element with lowest cost</p> <ul> <li>\\(\\hat{\\sigma} = \\arg\\min_{\\sigma\\in\\mathfrak{G}_N} \\sum_{i}^N \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)})\\)</li> </ul> <p>Pair-wise matching cost</p> <p>\\(\\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)}) = -\\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\hat{p}_{\\sigma(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)})\\)</p> <p>\\(\\hat{p}_{\\sigma_i}(c_i):\\) prediction with index \\(\\sigma(i)\\) we define probability of class \\(c_i\\)</p> <p>Hungarian loss</p> <p>\\(\\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^N \\left[-\\log \\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)})\\right]\\)</p> <p>Box-Loss</p> <p>\\(\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{\\text{L1}}\\|b_i - \\hat{b}_{\\sigma(i)}\\|_1\\)</p>"},{"location":"cv/detection/DETR/#detr-architecture","title":"DETR architecture","text":"<p>CNN Backbone</p> <p>As a feature extractor</p> <p>Transformer encoder</p> <p>Since the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings that are added to the input of each attention layer.</p> <p>Transformer decoder</p> <p>The difference with the original transformer is that our model decodes the N objects in parallel at each decoder layer, while transformer use an autoregressive model that predicts the output sequence one element at a time.</p> <p>Prediction feed-forward networks (FFNs)</p> <p>The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer.</p> <p>Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total N = 100 prediction slots in DETR decoder. </p> <p>This shows that the prediction box focuses on different part of the image.</p>"},{"location":"cv/detection/DETR/#rt-detr","title":"RT-DETR","text":""},{"location":"cv/detection/DETR/#fast-detr","title":"Fast-DETR","text":""},{"location":"cv/detection/DETR/#detrs-fastdetr","title":"DETRs-FastDETR","text":""},{"location":"cv/detection/DETR/#sam-det","title":"SAM-Det","text":""},{"location":"cv/detection/DETR/#ultra-detr","title":"ULTRA-DETR","text":""},{"location":"cv/detection/DINO/","title":"DINO","text":""},{"location":"cv/detection/DINO/#dino","title":"DINO","text":""},{"location":"cv/detection/DINO/#grounding-dino","title":"Grounding DINO","text":""},{"location":"cv/detection/ExtremeNet/","title":"ExtremeNet","text":""},{"location":"cv/detection/ExtremeNet/#extremenet","title":"ExtremeNet","text":"<ul> <li>Precticting the corner where object does not lies is hard for CNNs.</li> <li>Represent objects by their extreme points.</li> <li>No need to predict embeddings for the box computation.</li> </ul>  ExtremeNet Arch."},{"location":"cv/detection/ExtremeNet/#applications","title":"Applications","text":"<ul> <li>Extreme points are used commonly for annotation</li> </ul>"},{"location":"cv/detection/InternImage/","title":"InternImage","text":""},{"location":"cv/detection/OWL/","title":"OWLv2","text":""},{"location":"cv/detection/Overfeat/","title":"Overfeat","text":""},{"location":"cv/detection/Overfeat/#overfeat","title":"Overfeat","text":"<p>Slidingwindow + bbox regression + classification</p>"},{"location":"cv/detection/Overfeat/#sliding-window","title":"Sliding Window","text":"<p>Implicity encoded in the CNN architecture. Use sliding widow at different scale.</p>"},{"location":"cv/detection/Overfeat/#localization","title":"Localization","text":"<p>Regression</p>"},{"location":"cv/detection/Overfeat/#detection","title":"Detection","text":"<p>Classification</p>"},{"location":"cv/detection/Overfeat/#cons","title":"Cons","text":"<ul> <li>Needs fixed sized window as the fully-connected layer need to have fixed input.</li> <li>Expensive to try out all the possible positions, scales and aspect ratio. (Choose only the potential location)</li> </ul>"},{"location":"cv/detection/RCNN/","title":"RCNNs","text":""},{"location":"cv/detection/RCNN/#rcnn","title":"RCNN","text":""},{"location":"cv/detection/RCNN/#rcnn_1","title":"RCNN","text":"<p>Steps</p> <ol> <li> <p>Scan the input image for possible objects using an algorithm called Selective Search, generating ~2000 region proposals</p> </li> <li> <p>Warp to a fix size 227 x 227</p> </li> <li> <p>Run a convolutional neural net (CNN) on top of each of these region proposals</p> </li> <li> <p>Make the output of each CNN and feed it into:</p> <p>a) an SVM to classify the region and </p> <p>b) a linear regressor to tighten the bounding box of the object, if such an object exists.</p> </li> </ol>"},{"location":"cv/detection/RCNN/#training","title":"Training","text":"<ol> <li>Pre-train the CNN on ImageNet</li> <li>Finetune the CNN on the number of classes the detector is aiming to classify (softmax loss).</li> <li>Train a linear Support Vector Machine classifier to classify image regions. One SVM per class! (hinge loss)</li> <li>Train the bounding box regressor (L2 loss)</li> </ol> <p>Cons 1. If we have overlapping window then we will do ConvNet computation for each of the pixels more than 1 times. This increases extra computation.</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#spp-net","title":"SPP Net","text":"<p>Makes the RCNN fast at test time.</p> <p>Issues</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#fast-rcnn","title":"Fast RCNN","text":"<ol> <li>Performing feature extraction over the image before proposing regions, thus only running one CNN over the entire image instead of 2000 CNN\u2019s over 2000 overlapping regions.</li> <li>After conv5 there is FC layer we need to make all the deature size need to be of same size using RoI Pooling layer.</li> <li>Replacing the SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn","title":"Faster RCNN","text":"<ul> <li>Removes region proposal network Can we reuse our CNN feature and still be able to create this proposal.</li> </ul> <p>How to extract proposals.</p> <ul> <li> <p>How many proposals?</p> <ul> <li>Decide a fix number</li> <li>set of 9 anchor box per location (3 scales, 3 aspect ratio)</li> </ul> </li> <li> <p>Where are they placed?</p> <ul> <li>Densly</li> </ul> </li> <li> <p>For each of the location get the descriptor of 256-d by 3X3 filtermap</p> </li> <li> <p>Pass the descriptor to the classification layer and regression layer</p> </li> </ul>"},{"location":"cv/detection/RCNN/#rpn-training","title":"RPN Training","text":"Region Proposal Network  <p>Classification Ground-Truth</p> <p>\\(p^{*}\\) Amount of anchor box overlapping with the Ground-Truth.</p> <p>\\(p^{*} = 1\\) if IoU &gt; 0.7 (anchor is in foreground)</p> <p>\\(p^{*} = 0\\) if IoU &lt; 0.3 (anchor is in background)</p> <p>For training only consider above two case.</p> <ol> <li>Randomly sample 256 sample, form mini-batch.</li> <li>Calculate binary CE loss</li> <li>Anchor box containing object will go through regression box</li> </ol> <p>Anchor box \\((x_a, y_a, w_a, h_a)\\), \\(x_a, y_a\\) is center of box and rest width and height respectively.</p> <ol> <li>Network actually predicts are \\((t_x, t_y, t_w, t_h)\\) which are relative.</li> </ol> <p>\\(t_x = (x-x_a)/w_a\\)</p> <p>\\(t_y = (y-y_a)/h_a\\)</p> <p>\\(t_w = \\log(w/w_a)\\)</p> <p>\\(t_h = \\log(h/h_a)\\)</p> <ol> <li>Smooth L1 loss on regression targets</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn-training","title":"Faster RCNN Training","text":"<p>Can be train jointly. But in paper it is trained in following manner.     - RPN classification (object/non-object)     - RPN regression (anchor -&gt; proposal)     - Fast R-CNN classification (type of object)     - Fast R-CNN regression (proposal -&gt; box)</p> <p>Pros</p> <ol> <li>10x faster at test time wrt Fast R-CNN</li> <li>Trained end-to-end including feature extraction, region proposals, classifier and regressor.</li> <li>More accurate, since proposals are learned. RPN is fully convolutional.</li> </ol>"},{"location":"cv/detection/RCNN/#conclusion","title":"Conclusion","text":"R-CNN Fast RCNN Faster RCNN Test time per image (sec) 50 2 0.2 Speeed-Up 1X 25X 250X mAP (VOC 2007) 66.0 66.9 66.9"},{"location":"cv/detection/RetinaNet/","title":"RetinaNet","text":""},{"location":"cv/detection/RetinaNet/#retinanet","title":"RetinaNet","text":"<p>Since there are lots of anchor box in which there is no object and very few of them object. We need to incorporate this information in the loss function which is done with the weighting of the loss function.</p>  Focal Loss  <p>As \\(\\gamma\\) increases the easy sample weight decreases.</p>"},{"location":"cv/detection/RetinaNet/#key-point","title":"Key Point","text":"<ul> <li>Proposed: Focal loss</li> <li>Powerful feature extraction: ResNet</li> <li>Multi-scale prediction</li> <li>9 anchors per level, each one with a classification and regression target</li> </ul>"},{"location":"cv/detection/SSD/","title":"SSD","text":""},{"location":"cv/detection/SelectiveSearch/","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#selective-search","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#wip","title":"WIP","text":""},{"location":"cv/detection/SwinTransformer/","title":"Swin Transformer","text":""},{"location":"cv/detection/YOLO-World/","title":"YOLO-World","text":""},{"location":"cv/detection/YOLO-World/#yolo-world","title":"YOLO-World","text":"<ul> <li> <p>YOLO with openvocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets.</p> </li> <li> <p>Propose a new Re-parameterizable VisionLanguage Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information.</p> </li> <li> <p>\\(\\Omega = \\{B_i, t_i \\}_{i=1}^{N}\\) where \\(t_i\\) is the corresponding text for the region \\(B_i\\)</p> </li> </ul>  YOLO-World Architecture   YOLO-World Architecture  <p>Architecture 1. YOLO Detector      - YOLOv8 2. Text Encoder      - CLIP 3. Text Contrastive Head      - object-text similarity     - \\(s_{k,j} = \\alpha \\cdot \\text{L2-Norm}(e_k) \\cdot \\text{L2-Norm}(w_j)^T + \\beta,\\)     - \\(e_k\\) object embedding and \\(w_j\\) word embedding     - \\(\\alpha\\) and \\(\\beta\\) are learnable.</p>"},{"location":"cv/detection/YOLO-World/#re-parameterizable-vision-language-pan","title":"Re-parameterizable Vision-Language PAN","text":"<ul> <li>establish the feature pyramids {P3,P4,P5} with the multi-scale image features {C3,C4,C5}</li> <li>propose the Text-guided CSPLayer (T-CSPLayer) and Image-Pooling Attention (I-Pooling Attention) to further enhance the interaction between image features and text features, which can improve the visual-semantic representation for open-vocabulary capability</li> </ul>  RepVL-PAN"},{"location":"cv/detection/YOLO-World/#text-guided-csplayer","title":"Text-guided CSPLayer","text":"<p>Text-guided cross-stage partial layers</p> <ul> <li>text embeddings W</li> <li>Image features \\(X_l \\in R^{H \\times W \\times D} (l \\in \\{3, 4, 5 \\})\\)</li> <li>\\(\\delta\\) sigmoid function \\(X_l' = X_l \\cdot \\delta(\\max_{j \\in \\{1..C\\}} (X_lW_j^{\\top}))\\)</li> </ul>"},{"location":"cv/detection/YOLO-World/#image-pooling-attention","title":"Image-Pooling Attention","text":"<ul> <li>Max-Pooling output \\(\\tilde{X}\\)</li> </ul> <p>\\(W' = W + \\text{MultiHead-Attention}(W, \\tilde{X}, \\tilde{X})\\)</p>"},{"location":"cv/detection/YOLO-World/#training","title":"Training","text":"<p>\\(\\mathcal{L}(I) = \\mathcal{L}_{con} + \\lambda_I \\cdot (\\mathcal{L}_{iou} + \\mathcal{L}_{dfl})\\)</p> <ul> <li>\\(\\mathcal{L}_{con}\\) : region-text contrastive loss</li> <li>\\(\\mathcal{L}_{dfl}\\) : distributed focal loss</li> <li>\\(\\lambda_I \\in \\{0, 1\\}\\) <ul> <li>1 if \\(I\\) is from detection or grounding data</li> <li>0 if \\(I\\) is from the image-text data</li> </ul> </li> </ul>"},{"location":"cv/detection/YOLO/","title":"YOLO","text":""},{"location":"cv/detection/YOLO/#yolo","title":"YOLO","text":"<p>It does not have region proposal network and also it does not have the fully-connected layer. </p>  You Only Look Once"},{"location":"cv/detection/YOLO/#process","title":"Process","text":"<ol> <li>Divide the image into grid (SxS cells).</li> <li>Predict B anchor box at the center of each box along with the confidence score</li> <li>Predict C classes for each grid cell.</li> </ol> <p>YOLO-Tensor : SxS(Bx5 + C) </p> <p>where, - SxS : number of grid</p> <ul> <li> <p>B : Number of bbox \\((P_c, b_x, b_y, b_h, b_w)\\)</p> </li> <li> <p>C : Number of classes</p> </li> </ul>  Predicting bounding box and confidence score for each cell.   Predicting class probability for each cell."},{"location":"cv/detection/YOLO/#loss-function","title":"Loss Function","text":"<p>\\(L_{total} = L_{localization} + L_{confidence} + L_{classification}\\)</p> <p>\\(L_{localization} = \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 + (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2]\\)</p> <p>\\(L_{confidence} =  \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 + \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} (C_i - \\hat{C}_i)^2\\)</p> <p>\\(L_{classification} = \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum_{c\\in classes} (p_i(c) - \\hat{p}_i(c))^2\\)</p> <p>\\(\\lambda_{coord} = 5\\)</p> <p>\\(\\lambda_{noobj} = 0.5\\)</p>"},{"location":"cv/detection/YOLO/#yolo-papers","title":"YOLO Papers","text":""},{"location":"cv/detection/YOLO/#yolo-survey","title":"YOLO Survey","text":""},{"location":"cv/detection/YOLO/#yolo-v1-slides","title":"YOLO-v1 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v2-slides","title":"YOLO-v2 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v3","title":"YOLO-v3","text":""},{"location":"cv/detection/YOLO/#yolo-v4","title":"YOLO-v4","text":""},{"location":"cv/detection/YOLO/#yolo-v5-colab","title":"YOLO-v5 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v6-colab","title":"YOLO-v6 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v7-colab","title":"YOLO-v7 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v8","title":"YOLO-v8","text":""},{"location":"cv/detection/YOLO/#yolo-nas","title":"YOLO-NAS","text":""},{"location":"cv/detection/YOLOE/","title":"YOLOE","text":""},{"location":"cv/faceDetection/","title":"Face Detection","text":""},{"location":"cv/faceDetection/#face-detection","title":"Face Detection","text":""},{"location":"cv/faceDetection/#haar-cascade","title":"Haar Cascade","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#hog-svm","title":"HoG + SVM","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#facenet","title":"FaceNet","text":"FaceNet   Triplet Loss"},{"location":"cv/faceDetection/#loss","title":"Loss","text":"<p>\\(L = \\sum_{i}^{N} \\left[\\|f(x_i^a) - f(x_i^p)\\|_2^2 - \\|f(x_i^a) - f(x_i^n)\\|_2^2 + \\alpha\\right]_+\\)</p>"},{"location":"cv/nerf/","title":"NeRF","text":""},{"location":"cv/nerf/#nerf-video","title":"NeRF Video","text":""},{"location":"cv/nerf/#code","title":"Code","text":""},{"location":"cv/objectTracking/","title":"Object Tracking","text":""},{"location":"cv/objectTracking/#object-tracking","title":"Object Tracking","text":"<p>To continuously locate and maintain the identity of target objects as they move through video frames.</p> <p>Tracking is similarity measurement, correlation, correspondence, Matching/retrieval &amp; data association.</p>"},{"location":"cv/objectTracking/#learining","title":"Learining","text":""},{"location":"cv/objectTracking/#appearance","title":"Appearance","text":"<p>we need to know how the target looks like - Single object tracking - Re-identification</p>"},{"location":"cv/objectTracking/#motion","title":"Motion","text":"<p>To make predictions of where the targets goes - Trajectory prediction</p>"},{"location":"cv/objectTracking/#single-target-tracking","title":"Single Target Tracking","text":""},{"location":"cv/objectTracking/#as-a-matchingcorrespondence-problem","title":"As a matching/correspondence problem","text":"<ul> <li>GOTURN: no online appearance modeling</li> </ul> <p>Input: what to track?</p> <p>Architecture: conv + concatenate + FC</p> <p>Pros: - No training, very fast as it is juct template matching problem</p> <p>Cons: - Does not work if the object moves very fast and goes out of search window.</p>"},{"location":"cv/objectTracking/#as-an-appearance-learning-problem","title":"As an appearance learning problem","text":"<ul> <li>MDNet: quick online finetuning of the network</li> <li>Slow: not suitable for real-time applications</li> <li>Solution: train as few layers as possible</li> </ul> <p>At test time, we need to train fc6 (up to fc4 if wanted)</p> <p>Pros: - No previous location assumption, the object can move anywhere in the image</p> <p>Cons: - Not as fast as GOTURN</p>"},{"location":"cv/objectTracking/#as-a-temporal-prediction-problem","title":"As a (temporal) prediction problem","text":"<ul> <li>ROLO = CNN + LSTM</li> </ul> <p>LSTM receives the heatmap for the object\u2019s position and the 4096 descriptor of the image</p>"},{"location":"cv/objectTracking/#challanges","title":"Challanges","text":"<ul> <li>Occlusions</li> <li>Viewpoint/pose/blur/illumination variations (in a few frames of a sequence)</li> <li>Background clutter</li> </ul>"},{"location":"cv/objectTracking/#multiple-object-tracing","title":"Multiple Object Tracing","text":""},{"location":"cv/objectTracking/#online-tracking","title":"Online Tracking","text":"<ul> <li>Processes two frames at a time</li> <li>For real-time applications</li> <li>Prone to drifting \u00e0 hard to recover from errors or occlusions</li> </ul> <p>Process - Track initialization (e.g. using a detector) - Prediction of the next position (motion model)     - Kalman filter     - Recurrent architecture     - constant velocity model (works really well at high framerates and without occlusions!)</p> <ul> <li>Matching predictions with detections (appearance model)<ul> <li>Bipartite matching</li> </ul> </li> </ul>"},{"location":"cv/objectTracking/#track-initialization","title":"Track initialization","text":"<p>Making a detector into a tracktor - Tracktor: a method trained as a detector but with tracking capabilities. - Where did the detection with ID1 go in the next frame? </p> <p>Two-Step Detector - Region Proposal - Regression</p> <p>Pros - Tracktor are online - We can train our model on still images - We can reuse an extremely well-trained regressor</p> <p>Cons - Confusion in crowded places as there is no notion of identification. - The track is killed if the target becomes occluded. - Will not work if the object or the camera has large motions.</p> <p>1st &amp; 2nd can be solved using ReID (Re-Identification). While 2nd &amp; 3rd can be solved using Motion model.</p> <p>Modeling Appearence : Re-ID Modeling Motion : Model Motion </p>"},{"location":"cv/objectTracking/#prediction-of-the-next-position","title":"Prediction of the next position","text":""},{"location":"cv/objectTracking/#matching-predictions-with-detections-appearance-model","title":"Matching predictions with detections (appearance model)","text":""},{"location":"cv/objectTracking/#offline-tracking","title":"Offline Tracking","text":"<ul> <li>Processes a batch of frames</li> <li>Good to recover from occlusions</li> <li>Not suitable for real-time applications</li> <li>Suitable for video analysis</li> </ul>"},{"location":"cv/objectTracking/#challanges_1","title":"Challanges","text":"<ul> <li>Multiple objects of the same type</li> <li>Heavy occlusions</li> <li>Appearance is often very similar</li> </ul>"},{"location":"cv/objectTracking/#applications","title":"Applications","text":"<ul> <li>Surveillance &amp; Security</li> <li>Traffic Monitoring</li> <li>Autonomous Vehicles</li> <li>Sports Analytics</li> <li>Human-Computer Interaction</li> <li>Medical Imaging</li> <li>Robotics</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/","title":"Bipartite Matching","text":""},{"location":"cv/objectTracking/BipartiteMatching/#bipartite-matching","title":"Bipartite Matching","text":"<ul> <li>Links detections with predictions using distance metrics</li> <li>Uses IoU, Pixel, or 3D distances between boxes</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#process","title":"Process","text":"<ul> <li>Calculate distances between boxes</li> <li>Apply Hungarian algorithm for optimal matching</li> <li>Set thresholds to handle missing/unsuitable matches</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#edge-cases","title":"Edge Cases","text":"<ul> <li>Missing prediction: Add dummy nodes</li> <li>No suitable match: Use cost threshold</li> <li>Unmatched boxes: Create new tracks</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#output","title":"Output","text":"<ul> <li>Matched pairs</li> <li>New tracks from unmatched detections</li> <li>Lost tracks from unmatched predictions</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#hungarian-algorithm","title":"Hungarian algorithm","text":"<p>Demo</p>"},{"location":"cv/objectTracking/KalmanFilter/","title":"Kalman Filter","text":""},{"location":"cv/opticalFlow/","title":"Optical Flow","text":""},{"location":"cv/opticalFlow/#optical-flow","title":"Optical Flow","text":"<p>Optical flow refers to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer (camera) and the scene. It helps understand how objects move in a sequence of images.</p> <p>The optical flow problem involves estimating a dense vector field where each vector represents the displacement of a pixel from one frame to the next. This displacement field captures both the magnitude and direction of motion for each point in the image.</p> <p>Key characteristics of optical flow: 1. Dense Motion Field: Unlike feature tracking which follows specific points, optical flow estimates motion for every pixel in the image 2. Temporal Consistency: Assumes that pixel intensities remain constant between consecutive frames (brightness constancy assumption) 3. Spatial Smoothness: Nearby pixels tend to move in similar ways (smoothness constraint)</p>"},{"location":"cv/opticalFlow/#methods","title":"Methods:","text":"<ul> <li>Patched Based<ol> <li>Lucas-Kanade</li> <li>Horn-Shunck</li> </ol> </li> <li>NN Based<ol> <li>FlowNet</li> </ol> </li> </ul>"},{"location":"cv/opticalFlow/#perception-of-motion","title":"Perception of Motion","text":"Figure 1: Illustration of optical flow and motion perception. The top left image shows a person running with a static camera (no camera motion), while the top right image shows the same person running but with the camera moving in the opposite direction. The bottom diagrams visualize the resulting optical flow vectors for each scenario. <p>Assuming Image intensity is constant.</p> <p>Brightness Constancy Equation:</p> <p>\\(I(x,y,t) \\approx I(x+dx,y+dy,t+dt)\\)</p> <p>Using Taylor Series Expansion:</p> <p>\\(I(x(t)+u.\\Delta t,y+v.\\Delta t) - I(x(t),y(t),t) \\approx 0\\)</p> <p>\\(I_x \\cdot u +  I_y \\cdot v + I_t = 0\\) (Brightness Constancy Constraint)</p> <p>\\([u, v]\\) is the optical flow.</p>"},{"location":"cv/opticalFlow/#lucas-and-kanade","title":"Lucas and Kanade","text":"<p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\(2(I_xu+ I_yv+ I_t)I_x = 2(I_xu+ I_yv+ I_t)I_y = 0\\)</p> <p>\\(\\begin{bmatrix} \\sum I_{x}^2 &amp; \\sum I_{x}I_{y} \\\\ \\sum I_{x}I_{y} &amp; \\sum I_{y}^2 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} \\sum I_{x}I_{t} \\\\ \\sum I_{y}I_{t} \\end{bmatrix}\\)</p> <p>Structural Tensor representation:</p> <p>\\(\\begin{bmatrix} T_{xx} &amp; T_{xy} \\\\ T_{xy} &amp; T_{yy} \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} T_{xt} \\\\ T_{yt} \\end{bmatrix}\\)</p> <p>$u = \\frac{T_{yt}T_{xy} - T_{xt}T_{yy}}{T_{xx}T_{yy} - T_{xy}^2} \\text{ and } v = \\frac{T_{xt}T_{xy} - T_{yt}T_{xx}}{T_{xx}T_{yy} - T_{xy}^2} $</p>"},{"location":"cv/opticalFlow/#issues","title":"Issues","text":"<ul> <li>Brightness constancy is not satisfied (Correlation based method could be used)</li> <li>A point may not move like its neighbors (Regularization based methods)</li> <li>The motion may not be small (Taylor does not hold!) (Multi-scale estimation could be used)</li> </ul>"},{"location":"cv/opticalFlow/#horn-schunck","title":"Horn &amp; Schunck","text":"<p>Global method with smoothness constraint to solve aperture problem</p> <p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 + \\alpha^2(|\\nabla u|^2 + |\\nabla v|^2) dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\((I_xu+ I_yv+ I_t)I_x - \\alpha^2(|\\nabla u|)= (I_xu+ I_yv+ I_t)I_y + \\alpha^2(|\\nabla u|) = 0\\)</p>"},{"location":"cv/opticalFlow/#flownet","title":"FlowNet","text":"<p>End-to-end frame work to for optical flow prediction.</p>"},{"location":"cv/opticalFlow/#simplenet","title":"SimpleNet","text":"<p>Both input images together and feed them through a rather generic network, allowing the network to decide itself how to process the image pair to extract the motion information.</p>"},{"location":"cv/opticalFlow/#flownetcorr","title":"FlowNetCorr","text":"<p>First produce meaningful representations of the two images separately and then combine them on a higher level. <code>correlation layer</code> performs multiplicative patch comparisons between two feature maps.</p> <p>Correlation of 2 patches of size \\(K \\times K\\) is given by :</p> <p>\\(c(\\mathbf{x}_1, \\mathbf{x}_2) = \\sum_{\\mathbf{o} \\in [-k,k] \\times [-k,k]} \\langle \\mathbf{f}_1(\\mathbf{x}_1 + \\mathbf{o}), \\mathbf{f}_2(\\mathbf{x}_2 + \\mathbf{o}) \\rangle\\)</p> <p>where \\(K = 2k+1\\). Above equation is similar to convolution but it is convolution of one data with another instead of filter.</p>"},{"location":"cv/opticalFlow/#flownetrefine","title":"FlowNetRefine","text":"<p>The main ingredient are \u2018upconvolutional\u2019 layers, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. To perform the refinement, we apply the \u2018upconvolution\u2019 to feature maps, and concatenate it with corresponding feature maps from the \u2019contractive\u2019 part of the network and an upsampled coarser flow prediction (if available). </p> <p>This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps.</p>"},{"location":"cv/opticalFlow/#application","title":"Application","text":"<ol> <li>Motion based segmentation</li> <li>SfM</li> <li>Alignment (e.g., UAV analysis)</li> <li>Video Compression</li> <li>Object Tracking</li> <li>Deformation Analysis</li> </ol>"},{"location":"cv/poseEstimation/","title":"Pose Estimation","text":""},{"location":"cv/poseEstimation/#pose-estimation","title":"Pose Estimation","text":"<ul> <li>Estimate a 2D pose (x,y) coordinates for each joint from a RGB image.</li> <li>17 joints</li> <li>Challanges<ul> <li>occlusions</li> <li>clothing</li> <li>extreme poses</li> <li>viewpoint changes etc.</li> </ul> </li> </ul>"},{"location":"cv/poseEstimation/#direct-regression","title":"Direct Regression","text":"DeepPose"},{"location":"cv/poseEstimation/#heatmap-predicion","title":"HeatMap Predicion","text":"<ul> <li>Instead of prediction by regression, for each joint one predicts a full image with a heatmap of the joint location</li> <li>Powerful representation, easier to predict a confidence per location, rather than regress a value for the position</li> <li>Ground truth (GT) heatmap is constructed by placing a 2D Gaussian around the joint position (e.g. variance 1.5 pixels)</li> <li>Loss: MSE between predicted and GT heatmap</li> </ul>  Newell predicted heatmap  <p>Bringing the structure of the problem - Body parts are linked to each other - Body symmetries - Joint limits, e.g., elbow cannot bend backwards - Physical connectivity: elbow connected to wrist</p> <p>Using graphical models also allows us to find the pose of several targets</p>  DeepCut  <p>Alternatively one ca do two stage process</p> <ol> <li>Object Detection</li> <li>Pose Estimation</li> </ol>"},{"location":"cv/segmentation/","title":"Image Segmentatio","text":""},{"location":"cv/segmentation/#semantic-segmentation","title":"Semantic Segmentation","text":""},{"location":"cv/segmentation/#instace-based-segmentation","title":"Instace-Based Segmentation","text":""},{"location":"cv/segmentation/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Region-based<ul> <li>e.g., IoU or Jaccard index, F-measure  or Dice\u2019s coefficient,  weighted F-measure,</li> </ul> </li> <li>Boundary-based <ul> <li>CM, boundary F-measure, boundary IoU, boundary displacement error (BDE), Hausdorff distances, </li> </ul> </li> <li>Structure-based<ul> <li>S-measure, E-measure, </li> </ul> </li> <li>Confidence-based <ul> <li>MAE </li> </ul> </li> </ul>"},{"location":"cv/segmentation/#dichotomous-image-segmentation","title":"Dichotomous Image Segmentation","text":""},{"location":"cv/segmentation/#dis","title":"DIS","text":""},{"location":"cv/segmentation/#birefnet","title":"BiRefNet","text":""},{"location":"cv/segmentation/#methods","title":"Methods","text":""},{"location":"cv/segmentation/#pdfnet","title":"PDFNet","text":""},{"location":"cv/segmentation/#ben","title":"BEN","text":""},{"location":"cv/segmentation/BiRefNet/","title":"BiRefNet","text":""},{"location":"cv/segmentation/BiRefNet/#birefnet","title":"BiRefNet","text":"<p>Bilateral Reference for High-Resolution Dichotomous Image Segmentation</p> <ul> <li>Swin-Transformer Based</li> <li>inward reference(InRef) and an outward reference (OutRef)</li> </ul> <p>Two essential Module:</p> <ol> <li>Localization module (LM) : <ul> <li>object localization using global semantic information</li> <li>extract hierarchical features from vision transformer backbone, which are combined and squeezed to obtain corase predictions in low resolution in deep layers.</li> </ul> </li> <li>Reconstruction module (RM) : <ul> <li>hierarchical patches of images provide the source reference, and gradient maps serve as the target reference.</li> <li>the inward and outward references as bilateral references (BiRef), in which the source image and the gradient map are fed into the decoder at different stages.</li> </ul> </li> </ol>  BiRefNet Comparison"},{"location":"cv/segmentation/BiRefNet/#localization-module","title":"Localization Module","text":"BiRefNet Architecture  <ul> <li>Transformer Encoder extract the fearures at different stages i.e., \\(F_1^e, F_2^e, F_3^e\\) with resolution at 4,8,16,32. </li> <li>The features of the first four \\(\\{F_i^e\\}_{i=1}^3\\) are transferred to the corresponding decoder stages with lateral connections (1\u00d71 convolution layers).</li> <li>These features are stacked and concatenated in the last encoder block to generate \\(F^e\\) then fed into a classification module.</li> </ul> <p>To enlarge the receptive fields to cover features of large objects and focus on local features for high precision simultaneously Atrous Spatial Pyramid Pooling (ASPP)  is used for multi-context fusion.</p>"},{"location":"cv/segmentation/BiRefNet/#reconstruction-module","title":"Reconstruction Module","text":"BiRef Blocks  <ul> <li> <p>Small receptive field (RFs) lead to inadequate context information to locate the right target on a large background, whereas large RFs often result in insufficient feature extraction in detailed areas.</p> </li> <li> <p>To achieve balance, using reconstruction block (RB) in each BiRef block as a replacement for the vanilla residual blocks.</p> </li> <li> <p>In RB, we employ deformable convolutions with hierarchical receptive fields (i.e., 1\u00d71, 3\u00d73, 7\u00d77) and an adaptive average pooling layer to extract features with RFs of various scales.</p> </li> <li> <p>These features extracted by different RFs are then concatenated as \\(F_i^{\\theta}\\), followed by a 1\u00d71 convolution layer and a batch normalization layer to generate the output feature of RM \\(F_i^{d'}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#bilateral-reference","title":"Bilateral Reference","text":"<ul> <li> <p>inward reference(InRef) and an outward reference (OutRef)</p> </li> <li> <p>In InRef, images \\(I\\) with original high resolution are cropped to patches \\(\\{P_{k=1}^N\\}\\) of consistent size with the output features of the corresponding decoder stage.</p> </li> <li> <p>These patches are stacked with the original feature \\(F_i^{d+}\\) to be fed into the RM.</p> </li> <li> <p>In OutRef, we use gradient labels to draw more attention to areas of richer gradient information which is essential for the segmentation of fine structures.</p> </li> <li> <p>First, we extract the gradient maps of the input images as \\(G_i^{gt}\\). Meanwhile, \\(F_i^{\\theta}\\) is used to generate the feature \\(F_i^G\\) to produce the predicted gradient maps \\(\\hat{G}^i\\)</p> </li> <li> <p>It passes through a conv and a sigmoid layer and is used to generate the gradient referring attention \\(A_i^G\\), which is then multiplied by \\(F_i^{d'}\\) to generate output of the BiRef block as \\(F_{i\u22121}^{d}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#loss","title":"Loss","text":"<p>\\(L = L_{pixel} + L_{region} + L_{boundary} + L_{semantic} \\\\ = \\lambda_1 L_{BCE} + \\lambda_2 L_{IoU} + \\lambda_3 L_{SSIM} + \\lambda_4 L_{CE}\\)</p> <p>\\(L_{BCE} = -\\sum_{(i,j)} [G(i,j) \\log(M(i,j)) + (1-G(i,j)) \\log(1-M(i,j))]\\)</p> <p>\\(L_{IoU} = 1 - \\frac{\\sum_{r=1}^H \\sum_{c=1}^W M(i,j)G(i,j)}{\\sum_{r=1}^H \\sum_{c=1}^W [M(i,j)+G(i,j)-M(i,j)G(i,j)]}\\)</p> <p>\\(L_{SSIM} = 1 - \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\\)</p> <p>\\(L_{CE} = -\\sum_{c=1}^N y_{o,c}\\log(p_{o,c})\\)</p>"},{"location":"cv/segmentation/DIS/","title":"DIS","text":""},{"location":"cv/segmentation/DIS/#dis","title":"DIS","text":"<p>Dichotomous Image Segmentation (DIS) proposed IS-Net. IS-Net as 3 components</p> <ol> <li>ground truth (GT) encoder,</li> <li>image segmentation component (\\(U^2\\)-Net with an input convolution layer before its first encoder stage.)</li> <li>intermediate supervision strategy</li> </ol>"},{"location":"cv/segmentation/DIS/#1st-stage","title":"1st Stage","text":"<p>Self-supervised training of the GT-encoder</p> <p>\\(L_{gt} = \\sum_{d=1}^{D} \\lambda_{d}^{gt} BCE(F_{gt}(\\theta_{gt}, G)_d, G)\\)</p> <p>GT encoder will be frozen.</p>"},{"location":"cv/segmentation/DIS/#2nd-stage","title":"2nd Stage","text":"<p>High dimensional intermediate features</p> <p>\\(f_{D}^{G} = F_{gt}^{-}(\\theta_{gt}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <ul> <li> <p>\\(F_{gt}^{-}\\) represents the \\(F_{gt}\\)  without the last convolution layers for generating the probability maps.</p> </li> <li> <p>\\(F_{gt}^{-}\\) is to supervise those corresponding features \\(f_{D}^{I}\\) from the segmentation model \\(F_{sg}\\)</p> </li> </ul> <p>High dimensional intermediate features from image segmentation component</p> <p>\\(f_{I}^{G} = F_{sg}^{-}(\\theta_{sg}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <p>Feature Consistency Loss (intermediate supervision)</p> <p>\\(L_{fs} = \\sum_{d=1}^{D} \\lambda_{d}^{fs} ||f_{d}^{I} - f_{d}^{G}||^2\\)</p> <p>\\(L_{sg} = \\sum_{d=1}^{D} \\lambda_{d}^{sg} BCE(F_{sg}(\\theta_{sg}, I), G)\\)</p> <p>Loss for \\(F_{sg}\\) is  \\(L = L_{fs} + L_{sg}\\)</p>  IS-Net"},{"location":"cv/segmentation/DIS/#results","title":"Results","text":"Result"},{"location":"cv/segmentation/DIS/#meric-human-correction-efforts-hce","title":"Meric : Human Correction Efforts (HCE)","text":""},{"location":"ml/","title":"Machine Learning","text":""},{"location":"ml/#machine-learning","title":"Machine Learning","text":"<ul> <li> Linear Regression</li> <li> Logistic Regression</li> <li> Naive Bayes</li> <li> Principal Component Analysis (PCA)</li> <li> Linear Discriminant Analysis (LDA)</li> <li> k-Nearest Neighbors (k-NN)</li> <li> k-means Clustering</li> <li> Support Vector Machine (SVM)</li> <li> Decision Tree</li> <li> Ensemble Method</li> <li> Perceptron</li> <li> Neural Network</li> <li> Convolutional Neural Network (CNN)</li> </ul>"},{"location":"ml/DTQn/","title":"Decision Tree","text":""},{"location":"ml/DTQn/#decision-tree","title":"Decision Tree","text":"<ul> <li> <p>explain random forest, bagging.</p> </li> <li> <p>what is random in random forest ?</p> </li> <li> <p>what is a weak learner ?</p> </li> <li> <p>what is a decision tree?</p> </li> <li> <p>How does split takes place in decision tree?</p> </li> <li> <p>how random forest is better than decision tree ?</p> </li> <li> <p>how to avoid overfitting ? Are decision tree in random forest overfitting ?</p> </li> <li> <p>how bias/variance helps random forest ?</p> </li> <li> <p>what is boosting, how does bias/variance work in it ?</p> </li> <li> <p>Gradient boosting tree, how the variance is low ?</p> </li> <li> <p>Given a single DT with accuracy of 70% what will be the accuracy of an ensemble model made with this tree(assume 3 trees in the ensemble model).</p> </li> <li> <p>Intuition behind increasing weights of examples in boosting.</p> </li> <li> <p>Percentage of unique samples in bagging.</p> </li> <li> <p>What is XGboost?</p> </li> <li> <p>Why boosting reduces bias?</p> </li> <li> <p>Why bagging reduces variance? Math and intuition</p> </li> <li> <p>How does boosting and random forest affect bias and variance.</p> </li> </ul>"},{"location":"ml/DecisionTree/","title":"Decision Tree","text":""},{"location":"ml/DecisionTree/#decision-tree","title":"Decision Tree","text":"<p>A supervised learning method used for: 1. Regression 2. Classification</p>"},{"location":"ml/DecisionTree/#terminology","title":"Terminology","text":"<ol> <li>Root Node: Starting point of the tree</li> <li>Decision Node: Internal node where a split occurs</li> <li>Leaf Node: Terminal node containing predictions</li> <li>Sub Tree: Part of the tree below a node</li> <li>Splitting: Process of dividing a node into two or more sub-nodes</li> </ol>"},{"location":"ml/DecisionTree/#core-concepts","title":"Core Concepts","text":"<ol> <li>Recursive Binary Splitting</li> <li>Greedy approach at each step</li> <li>Non-parametric method</li> <li>Can handle non-linear relationships</li> </ol>"},{"location":"ml/DecisionTree/#tree-construction","title":"Tree Construction","text":""},{"location":"ml/DecisionTree/#feature-selection-measures","title":"Feature Selection Measures","text":"<p>For Classification: 1. Gini Index: \\(\\sum_{k=1}^K p\u0302_k(1-p\u0302_k)\\)    - Measures node purity    - Range: [0,0.5] for binary    - 0 = pure node, 0.5 = equal distribution</p> <ol> <li>Entropy: -\\(\\sum_{k=1}^K p\u0302_k\\log(p\u0302_k)\\)</li> <li>Measures information gain</li> <li>Range: [0,log(k)]</li> <li> <p>0 = pure node</p> </li> <li> <p>Misclassification Error: 1 - max(p\u0302\u1d62)</p> </li> <li>Less used in practice</li> <li>Not sensitive enough for tree growth</li> </ol> <p>For Regression:</p> <p>For region \\(R_j\\), 1. RSS (Residual Sum of Squares) : \\(RSS = \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i -\\hat{y}_i)^2\\) 2. MSE (Mean Squared Error) : \\(MSE = \\frac{1}{N} \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i -\\hat{y}_i)^2\\) 3. MAE (Mean Absolute Error) : \\(MAE = \\frac{1}{N} \\sum_{j=1}^J \\sum_{i \\in R_j} |y_i -\\hat{y}_i|\\)</p>"},{"location":"ml/DecisionTree/#building-process","title":"Building Process","text":"<ul> <li>Uses top-down, greedy approach (binary splitting)</li> <li>At each step, makes locally optimal split</li> <li>Prediction: Mean of response variable in each leaf</li> </ul> <p>Splitting Process 1. For each feature:    - For numerical: Find best splitting threshold    - For categorical:       * Binary: Two groups      * Multi-class: Consider all possible groupings 2. Calculate impurity measure for each split 3. Select split that maximizes information gain:    \\(IG = I(parent) - \\sum_{j=1}^m \\frac{N_j}{N}I(j)\\)    where I = impurity measure</p> <p>Algorithm Steps 1. Consider all predictors and possible cut points 2. Calculate RSS for each potential split 3. Select split with minimum RSS 4. Repeat until stopping criteria met</p> <p>Stopping Criteria 1. Minimum samples at internal node 2. Minimum samples at leaf node 3. Maximum depth of tree 4. Maximum number of leaf nodes</p>"},{"location":"ml/DecisionTree/#pruning","title":"Pruning","text":"<p>Cost complexity function: \\(\\sum_{m=1}^{|T|} \\sum_{i:x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|\\)</p> <p>where \\(|T|\\) is tree size and \\(\\alpha\\) is complexity parameter</p> <p>Pruning Process 1. Grow maximum size tree 2. Prune back using cost complexity</p>"},{"location":"ml/DecisionTree/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"<p>Advantages - Interpretable and visualizable - Handles both numerical and categorical data - Minimal preprocessing needed - Captures non-linear relationships - Handles missing values well</p> <p>Disadvantages - High variance (unstable) - Prone to overfitting - Biased towards dominant classes - May create biased trees with imbalanced datasets</p>"},{"location":"ml/Ensemble/","title":"Ensemble","text":""},{"location":"ml/Ensemble/#bagging-bootstrap-aggregating","title":"Bagging (Bootstrap Aggregating)","text":"<p>Bootsrapping : Sample with replacement</p> <ul> <li>Creates multiple trees using bootstrap samples</li> <li>Uses all features</li> <li>No pruning (reduces bias)</li> <li>Reduces variance through averaging</li> <li>Problem: Creates correlated trees</li> </ul>"},{"location":"ml/Ensemble/#random-forest","title":"Random Forest","text":"<ul> <li>Extension of bagging</li> <li>Randomly selects subset of features at each split</li> <li>Feature subset size:<ul> <li>Classification: \\(\\sqrt{M}\\)</li> <li>Regression: \\(M/3\\)</li> <li>Can be reduced for correlated features</li> </ul> </li> </ul>"},{"location":"ml/Ensemble/#boosting","title":"Boosting","text":"<ul> <li>Sequential tree growth</li> <li>Each tree learns from previous errors</li> <li>Controls tree depth</li> <li> <p>Types:</p> <ol> <li> <p>Gradient Boosting</p> <ul> <li>Fits trees to residuals</li> <li>Slow learning procedure</li> <li>Uses gradient descent</li> </ul> </li> <li> <p>AdaBoost</p> <ul> <li>Adjusts observation weights</li> <li>Focus on misclassified instances</li> <li>Adaptive learning rate</li> </ul> </li> <li> <p>XGBoost</p> <ul> <li>Regularized gradient boosting</li> <li>Better control over overfitting</li> <li>Advanced features:</li> <li>L1 (Lasso) &amp; L2 (Ridge) regularization</li> <li>Handling missing values</li> <li>Tree pruning</li> </ul> </li> </ol> </li> </ul>"},{"location":"ml/KNN/","title":"k-NN","text":""},{"location":"ml/KNN/#k-nearest-neighbor-knn","title":"k-nearest neighbor (KNN)","text":"<p>It is non-parametric learning algorithm. It is mainly used for classification but also can be used for regression by averaging out the nearest value based on distance.</p>"},{"location":"ml/KNN/#process","title":"Process","text":"<ol> <li>Choose the number of k and a distance metric(Euclidean, Manhattan, Cosine etc.)</li> <li>Find the k-nearest neighbors of the data record that we want to classify</li> <li>Assign the class label by majority vote</li> </ol> <p>The right choice of k is crucial to finding a good balance between overfitting and underfitting.</p>"},{"location":"ml/KNN/#k","title":"k","text":"<p><code>Effect of k:</code> As k increases, variance decreases while bias increases. Conversely, as k decreases, variance increases while bias decreases.</p> <p><code>Choice of k:</code> Choosig based on validation error:</p>"},{"location":"ml/KNN/#key-points","title":"Key Points","text":"<ul> <li>It is a memory-based approach immediately adapts as we collect new training data. </li> <li>The computational complexity for classifying new examples grows linearly with the number of examples in the training dataset in the worst-case scenario.</li> <li>KNN is very susceptible to overfitting due to the curse of dimensionality (the closest neighbors as being too far away in a high-dimensional space to give a good estimate.). Regularization method cannot be applied here.</li> <li>All the features should be scaled as we will be taking distnace based on features.</li> <li>Optimization can be done through the dimensionality reduction by using method like PCA, LDA etc.</li> </ul>"},{"location":"ml/KNN/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/LinearRegression/","title":"Linear Regression","text":""},{"location":"ml/LinearRegression/#linear-regression","title":"Linear Regression","text":"<p>Linear Regression would be appropriate since we are predicting a continuous value.</p> <p>Linear Regression works when these 4 assumtion being followed:</p> <ol> <li> <p>Linearity: this means that the relationship must be linear between the independent variables and dependent variables.  \\(y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) or \\(y= \\beta_0 + \\beta_1 sin(x) + \\beta_2 cosx(x)\\)</p> </li> <li> <p>Homoscedasticity: Constant variance of residuals. </p> </li> <li> <p>Independence: independent variables (observations) are not highly correlated.</p> </li> <li> <p>Normality: Residuals are normally distributed for any fixed value of our observations </p> </li> </ol> <p>Note</p> <ul> <li> <p>Find the collinearity by using Variance Inflation Factors (VIF). VIF &gt; 5 variable are dependent.</p> </li> <li> <p>Solve collinearity by either removing one of the features or linearly combine both features. </p> </li> </ul>"},{"location":"ml/LinearRegression/#metrics","title":"Metrics","text":"<ul> <li> <p>Root Mean Square Error (RMSE) : Calculates the average of the squared difference between the predicted and actual values. Thus, larger errors (outliers or poor prediction) are flagged  more than when using MAE due to squaring errors. \\(RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2}{N}}\\)</p> </li> <li> <p>Mean Absolute Error (MAE) : Calculates the average of the absolute difference between the predicted and actual values. As a result, it does not punish large errors as much as RMSE. \\(MAE = \\frac{\\sum_{i=1}^{N}|y_i - \\hat{y_i}|}{N}\\)</p> </li> </ul>"},{"location":"ml/LinearRegression/#methods","title":"Methods","text":"<ol> <li> <p>Closed form solution</p> <ul> <li>\\(XW=y\\)</li> <li>\\(X^TXW=X^Ty\\)</li> <li>\\(w = (X^TX)^{-1}X^Ty\\)</li> <li>Useful, when optimal solution is needed. Issue when inverse does notexist and computationally expensive when data is too large.</li> </ul> </li> <li> <p>Optimization algorithm, typically Gradient Descent (GD) or Stochastic Gradient Descent (SGD).</p> <ul> <li>\\(\\text{L} = \\frac{1}{2} ||\\hat{y} - y||^2\\) where \\(\\hat{y} = X*W + b\\) </li> <li>\\(\\frac{\\partial L}{\\partial W} = X*(\\hat{y}- y)\\)</li> <li>\\(\\frac{\\partial L}{\\partial W} = \\hat{y}- y\\)</li> </ul> </li> </ol>"},{"location":"ml/LinearRegression/#feature-importance","title":"Feature Importance","text":"<p>If the features are normalized then the coefficients are an indication of feature importance, i.e. features with higher coefficients are more useful for  prediction.</p>"},{"location":"ml/LinearRegression/#prediction","title":"Prediction","text":"<p>\\(y = \\sum_{i}w_ix_i + b\\)</p>"},{"location":"ml/LinearRegression/#question","title":"Question","text":"Why linear regression is called linear? <ul> <li>The relationship between the independent variables (X) and dependent variable (Y) is expressed as a linear combination of parameters (coefficients).</li> <li>The coefficients (\u03b2) appear in the equation in a linear way: Y = \u03b2\u2080 + \u03b2\u2081X\u2081 + \u03b2\u2082X\u2082 + ... + \u03b2\u2099X\u2099</li> <li>These parameters are not raised to powers or modified by other functions.</li> <li>The variables themselves (X) can be non-linear (like X\u00b2, log(X), etc.)</li> <li>For example: Y = \u03b2\u2080 + \u03b2\u2081X\u00b2 is still a linear regression model because the coefficient \u03b2\u2081 is linear</li> </ul> Explain the concept of correlation between features, its implications, and potential problems in machine learning models, particularly in regression. <ul> <li>Measures linear relationship between variables</li> <li>Range: -1 to +1</li> <li>Shows how variables move together</li> <li>Problems with Correlated Features:<ul> <li>Multicollinearity in regression</li> <li>Unstable coefficients</li> <li>Reduced model interpretability</li> </ul> </li> </ul>"},{"location":"ml/LinearRegression/#code","title":"Code","text":"<p>Numpy</p> Closed FormGradient Form <pre><code>class LinearRegressionClosedForm:\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        bias = np.ones((n_samples, 1))\n        X_new = np.column_stack((X, bias))\n        W = np.linalg.inv(X_new.T @ X_new) @ X_new.T @ y\n        self.weights = W[:-1]\n        self.bias = W[-1]\n\n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n</code></pre> <pre><code>class LinearRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # gradient descent\n        for _ in range(self.n_iters):\n            y_predicted = np.dot(X, self.weights) + self.bias\n            # compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n\n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n</code></pre> <p>Pytorch</p> Parameter BasedLinear Layer Based <pre><code>class LinearRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegression, self).__init__()\n        self.weights = nn.Parameter(torch.randn(input_dim, output_dim, requires_grad=True))\n        self.bias = nn.Parameter(torch.randn(1, output_dim, requires_grad=True))\n\n    def forward(self, x):\n        return  torch.matmul(x, self.weights) + self.bias\n</code></pre> <pre><code>class LinearRegressionV2(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionV2, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return  self.linear(x)\n</code></pre>"},{"location":"ml/LogisticRegression/","title":"Logistic Regression","text":""},{"location":"ml/LogisticRegression/#logistic-regression","title":"Logistic Regression","text":"<p>It is used for te classification problem. It uses linear regressing equaition to predict the class probabilities.</p> <p>Equation:</p> <p>\\(y = wx+b\\)</p> <p>This \\(y\\) is feed to the sigmoid function to get the output between 0 and 1 as probailities. So,</p> <p>\\(y = \\frac{1}{1+ e^{-(wx+b)}}\\)</p> <p>Logistic regression doesn't require: - Normality of residuals - Homoscedasticity</p> <p>Logistic regression specifically requires: - Binary/categorical outcome - Linear relationship with log odds (not the outcome itself)</p>"},{"location":"ml/LogisticRegression/#effect-of-outlier","title":"Effect of Outlier","text":"<p>Since here we focus on finding the decision boundry that linearly seperate the classes. So we mostly focus on the points which are closer to the boundry. Therefore, outlier will have very less effect here.</p>"},{"location":"ml/LogisticRegression/#logistic-regression-as-maximum-likelihood-estimationmle","title":"Logistic Regression as Maximum Likelihood Estimation(MLE)","text":"<p>Assuming the Bernoulli distribution (i.e., binary classification). Let, \\(y \\in \\{0,1\\}\\) if \\(p\\) is the probability of class as 1. Then according to MLE we need to maximize \\(p^y\\) if class is 1 and \\((1-p)^{(1-y)}\\) if class is 0.</p> <p>\\(L = \\Pi_{i=1}^{N} p_i^{y_i} (1-p_i)^{(1-y_i)}\\)</p> <p>Multipying such large number may result in the overflow. So take <code>log</code> on both side.</p> <p>\\(L = \\sum_{i=1}^{N} (y_i \\ln p_i + (1 - y_i) \\ln (1 - p_i))\\)</p> <p>\\(Loss = -Likelihood\\)</p> <p>This loss penelizes much more than MSE when prediciton is wrong.</p>"},{"location":"ml/LogisticRegression/#optimization","title":"Optimization","text":"<ul> <li>\\(z^{(i)} = wx^{(i)} + b\\)<ul> <li>\\(\\frac{\\partial z^{(i)}}{\\partial w} = x^{(i)}\\) </li> </ul> </li> <li>\\(\\hat{y}^{(i)} = \u03c3(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}\\)<ul> <li>\\(\\frac{\\partial \\hat{y}^{(i)}}{\\partial z^{(i)}} = \\hat{y}^{(i)}(1-\\hat{y}^{(i)})\\)</li> </ul> </li> <li>\\(J(w,b) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]\\)<ul> <li>\\(\\frac{\\partial J}{\\partial \\hat{y}^{(i)}} = -\\frac{1}{m} [\\frac{y^{(i)}}{\\hat{y}^{(i)}} - \\frac{1-y^{(i)}}{1-\\hat{y}^{(i)}}] = -\\frac{1}{m} [\\frac{y^{(i)} - \\hat{y}^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}]\\)</li> </ul> </li> <li> <p>\\(\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial \\hat{y}^{(i)}} \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial w}\\)</p> </li> <li> <p>\\(\\frac{\\partial J}{\\partial w} =  -\\frac{1}{m} [\\frac{y^{(i)} - \\hat{y}^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}] \\cdot \\hat{y}^{(i)}(1-\\hat{y}^{(i)}) \\cdot x^{(i)}\\)</p> <ul> <li>\\(\\frac{\\partial J}{\\partial w} =  -\\frac{1}{m} (y^{(i)} - \\hat{y}^{(i)}) \\cdot x^{(i)}\\)</li> </ul> </li> <li>\\(\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})x^{(i)}\\)</li> </ul> <p>Similarly,</p> <ul> <li>\\(\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\\)</li> </ul>"},{"location":"ml/LogisticRegression/#prediction","title":"Prediction","text":"<p>Here, \\(pred = \\frac{1}{1+ e^{-(wx+b)}}\\), if \\(pred &gt; \\tau\\) then class 1 else class 0.</p> <p>\\(\\tau\\) is decided according to problem statement.</p>"},{"location":"ml/LogisticRegression/#multi-class-n","title":"Multi-Class (N)","text":"<ol> <li> <p>One-vs-all: We need to have N models </p> <p>\\(pred = \\text{argmax}_{i} f_{i}(x)\\)</p> </li> <li> <p>One-vs-one: We need to have \\(\\binom{N}{2}\\) models, where each model is trained to distinguish between a pair of classes. For N classes, this results in \\(\\frac{N(N-1)}{2}\\) binary classifiers. The prediction is made by majority voting across all pairwise comparisons:</p> <p>\\(pred = argmax_{i} \\sum_{j \\neq i} \\mathbb{I}(f_{ij}(x) = i)\\)</p> <p>where \\(f_{ij}(x)\\) is the binary classifier for classes i and j, and \\(\\mathbb{I}\\) is the indicator function.</p> </li> <li> <p>Mathematical: Use softmax instead of sigmoid and use cross-entropy loss.</p> </li> </ol>"},{"location":"ml/LogisticRegression/#question","title":"Question","text":"Why logistic regression is a classifier and not regression? <ul> <li>Logistic regression outputs probabilities between 0 and 1</li> <li>These probabilities are then converted to binary classes (0 or 1) using a threshold \\(\\tau\\)</li> </ul> Why do we use cross-entropy instead of mean square errors in logistic regression? <ul> <li>Cross-entropy gives larger gradients for wrong predictions, leading to faster and better learning, especially when predictions are far from actual values.</li> </ul> How to extend the sigmoid function for multi-class classification? <ul> <li>By using softmax</li> </ul> Suppose you have a logistic regression model as a black box. How can you determine the weights? <ul> <li>We can determine the weights using n+1 strategic queries (where n is the number of features).<ul> <li>Finding Bias Term (b)<ul> <li>First, input a zero vector (all features set to 0)</li> <li>The output will give us the bias term as there's no contribution from any weights</li> </ul> </li> <li>Finding Individual Weights<ul> <li>Use the columns of an identity matrix as inputs</li> <li>When we input [1,0,0,...], the output will reflect bias + weight1 and so on.</li> </ul> </li> </ul> </li> </ul> BCE loss is convex function? <ul> <li> <p>For Linear Model:</p> <ul> <li>BCE with respect to w and b is convex<ul> <li>Sigmoid is monotonic</li> <li>Log loss is convex</li> <li>Composition maintains convexity here</li> </ul> </li> </ul> </li> <li> <p>For Neural Networks:</p> <ul> <li>BCE is NOT convex<ul> <li>Non-linear activation functions</li> <li>Multiple layers</li> <li>Complex compositions</li> </ul> </li> </ul> </li> </ul>"},{"location":"ml/LogisticRegression/#code","title":"Code","text":"NumpyPyTorch <pre><code>class LogisticRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # gradient descent\n        for _ in range(self.n_iters):\n            # approximate y with linear combination of weights and x, plus bias\n            linear_model = np.dot(X, self.weights) + self.bias\n            # apply sigmoid function\n            y_predicted = self._sigmoid(linear_model)\n\n            # compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights) + self.bias\n        y_predicted = self._sigmoid(linear_model)\n        y_predicted_cls = [1 if i &gt; 0.5 else 0 for i in y_predicted]\n        return np.array(y_predicted_cls)\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n</code></pre> <pre><code>class LogisticRegression(nn.Module):\n    def __init__(self, input_features):\n        super(LogisticRegression, self).__init__()\n        self.layer1 = nn.Linear(input_features, 8)\n        self.layer2 = nn.Linear(8, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.bn = nn.BatchNorm1d(8)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = torch.sigmoid(self.layer2(x))\n        return x\n</code></pre>"},{"location":"ml/MATH/","title":"Co-Variance:","text":""},{"location":"ml/MATH/#co-variance","title":"Co-Variance:","text":"<p>Covariance is a measures the extent two features either increase or decrease with each other (range ( \u2212 \u221e , \u221e ) ). A covariance score of 0 indicates that both features are not related. If the covariance score is positive it means that both features increase in the same direction and a negative score indicates an inverse relationship between the two features. </p> <p>A Covariance matrix is used in PCA, Gaussian mixture models (GMMs) and Mahalanobis Distance. </p>"},{"location":"ml/MATH/#correlation","title":"Correlation","text":"<p>Correlation lets us know the strength and direction of the two features (range ( \u2212 1 , 1 ) ). If we say two features are correlated, then we can say that a change in one feature creates an impact/change in another variable. A positive correlation indicates that as one feature increases the other will increase, a correlation score of 0 indicates no relationship between variables and a negative correlation indicates that as one feature increases the other will  decrease. </p> <p>Correlation is just in large amounts of data is to find patterns, e.g. correlated features within a dataset. </p>"},{"location":"ml/MATH/#bayes-equation","title":"Bayes Equation","text":""},{"location":"ml/NaiveBayes/","title":"Naive Bayes","text":""},{"location":"ml/NaiveBayes/#naive-bayes","title":"Naive Bayes","text":"<p>Naive Bayes classifiers have a general assumption that the effect of an attribute value on a given class in independent of the values of the other attributes. This assumption is called class-conditional independence.</p> <p>\\(P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\\)</p> <p>\\(\\text{posterior} = \\frac{\\text{likelihood prob} * \\text{prior}}{\\text{marginal}}\\)</p> <p>In the context of Naive Bayes classification, we often work with log probabilities to avoid numerical underflow. The log posterior probability is calculated as:</p> <p>\\(\\log P(A|B) = \\log P(B|A) + \\log P(A) - \\log P(B)\\)</p> <p>The denominator \\(P(B)\\) is omitted since it's constant across classes and doesn't affect the \\(\\text{argmax}\\)</p> <p>\\(\\log P(A|B) = \\log P(B|A) + \\log P(A)\\)</p> <p>\\(posterior = posterior + prior\\)</p> <p>This matches the implementation shown in the code where: - \\(\\log P(A)\\) is the prior probability - \\(\\log P(B|A)\\) is calculated as the sum of log probabilities from the PDF</p>"},{"location":"ml/NaiveBayes/#additional-details","title":"Additional Details","text":"<p>Prior Probability[P(A)]: The prior probability represents our initial belief about the probability of each class before seeing any evidence. It's calculated by dividing the number of instances of a particular class by the total number of instances in the training dataset.</p> <p>Likelihood Probability[P(B|A)]: This represents the probability of observing the features given a particular class. It measures how likely we are to see these features if the class is true.</p> <p>Marginal Probability[P(B)]: This is the probability of observing the features regardless of the class. It acts as a normalizing constant to ensure our probabilities sum to 1.</p> <p>Posterior Probability[P(A|B)]: This is our final probability of a class given the observed features. It's calculated by multiplying the likelihood by the prior and dividing by the marginal probability.</p> <p>\\(P(x|c) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}} e^{-\\frac{(x-\\mu_c)^2}{2\\sigma_c^2}}\\)</p> <p>we assume that the data follows Gaussian distribution, due  to which the probability of an item, given a class label c, can be defined as</p>"},{"location":"ml/NaiveBayes/#advantages","title":"Advantages","text":"<ol> <li>Simple and fast to train and predict</li> <li>Works well with small datasets</li> <li>Can handle both continuous and discrete features</li> <li>Less prone to overfitting</li> </ol>"},{"location":"ml/NaiveBayes/#disadvantages","title":"Disadvantages","text":"<ol> <li>Assumes features are independent (naive assumption)</li> <li>May not perform well when features are correlated</li> <li>Requires features to be normally distributed for optimal performance</li> <li>Sensitive to feature scaling</li> </ol>"},{"location":"ml/NaiveBayes/#laplacian-smoothing","title":"Laplacian Smoothing","text":"<p>Laplacian smoothing (also known as additive smoothing) is a technique used to handle zero probability problems in Naive Bayes classification. It adds a small constant \u03b1 to the count of each feature-class combination to prevent zero probabilities.</p> <p>For a feature value x and class c, the smoothed probability is calculated as:</p> <p>\\(P(x|c) = \\frac{count(x,c) + \\alpha}{count(c) + \\alpha|V|}\\)</p> <p>Where</p> <ul> <li>count(x,c) is the number of times feature x appears with class c</li> <li>count(c) is the number of instances of class c</li> <li>\u03b1 is the smoothing parameter (typically 1)</li> <li>|V| is the size of the vocabulary (number of unique feature values)</li> </ul>"},{"location":"ml/NaiveBayes/#question","title":"Question","text":"Why Naive Bayes is called Naive? <ul> <li>Simplified assumption that all features in a dataset are independent of each other, even though in real-world scenarios this is rarely true.</li> <li>\\(P(y|x\u2081,x\u2082,...,x\u2099) = \\frac{P(x\u2081|y) \u00d7 P(x\u2082|y) \u00d7 ... \u00d7 P(x\u2099|y) \u00d7 P(y)}{P(X)} = \\frac{P(y) \\prod_{i=1}^{n} P(x_i|y)}{P(X)}\\)</li> </ul>"},{"location":"ml/NaiveBayes/#code","title":"Code","text":"NumpyPyTorch <pre><code>class NaiveBayes:\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self._classes = np.unique(y)\n        n_classes = len(self._classes)\n\n        # calculate mean, var, and prior for each class\n        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._priors = np.zeros(n_classes, dtype=np.float64)\n\n        for idx, c in enumerate(self._classes):\n            X_c = X[y == c]\n            self._mean[idx, :] = X_c.mean(axis=0)\n            self._var[idx, :] = X_c.var(axis=0)\n            self._priors[idx] = X_c.shape[0] / float(n_samples)\n\n    def predict(self, X):\n        y_pred = [self._predict(x) for x in X]\n        return np.array(y_pred)\n\n    def _predict(self, x):\n        posteriors = []\n\n        # calculate posterior probability for each class\n        for idx, c in enumerate(self._classes):\n            prior = np.log(self._priors[idx])\n            posterior = np.sum(np.log(self._pdf(idx, x)))\n            posterior = prior + posterior\n            posteriors.append(posterior)\n\n        # return class with highest posterior probability\n        return self._classes[np.argmax(posteriors)]\n\n    def _pdf(self, class_idx, x):\n        mean = self._mean[class_idx]\n        var = self._var[class_idx]\n        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n        denominator = np.sqrt(2 * np.pi * var)\n        return numerator / denominator\n</code></pre> <pre><code>\n</code></pre>"},{"location":"ml/SVM/","title":"SVM","text":""},{"location":"ml/SVM/#svm","title":"SVM","text":"<p>Support Vector Machine</p> <ul> <li>Used for both classification and regression tasks.</li> <li>Core idea: Find optimal hyperplane that maximizes margin between classes.</li> </ul>"},{"location":"ml/SVM/#geometric-interpretation","title":"Geometric Interpretation","text":"<ul> <li>There are many hyperplanes that separate positive and negative points.</li> <li>We want to find the margin-maximizing hyperplane.</li> <li>Margin is the perpendicular distance between two hyperplanes \\(H^{+}\\) and \\(H^{-}\\).</li> <li>As margin increases, generalization ability increases.</li> <li>Points through which \\(H^{+}\\) or \\(H^{-}\\) pass are called support vectors.</li> </ul>"},{"location":"ml/SVM/#alternate-geometric-interpretation","title":"Alternate Geometric Interpretation","text":"<ul> <li>Find the convex hull for positive and negative points.</li> <li>Find the shortest line connecting these hulls.</li> <li>The plane bisecting this line perpendicularly gives the margin maximizer.</li> </ul>"},{"location":"ml/SVM/#derivation","title":"Derivation","text":"<ul> <li>\\(H\\): Margin Maximizing hyperplane</li> <li>\\(H\\): \\(w^T x + b = 0\\) </li> <li>\\(H^{+}\\): \\(w^T x + b = 1\\) (positive class boundary)</li> <li>\\(H^{-}\\): \\(w^T x + b = -1\\) (negative class boundary)</li> <li>Margin: \\(d = \\frac{2}{||w||}\\) (derived from geometric distance formula)</li> </ul> <p>Problem Statement</p> <ul> <li>For linearly separable data:</li> <li>Find \\(w^{\\star}\\) and \\(b^{\\star}\\) such that \\(\\frac{2}{||w||}\\) is maximized</li> <li>Subject to: \\(y_i(w^T x_i + b) \\ge 1 \\text{  } \\forall i\\) </li> <li> <p>\\(w^{\\star}, b^{\\star} = \\text{arg max}_{w,b} \\frac{2}{||w||} = \\text{arg min}_{w,b} \\frac{||w||^2}{2}\\)</p> </li> <li> <p>For non-linearly separable data:</p> </li> <li>Introduce slack variables \\(\\xi_i \\ge 0\\)</li> <li>\\(\\xi_i = 0\\) for correctly classified points</li> <li>\\(\\xi_i &gt; 0\\) measures violation of margin</li> <li>Objective: \\(w^{\\star}, b^{\\star} = \\text{arg min}_{w,b} \\frac{||w||^2}{2} + C \\sum_i^N \\xi_i\\)</li> <li>\\(C\\): regularization parameter (higher \\(C\\) \u2192 stricter margin)</li> </ul>"},{"location":"ml/SVM/#loss-minimization-hinge-loss","title":"Loss minimization: Hinge Loss","text":"<ul> <li>\\(L_{hinge}(y_i, f(x_i))=\\max(0, 1-y_i f(x_i))\\) where \\(f(x_i) = w^T x_i + b\\)</li> <li>When \\(y_i f(x_i) \\ge 1\\): Loss \\(= 0\\) (correct classification with margin)</li> <li>When \\(y_i f(x_i) &lt; 1\\): Loss \\(&gt; 0\\) (violation of margin)</li> <li>Complete objective: \\(\\mathcal{L} = \\min_{w,b} \\frac{1}{n}\\sum_i^N \\max(0, 1 - y_i(w^T x_i +b)) + \\lambda ||w||^2\\)</li> </ul>"},{"location":"ml/SVM/#optimization","title":"Optimization","text":"<ul> <li>Gradient descent can be used to minimize hinge loss</li> <li>Challenging due to non-differentiability at \\(y_i(w^T x_i + b) = 1\\)</li> <li>Sub-gradients are used at non-differentiable points</li> </ul> <p>Gradient Computation:</p> <p>\\(\\frac{\\partial J}{\\partial w} = \\lambda w - \\frac{1}{n}\\sum_{i=1}^n y_i x_i \\cdot I(y_i(w^T x_i + b) &lt; 1)\\)</p> <p>\\(\\frac{\\partial J}{\\partial b} = -\\frac{1}{n}\\sum_{i=1}^n y_i \\cdot I(y_i(w^T x_i + b) &lt; 1)\\)</p> <p>where \\(I()\\) is the indicator function.</p> <p>Gradient Updates:</p> <ol> <li>For points outside margin (\\(y_i(w^T x_i + b) \\geq 1\\)):</li> <li>Only regularization affects update</li> <li>\\(w = w - \\eta \\lambda w\\)</li> <li> <p>\\(b\\) remains unchanged</p> </li> <li> <p>For margin violations (\\(y_i(w^T x_i + b) &lt; 1\\)):</p> </li> <li>Both loss and regularization affect update</li> <li>\\(w = w - \\eta(\\lambda w - y_i x_i)\\)</li> <li>\\(b = b + \\eta y_i\\)</li> </ol> <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"ml/SVM/#dual-form-of-svm","title":"Dual form of SVM","text":"<p>Primal \\(\\min_{w,b,\\xi} \\frac{||w||^2}{2} + C \\sum_i^N \\xi_i\\) subject to: \\(y_i(w^T x_i + b) \\ge 1 - \\xi_i\\) and \\(\\xi_i \\ge 0\\) \\(\\forall i\\)</p> <p>Dual \\(\\max_{\\alpha} \\sum_i^N \\alpha_i - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\) subject to: \\(\\sum_i^N \\alpha_i y_i = 0\\) and \\(0 \\le \\alpha_i \\le C\\) \\(\\forall i\\)</p>"},{"location":"ml/SVM/#kernel-svm","title":"Kernel SVM","text":"<ul> <li>For non-linearly separable data in input space, map to higher dimensional feature space</li> <li>Explicit mapping \u03c6(x) is computationally expensive</li> <li>Kernel trick: K(x,y) = \u03c6(x)\u1d40\u03c6(y)</li> <li>Replace dot products with kernel function in dual form</li> </ul> <p>Dual form with Kernel \\(\\max_{\\alpha} \\sum_i^N \\alpha_i - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)\\) subject to: \\(\\sum_i^N \\alpha_i y_i = 0\\) and \\(0 \\le \\alpha_i \\le C\\)</p> <p>Prediction \\(f(x) = \\text{sign}(\\sum_i \\alpha_i y_i K(x_i,x) + b)\\)</p>"},{"location":"ml/SVM/#mercers-theorem","title":"Mercer's Theorem","text":"<ul> <li>Kernel \\(K(x,y)\\) is valid if and only if:</li> <li>Symmetric: \\(K(x,y) = K(y,x)\\)</li> <li>Positive semi-definite: \\(\\int\\int K(x,y)g(x)g(y)dxdy \\ge 0\\) for all g</li> <li>Ensures existence of feature space mapping \\(\\phi\\)</li> <li>Guarantees convergence of kernel optimization</li> </ul>"},{"location":"ml/SVM/#common-kernels","title":"Common Kernels","text":"<p>Linear Kernel</p> <ul> <li>\\(K(x,y) = x^T y\\)</li> <li>Equivalent to no transformation</li> <li>Used when data is linearly separable</li> </ul> <p>Polynomial Kernel</p> <ul> <li>\\(K(x,y) = (\\gamma x^Ty + r)^d\\)</li> <li>Parameters: degree \\(d\\), \\(\\gamma &gt; 0, r \\ge 0\\)</li> <li>Maps to space of polynomials up to degree d</li> <li>Captures feature interactions</li> </ul> <p>RBF (Gaussian) Kernel</p> <ul> <li>\\(K(x,y) = exp(-\\gamma||x-y||^2)\\)</li> <li>Parameter: \\(\\gamma &gt; 0\\) controls spread</li> <li>Maps to infinite dimensional space</li> <li>Most commonly used for non-linear data</li> <li>\\(\\gamma\\) large \\(\\rightarrow\\) high variance, low bias</li> <li>\\(\\gamma\\) small \\(\\rightarrow\\) low variance, high bias</li> </ul>"},{"location":"ml/SVM/#inference","title":"Inference","text":"<ul> <li>\\(f(x) = \\text{sign}(\\sum_{i \\in SV} \\alpha_i y_i K(x_i,x) + b)\\)</li> <li>Computational cost depends on number of SVs</li> </ul>"},{"location":"ml/SVM/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/TODO/","title":"TODO","text":"<ul> <li> Write Code for multiclass, SGD logistic regression.</li> <li> Variance Inflation Factors (VIF)</li> <li> (Overfitting) Reduce the number of features: We can use some feature selection methods \u2013 filter-based (chi-square), wrapper based (Recursive Feature Elimination) or embedded  like Lasso regularization. </li> <li> SMOTE(data imbalance)</li> <li>number of principle component (PCA)</li> <li> <p> GMM code</p> </li> <li> <p>Complete SVR in SVM etc</p> </li> </ul>"},{"location":"ml/clustering/","title":"Clustering","text":""},{"location":"ml/clustering/#clustering","title":"Clustering","text":""},{"location":"ml/clustering/#question","title":"Question","text":"<ul> <li>what is K-mean and gaussian mixture model ? what is hard/soft clustering ? what is EM ?</li> <li>how will you evaluate clustering ?</li> <li>How you will perform clustering in a distributed environment(simultaneously on multiple machines). (IMP)</li> <li>Example where K means fails and Spectral Works</li> </ul>"},{"location":"ml/clustering/DBSCAN/","title":"DBSCAN","text":""},{"location":"ml/clustering/DBSCAN/#dbscan","title":"DBSCAN","text":"<p>DBSCAN is a density-based clustering algorithm.</p>"},{"location":"ml/clustering/DBSCAN/#process","title":"Process:","text":"<p>Parameters: \u03b5 (epsilon), n (min points)</p> <ol> <li>Start with an arbitrary point p from the dataset</li> <li>Find all points within \u03b5 radius of point p (\u03b5-neighbors)</li> <li>If number of \u03b5-neighbors \u2265 n:</li> <li>Create a new cluster</li> <li>Add point p and its \u03b5-neighbors to the cluster</li> <li>For each neighbor point:<ul> <li>Find its \u03b5-neighbors</li> <li>If \u2265 n neighbors, add them to the cluster</li> <li>Continue expanding until no more points can be added</li> </ul> </li> <li>Mark processed points as visited</li> <li>Repeat steps 1-4 with unvisited points until all points are processed</li> </ol> <p>Pros: Arbitrary cluster shapes</p> <p>Cons: Two parameters to tune and fixed \u03b5 can't handle varying densities</p>"},{"location":"ml/clustering/DBSCAN/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/GMM/","title":"GMM","text":""},{"location":"ml/clustering/GMM/#gaussian-mixture-models-gmm","title":"Gaussian Mixture Models (GMM)","text":"<ul> <li>Soft clustering algorithm where data points can belong to multiple clusters with probability scores</li> <li>Models data as a mixture of K Gaussian distributions</li> <li>Each cluster is represented by a Gaussian distribution with its own parameters</li> </ul>"},{"location":"ml/clustering/GMM/#key-components","title":"Key Components","text":"<ol> <li>Parameters for each Gaussian:</li> <li>Mean (\u03bc)</li> <li>Covariance matrix (\u03a3)</li> <li>Weight/mixing coefficient (\u03c0)</li> </ol>"},{"location":"ml/clustering/GMM/#process","title":"Process","text":"<ol> <li>Initialization:</li> <li>Randomly initialize parameters for K Gaussians</li> <li> <p>Set initial weights, means, and covariance matrices</p> </li> <li> <p>Expectation-Maximization (EM):</p> </li> </ol> <p>a) Expectation Step (E-step):    - Calculate probability of each data point belonging to each cluster    - Compute posterior probabilities (responsibilities)</p> <p>b) Maximization Step (M-step):    - Update Gaussian parameters using weighted averages    - Recalculate means, covariances, and mixing coefficients</p> <ol> <li>Convergence:</li> <li>Repeat E-step and M-step until parameters converge</li> <li>Monitor log-likelihood for convergence criteria</li> </ol>"},{"location":"ml/clustering/GMM/#advantages","title":"Advantages","text":"<ul> <li>Provides soft assignments (probabilities)</li> <li>Can model elliptical clusters</li> <li>More flexible than K-means</li> <li>Handles overlapping clusters well</li> </ul>"},{"location":"ml/clustering/GMM/#limitations","title":"Limitations","text":"<ul> <li>Computationally more expensive than K-means</li> <li>Sensitive to initialization</li> <li>May converge to local optima</li> </ul>"},{"location":"ml/clustering/GMM/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/HDBSCAN/","title":"HDBSCAN","text":""},{"location":"ml/clustering/HDBSCAN/#hdbscan","title":"HDBSCAN:","text":"<ul> <li>Only needs n parameter</li> <li>Handles varying densities</li> <li>Slower than k-means but more versatile</li> <li>Recommended as first clustering approach</li> </ul>"},{"location":"ml/clustering/HDBSCAN/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/HierarchicalClustering/","title":"Hierarchical Clustering","text":""},{"location":"ml/clustering/HierarchicalClustering/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<ul> <li>Agglomerative (bottom-up): Similar to flood filling, starts with individual points and merges closest pairs</li> <li>Divisive (top-down): Starts with all points in one cluster and recursively splits</li> </ul>"},{"location":"ml/clustering/HierarchicalClustering/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/KMeans/","title":"k-Means","text":""},{"location":"ml/clustering/KMeans/#k-means-clustering","title":"K-means Clustering","text":""},{"location":"ml/clustering/KMeans/#process","title":"Process","text":"<ul> <li>Initialize number of k</li> <li>Randomly choose k points in the data as centroids </li> <li>While points do not change: (centroids do not change or max number of iter)<ul> <li>Assign each data point to its nearest centroid.</li> <li>Recompute the centroids of each cluster </li> </ul> </li> </ul>"},{"location":"ml/clustering/KMeans/#k","title":"K","text":""},{"location":"ml/clustering/KMeans/#optimal-k","title":"Optimal K","text":"<p>To find the optimal number of clusters (k), we use the Elbow Method: 1. Calculate the Within-Cluster Sum of Squares (WSS) for different values of k 2. Plot WSS vs k 3. Look for the \"elbow\" point - where increasing k starts giving diminishing returns 4. Choose k at this elbow point</p>"},{"location":"ml/clustering/KMeans/#advantages","title":"Advantages","text":"<ol> <li> <p>Simple and Intuitive: Easy to understand and implement; works well for basic clustering tasks; widely used in practice.</p> </li> <li> <p>Fast and Efficient: Linear time complexity O(nkd) where n is samples, k is clusters, d is dimensions; scales well with large datasets.</p> </li> <li> <p>Memory Efficient: Only stores centroids and cluster assignments; minimal memory requirements compared to other clustering methods.</p> </li> </ol>"},{"location":"ml/clustering/KMeans/#drawbacks","title":"Drawbacks","text":"<ol> <li> <p>Sensitive to Initialization: Results vary based on initial centroid positions, may get stuck in local optima; solution is to run multiple times with different initializations.</p> </li> <li> <p>Assumes Spherical Clusters: Works best with spherical, similarly sized clusters; struggles with elongated or irregular shapes; not suitable for varying densities.</p> </li> <li> <p>Requires Pre-specification of k: Number of clusters must be known beforehand; elbow method is subjective; different metrics may suggest different optimal k values.</p> </li> <li> <p>Sensitive to Outliers: Outliers can significantly influence centroid positions; may create clusters just for outliers; solution is to pre-process data.</p> </li> </ol>"},{"location":"ml/clustering/KMeans/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/dimReduction/","title":"Dimensionality Reduction","text":""},{"location":"ml/dimReduction/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>High dimensional data could be a problem</p> <ul> <li>An obvious reason would be more computing power/time needed to build a model </li> <li>The curse of dimensionality: When the number of dimensions (features) increases our model starts to become more complex and dependent on the data it was trained on and so it overfits thus reducing our model performance </li> <li>For models that use distance metrics, when the number of dimensions is too high, then each datapoint seems very similar to each other. This is because, with very large features, the distance between two data points are almost equal since they are all very far from each other. </li> </ul> <p>Method to resolve : PCA, LDA, t-SNE(non-linear correlation), AutoEncoder</p>"},{"location":"ml/dimReduction/#question","title":"Question","text":"What is curse of dimensionality? <ul> <li> <p>As dimensions increase, data analysis becomes exponentially harder.</p> <ul> <li>Data becomes sparse</li> <li>Distances become meaningless</li> <li>More data needed exponentially</li> <li>Computational cost increases</li> </ul> </li> <li> <p>Solution</p> <ul> <li>Dimensionality reduction</li> <li>Feature selection</li> </ul> </li> </ul> Explain different methods for feature selection and dimensionality reduction. <ul> <li> <p>Forward Feature Selection</p> <ul> <li>Start: Empty feature set</li> <li>Process: Iteratively add best performing feature</li> <li>Selection: Based on performance metric</li> <li>\\(F_{i+1} = F_i \\cup \\{\\text{argmax}_{f \\in F_i} Score(F_i \\cup {f})\\}\\)</li> </ul> </li> <li> <p>Backward Feature Selection</p> <ul> <li>Start: Full feature set</li> <li>Process: Iteratively remove worst feature</li> <li>Selection: Based on performance drop</li> <li>\\(F_{i+1} = F_i \\ \\{\\text{argmin}_{f \\in F_i} Score(F_i \\ {f})\\}\\)</li> </ul> </li> <li> <p>Information Gain</p> </li> <li>Lasso vs Ridge</li> </ul>"},{"location":"ml/dimReduction/LDA/","title":"LDA","text":""},{"location":"ml/dimReduction/LDA/#linear-discriminant-analysis-lda","title":"Linear Discriminant Analysis (LDA)","text":"<ul> <li>LDA is a <code>supervised dimensionality</code> reduction and classification algorithm. </li> <li>Unlike PCA which focuses on maximizing variance, LDA aims to find a linear combination of features that <code>maximizes class separation while minimizing within-class variance</code>.</li> </ul>"},{"location":"ml/dimReduction/LDA/#within-class-scatter-matrix-sw","title":"Within-Class Scatter Matrix (SW)","text":"<p>Measures the variance within each class: \\(S_W = \\sum_c S_c\\)</p> <p>Where for each class c: \\(S_c = \\sum_{i \\in c} (x_i - \\bar{x}_c) \\cdot (x_i - \\bar{x}_c)^T\\)</p>"},{"location":"ml/dimReduction/LDA/#between-class-scatter-matrix-sb","title":"Between-Class Scatter Matrix (SB)","text":"<p>Measures the variance between different classes: \\(S_B = \\sum_{c} n_c \\cdot (\\bar{x}_c - \\bar{x}) \\cdot (\\bar{x}_c - \\bar{x})^T\\)</p> <p>Where</p> <ul> <li>\\(n_c\\) is the number of samples in class c</li> <li>\\(\\bar{x}_c\\) is the mean of class c</li> <li>\\(\\bar{x}\\) is the overall mean</li> </ul>"},{"location":"ml/dimReduction/LDA/#fishers-linear-discriminant","title":"Fisher's Linear Discriminant","text":""},{"location":"ml/dimReduction/LDA/#objective-function","title":"Objective Function","text":"<p>To maximize class separation while minimizing within-class variance, we use Fisher's criterion: \\(J(w) = \\frac{w^TS_Bw}{w^TS_Ww}\\)</p> <p>Taking the derivative with respect to w and setting it to zero: \\(\\frac{\\partial}{\\partial w}J(w) = \\frac{2S_B w(w^T S_W w) - 2S_W w(w^T S_B w)}{(w^T S_W w)^2} = 0\\)</p> <p>Simplifying,</p> <p>\\(S_B w(w^T S_W w) = S_W w(w^T S_B w)\\)</p> <p>\\(S_W^{-1}S_B w = \\frac{(w^T S_B w)}{(w^T S_W w)} w\\)</p> <p>\\(S_W^{-1}S_B w = \\lambda w\\)</p>"},{"location":"ml/dimReduction/LDA/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Calculate the within-class scatter matrix (\\(S_W\\))</li> <li>Calculate the between-class scatter matrix (\\(S_B\\))</li> <li>Compute eigenvalues and eigenvectors of \\(S_W^{-1}S_B\\)</li> <li>Select top k eigenvectors based on eigenvalues</li> <li>Transform the data using selected eigenvectors</li> </ol>"},{"location":"ml/dimReduction/LDA/#advantages","title":"Advantages","text":"<ul> <li>Preserves class discriminatory information</li> <li>Reduces dimensionality while maintaining class separation</li> <li>Works well for normally distributed classes</li> </ul>"},{"location":"ml/dimReduction/LDA/#limitations","title":"Limitations","text":"<ul> <li>Assumes normal distribution of features</li> <li>May fail if within-class covariance is not equal across classes</li> <li>Limited by the number of classes (max components = number of classes - 1)</li> </ul>"},{"location":"ml/dimReduction/LDA/#question","title":"Question","text":"Does LDA has linear decision boundary? <ul> <li>Yes, the decision boundary is a straight line in 2D space, a plane in 3D space, or a hyperplane in higher dimensions.</li> </ul>"},{"location":"ml/dimReduction/LDA/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/dimReduction/PCA/","title":"PCA","text":""},{"location":"ml/dimReduction/PCA/#pca","title":"PCA","text":"<p>It is <code>unspervised machine learning</code></p> <p>Assumption:</p> <ul> <li> <p>PCA needs a linear correlation  between all variables  (It should not have  non-linear correlations )</p> </li> <li> <p>It is a linear combination of variables that results in a line or axis/axes that explain a maximal amount of variance from the original dataset. More formally, the eigenvectors of the covariance matrix (of the data) are the principal components and the eigenvalues represent the amount of variance carried in each principal component. </p> </li> <li> <p>If the eigenvectors are, all the same, PCA would not be able to select which principal component since we select the top n eigenvectors and there would be no top n since they are all equal. </p> </li> <li> <p>It is not necessary to remove variables that are highly correlated because PCA would project all the correlated variables onto the same principal  component. </p> </li> <li> <p>Choose number of principle component such that it captures 95-99% or the variance.</p> </li> </ul>"},{"location":"ml/dimReduction/PCA/#process","title":"Process","text":"<ol> <li>Standardize our data (since, PCA is sensitive to the variance within features.)</li> <li>Calculate the covariance matrix </li> <li>Then using this matrix we calculate eigenvectors and eigenvalues and thus the principal components from the eigenvectors</li> </ol>"},{"location":"ml/dimReduction/PCA/#drawback","title":"Drawback","text":"<ol> <li>Computationally expensive </li> <li>Information is always lost </li> <li>Explainability becomes much more difficult.</li> </ol>"},{"location":"ml/dimReduction/PCA/#question","title":"Question","text":"How to determine the optimal number of principal components (hyperplanes) in PCA? <ul> <li>Variance explained = \\(\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^N \\lambda_i}\\) </li> <li>Scree Plot Components:<ul> <li>X-axis: Principal component number</li> <li>Y-axis: variance explained</li> </ul> </li> <li>Choose number of componet if the variance explained reaches the threshold<ul> <li>\\(k = min\\{n: \\frac{\\sum_{i=1}^n \\lambda_i}{\\sum_{i=1}^N \\lambda_i} \\geq \\text{threshold}\\}\\)</li> </ul> </li> </ul>"},{"location":"ml/dimReduction/PCA/#code","title":"Code","text":"NumpyPyTorch"},{"location":"nlp/","title":"Natural Language Procesing","text":""}]}