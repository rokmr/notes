{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#notes","title":"Notes","text":""},{"location":"#explore-and-star-on-github","title":"Explore and star on GitHub","text":"<ul> <li> <p> rokmr/kick-start-ai</p> </li> <li> <p> rokmr/machine-learning</p> </li> <li> <p> rokmr/computer-vision</p> </li> <li> <p> rokmr/notes</p> </li> </ul>"},{"location":"basics/","title":"Basics","text":""},{"location":"cv/","title":"Intro","text":""},{"location":"cv/#overview","title":"Overview","text":""},{"location":"cv/#core","title":"Core","text":"<ul> <li> Basics</li> <li> Image Operations</li> <li> Image Annotations</li> <li> Image Enhancement</li> <li> Image Filtering</li> <li> Image Feature</li> <li> Image Alignment</li> <li> Panorama</li> <li> HDR</li> <li> Image Classification</li> <li> Image Segmentation</li> <li> Edge Detection &amp; Contours</li> <li> Object Detection</li> <li> Face Detection</li> <li> Optical Flow</li> <li> Object Tracking</li> <li> Pose Estimation</li> <li> OCR</li> </ul>"},{"location":"cv/#advance","title":"Advance","text":"<ul> <li> AutoEncoder</li> <li> ViT</li> <li> NeRF</li> </ul>"},{"location":"cv/#generative","title":"Generative","text":"<ul> <li> Variational Encoder</li> </ul>"},{"location":"cv/detection/","title":"Overview","text":""},{"location":"cv/detection/#object-detection","title":"Object Detection","text":""},{"location":"cv/detection/#introduction","title":"Introduction","text":"<p>The goal of object detection is to predict a set of bounding boxes(x,y,w,h) and category labels for each object of interest.</p>"},{"location":"cv/detection/#traditional","title":"Traditional","text":""},{"location":"cv/detection/#template-matching-sliding-window","title":"Template Matching + Sliding Window","text":"<p>For every position you evaluate how much do the pixels in the image and template correlate.</p> <p>Cons</p> <ol> <li>Does not handle occlusions.</li> <li>Works with instance of object but not with class of it.</li> <li>Does not work if pose changes.</li> <li>Does not work if position, scale and aspect ratio changes.</li> </ol>"},{"location":"cv/detection/#feature-extraction-and-classification","title":"Feature Extraction and Classification","text":"<p>Learn multiple weak classifier to build a strong final decision.</p>"},{"location":"cv/detection/#feature-extraction","title":"Feature Extraction","text":"<p>Viola-Jones Detector</p> <p>Haar Features</p> <p>Histogram of Oriented Gradients(HOGs) Compute gradients in dense grids, compute gradients and create a histogram based on gradient direction</p> <p>Deformable Part Model (DPM) Based on HOG features but based on body part detection. More robust to different body poses.</p>"},{"location":"cv/detection/#classification","title":"Classification","text":"<p>It is done with the help of SVM.</p>"},{"location":"cv/detection/#general-object-detection","title":"General Object Detection","text":"<ul> <li>Class agnostic</li> <li>Object Proposals / Region of Intrest<ul> <li>Selective search</li> <li>Edge boxes</li> </ul> </li> </ul> <p>Localization</p>"},{"location":"cv/detection/#two-stage-detector","title":"Two-Stage Detector","text":"<ul> <li>R-CNN, Fast R-CNN, Faster R-CNN</li> <li>SPP-Net, R-FCN, FPN</li> </ul> <ul> <li>Overfeat</li> <li>R-CNN, Fast R-CNN, Faster R-CNN, SPP-Net</li> </ul>"},{"location":"cv/detection/#one-stage-detector","title":"One-Stage Detector","text":"<p>No need of Region Proposal Network</p> <p>They are very fast</p> <ul> <li>YOLO, SSD, RetinaNet</li> <li>CenterNet, CornerNet, ExtremeNet</li> </ul> <ul> <li>YOLO</li> <li>RetinaNet</li> <li>CornerNet</li> <li>CenterNet</li> <li>ExtremeNet</li> </ul>"},{"location":"cv/detection/#transformer-based-detector","title":"Transformer-Based Detector","text":"<ul> <li>DETR</li> </ul>"},{"location":"cv/detection/#methods","title":"Methods","text":"<ul> <li>Swin Transformer</li> <li>DINO</li> <li>InternImage</li> <li>OWL</li> </ul>"},{"location":"cv/detection/CenterNet/","title":"CenterNet","text":""},{"location":"cv/detection/CenterNet/#centernet","title":"CenterNet","text":"<ul> <li>Focus on the center of the object to infer its class.</li> <li>Use the corners as proposals, and the center to verify the class of the object and filter out outliers.</li> </ul>  CenterNet Architecture   Center Pooling Module"},{"location":"cv/detection/CornerNet/","title":"CornerNet","text":""},{"location":"cv/detection/CornerNet/#cornernet","title":"CornerNet","text":"<p>Bounding box cordinates as top-left and bottom-right corner.</p>  Hourglass network   Corner pooling"},{"location":"cv/detection/CornerNet/#issues","title":"Issues","text":"<ul> <li>Many incorrect bounding boxes (especially small) \\(\\rightarrow\\) too many False Positives</li> <li>Hypothesis: It is hard to infer the class of the box if the network is focused on the boundaries</li> </ul>"},{"location":"cv/detection/DETR/","title":"DETR","text":""},{"location":"cv/detection/DETR/#detr","title":"DETR","text":"<p>A direct set prediction approach to bypass the surrogate tasks (like proposals, anchors, window centers, non-maximum suppression).</p> <p>A encoder-decoder based architecure.</p> <p>It predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.</p>"},{"location":"cv/detection/DETR/#components","title":"Components","text":""},{"location":"cv/detection/DETR/#set-prediction","title":"Set Prediction","text":"<p><code>Bipartite matching loss</code></p> <p>The usual solution is to design a loss based on the Hungarian algorithm, to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match.</p>"},{"location":"cv/detection/DETR/#transformers-and-parallel-decoding","title":"Transformers and Parallel Decoding","text":"<p>Combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction.</p>"},{"location":"cv/detection/DETR/#object-detection","title":"Object detection","text":"<ol> <li>Two-Stage detectors</li> <li>One-Stage detectors</li> </ol> <p>The final performance of above systems heavily depends on the exact way these initial guesses are set.</p>"},{"location":"cv/detection/DETR/#model","title":"Model","text":""},{"location":"cv/detection/DETR/#object-detection-set-prediction-loss","title":"Object detection set prediction loss","text":"<p>DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image.</p> <p>\\(y:\\) ground-truth (\\(y_i = (c_i, b_i)\\), \\(c_i\\) rget class label and \\(b_i \\in [0,1]^4\\) is ground-truth box.)</p> <p>\\(\\hat{y} = \\{\\hat{y}_i\\}_{i=1}^{N}:\\) set of N predictions</p> <p>\\(y\\) also as a set of size \\(N\\) padded with \\(\\varnothing\\) (no object)</p> <p>Permutation of element with lowest cost:</p> <p>\\(\\hat{\\sigma} = \\arg\\min_{\\sigma\\in\\mathfrak{G}_N} \\sum_{i}^N \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)})\\)</p> <p>Pair-wise matching cost</p> <p>\\(\\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)}) = -\\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\hat{p}_{\\sigma(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)})\\)</p> <p>\\(\\hat{p}_{\\sigma_i}(c_i):\\) prediction with index \\(\\sigma(i)\\) we define probability of class \\(c_i\\)</p> <p>Hungarian loss</p> <p>\\(\\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^N \\left[-\\log \\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)})\\right]\\)</p> <p>Box-Loss</p> <p>\\(\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{\\text{L1}}\\|b_i - \\hat{b}_{\\sigma(i)}\\|_1\\)</p>"},{"location":"cv/detection/DETR/#detr-architecture","title":"DETR architecture","text":"<p>CNN Backbone</p> <p>As a feature extractor</p> <p>Transformer encoder</p> <p>Since the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings that are added to the input of each attention layer.</p> <p>Transformer decoder</p> <p>The difference with the original transformer is that our model decodes the N objects in parallel at each decoder layer, while Vaswani et al use an autoregressive model that predicts the output sequence one element at a time.</p> <p>Prediction feed-forward networks (FFNs)</p> <p>The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer.</p> <p>Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total N = 100 prediction slots in DETR decoder. </p> <p>This shows that the prediction box focuses on different part of the image.</p>"},{"location":"cv/detection/DETR/#rt-detr","title":"RT-DETR","text":""},{"location":"cv/detection/DETR/#fast-detr","title":"Fast-DETR","text":""},{"location":"cv/detection/DETR/#detrs-fastdetr","title":"DETRs-FastDETR","text":""},{"location":"cv/detection/DETR/#sam-det","title":"SAM-Det","text":""},{"location":"cv/detection/DETR/#ultra-detr","title":"ULTRA-DETR","text":""},{"location":"cv/detection/DINO/","title":"DINO","text":""},{"location":"cv/detection/DINO/#dino","title":"DINO","text":""},{"location":"cv/detection/DINO/#grounding-dino","title":"Grounding DINO","text":""},{"location":"cv/detection/ExtremeNet/","title":"ExtremeNet","text":""},{"location":"cv/detection/ExtremeNet/#extremenet","title":"ExtremeNet","text":"<ul> <li>Precticting the corner where object does not lies is hard for CNNs.</li> <li>Represent objects by their extreme points.</li> <li>No need to predict embeddings for the box computation.</li> </ul>  ExtremeNet Arch."},{"location":"cv/detection/ExtremeNet/#applications","title":"Applications","text":"<ul> <li>Extreme points are used commonly for annotation</li> </ul>"},{"location":"cv/detection/InternImage/","title":"InternImage","text":""},{"location":"cv/detection/OWL/","title":"OWLv2","text":""},{"location":"cv/detection/Overfeat/","title":"Overfeat","text":""},{"location":"cv/detection/Overfeat/#overfeat","title":"Overfeat","text":"<p>Slidingwindow + bbox regression + classification</p>"},{"location":"cv/detection/Overfeat/#sliding-window","title":"Sliding Window","text":"<p>Implicity encoded in the CNN architecture. Use sliding widow at different scale.</p>"},{"location":"cv/detection/Overfeat/#localization","title":"Localization","text":"<p>Regression</p>"},{"location":"cv/detection/Overfeat/#detection","title":"Detection","text":"<p>Classification</p>"},{"location":"cv/detection/Overfeat/#cons","title":"Cons","text":"<ul> <li>Needs fixed sized window as the fully-connected layer need to have fixed input.</li> <li>Expensive to try out all the possible positions, scales and aspect ratio. (Choose only the potential location)</li> </ul>"},{"location":"cv/detection/RCNN/","title":"RCNNs","text":""},{"location":"cv/detection/RCNN/#rcnn","title":"RCNN","text":""},{"location":"cv/detection/RCNN/#rcnn_1","title":"RCNN","text":"<p>Steps</p> <ol> <li> <p>Scan the input image for possible objects using an algorithm called Selective Search, generating ~2000 region proposals</p> </li> <li> <p>Warp to a fix size 227 x 227</p> </li> <li> <p>Run a convolutional neural net (CNN) on top of each of these region proposals</p> </li> <li> <p>Make the output of each CNN and feed it into:</p> <p>a) an SVM to classify the region and </p> <p>b) a linear regressor to tighten the bounding box of the object, if such an object exists.</p> </li> </ol>"},{"location":"cv/detection/RCNN/#training","title":"Training","text":"<ol> <li>Pre-train the CNN on ImageNet</li> <li>Finetune the CNN on the number of classes the detector is aiming to classify (softmax loss).</li> <li>Train a linear Support Vector Machine classifier to classify image regions. One SVM per class! (hinge loss)</li> <li>Train the bounding box regressor (L2 loss)</li> </ol> <p>Cons 1. If we have overlapping window then we will do ConvNet computation for each of the pixels more than 1 times. This increases extra computation.</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#spp-net","title":"SPP Net","text":"<p>Makes the RCNN fast at test time.</p> <p>Issues</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#fast-rcnn","title":"Fast RCNN","text":"<ol> <li>Performing feature extraction over the image before proposing regions, thus only running one CNN over the entire image instead of 2000 CNN\u2019s over 2000 overlapping regions.</li> <li>After conv5 there is FC layer we need to make all the deature size need to be of same size using RoI Pooling layer.</li> <li>Replacing the SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn","title":"Faster RCNN","text":"<ul> <li>Removes region proposal network Can we reuse our CNN feature and still be able to create this proposal.</li> </ul> <p>How to extract proposals.</p> <ul> <li> <p>How many proposals?</p> <ul> <li>Decide a fix number</li> <li>set of 9 anchor box per location (3 scales, 3 aspect ratio)</li> </ul> </li> <li> <p>Where are they placed?</p> <ul> <li>Densly</li> </ul> </li> <li> <p>For each of the location get the descriptor of 256-d by 3X3 filtermap</p> </li> <li> <p>Pass the descriptor to the classification layer and regression layer</p> </li> </ul>"},{"location":"cv/detection/RCNN/#rpn-training","title":"RPN Training","text":"Region Proposal Network  <p>Classification Ground-Truth</p> <p>\\(p^{*}\\) Amount of anchor box overlapping with the Ground-Truth.</p> <p>\\(p^{*} = 1\\) if IoU &gt; 0.7 (anchor is in foreground)</p> <p>\\(p^{*} = 0\\) if IoU &lt; 0.3 (anchor is in background)</p> <p>For training only consider above two case.</p> <ol> <li>Randomly sample 256 sample, form mini-batch.</li> <li>Calculate binary CE loss</li> <li>Anchor box containing object will go through regression box</li> </ol> <p>Anchor box \\((x_a, y_a, w_a, h_a)\\), \\(x_a, y_a\\) is center of box and rest width and height respectively.</p> <ol> <li>Network actually predicts are \\((t_x, t_y, t_w, t_h)\\) which are relative.</li> </ol> <p>\\(t_x = (x-x_a)/w_a\\)</p> <p>\\(t_y = (y-y_a)/h_a\\)</p> <p>\\(t_w = \\log(w/w_a)\\)</p> <p>\\(t_h = \\log(h/h_a)\\)</p> <ol> <li>Smooth L1 loss on regression targets</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn-training","title":"Faster RCNN Training","text":"<p>Can be train jointly. But in paper it is trained in following manner.     - RPN classification (object/non-object)     - RPN regression (anchor -&gt; proposal)     - Fast R-CNN classification (type of object)     - Fast R-CNN regression (proposal -&gt; box)</p> <p>Pros</p> <ol> <li>10x faster at test time wrt Fast R-CNN</li> <li>Trained end-to-end including feature extraction, region proposals, classifier and regressor.</li> <li>More accurate, since proposals are learned. RPN is fully convolutional.</li> </ol>"},{"location":"cv/detection/RCNN/#conclusion","title":"Conclusion","text":"R-CNN Fast RCNN Faster RCNN Test time per image (sec) 50 2 0.2 Speeed-Up 1X 25X 250X mAP (VOC 2007) 66.0 66.9 66.9"},{"location":"cv/detection/RetinaNet/","title":"RetinaNet","text":""},{"location":"cv/detection/RetinaNet/#retinanet","title":"RetinaNet","text":"<p>Since there are lots of anchor box in which there is no object and very few of them object. We need to incorporate this information in the loss function which is done with the weighting of the loss function.</p>  Focal Loss  <p>As \\(\\gamma\\) increases the easy sample weight decreases.</p>"},{"location":"cv/detection/RetinaNet/#key-point","title":"Key Point","text":"<ul> <li>Proposed: Focal loss</li> <li>Powerful feature extraction: ResNet</li> <li>Multi-scale prediction</li> <li>9 anchors per level, each one with a classification and regression target</li> </ul>"},{"location":"cv/detection/SSD/","title":"SSD","text":""},{"location":"cv/detection/SelectiveSearch/","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#selective-search","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#wip","title":"WIP","text":""},{"location":"cv/detection/SwinTransformer/","title":"Swin Transformer","text":""},{"location":"cv/detection/YOLO-World/","title":"YOLO-World","text":""},{"location":"cv/detection/YOLO-World/#yolo-world","title":"YOLO-World","text":"<ul> <li> <p>YOLO with openvocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets.</p> </li> <li> <p>Propose a new Re-parameterizable VisionLanguage Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information.</p> </li> <li> <p>\\(\\Omega = \\{B_i, t_i \\}_{i=1}^{N}\\) where \\(t_i\\) is the corresponding text for the region \\(B_i\\)</p> </li> </ul>  YOLO-World Architecture   YOLO-World Architecture  <p>Architecture 1. YOLO Detector      - YOLOv8 2. Text Encoder      - CLIP 3. Text Contrastive Head      - object-text similarity     - \\(s_{k,j} = \\alpha \\cdot \\text{L2-Norm}(e_k) \\cdot \\text{L2-Norm}(w_j)^T + \\beta,\\)     - \\(e_k\\) object embedding and \\(w_j\\) word embedding     - \\(\\alpha\\) and \\(\\beta\\) are learnable.</p>"},{"location":"cv/detection/YOLO-World/#re-parameterizable-vision-language-pan","title":"Re-parameterizable Vision-Language PAN","text":"<ul> <li>establish the feature pyramids {P3,P4,P5} with the multi-scale image features {C3,C4,C5}</li> <li>propose the Text-guided CSPLayer (T-CSPLayer) and Image-Pooling Attention (I-Pooling Attention) to further enhance the interaction between image features and text features, which can improve the visual-semantic representation for open-vocabulary capability</li> </ul>  RepVL-PAN"},{"location":"cv/detection/YOLO-World/#text-guided-csplayer","title":"Text-guided CSPLayer","text":"<p>Text-guided cross-stage partial layers</p> <ul> <li>text embeddings W</li> <li>Image features \\(X_l \\in R^{H \\times W \\times D} (l \\in \\{3, 4, 5 \\})\\)</li> <li>\\(\\delta\\) sigmoid function \\(X_l' = X_l \\cdot \\delta(\\max_{j \\in \\{1..C\\}} (X_lW_j^{\\top}))\\)</li> </ul>"},{"location":"cv/detection/YOLO-World/#image-pooling-attention","title":"Image-Pooling Attention","text":"<ul> <li>Max-Pooling output \\(\\tilde{X}\\)</li> </ul> <p>\\(W' = W + \\text{MultiHead-Attention}(W, \\tilde{X}, \\tilde{X})\\)</p>"},{"location":"cv/detection/YOLO-World/#training","title":"Training","text":"<p>\\(\\mathcal{L}(I) = \\mathcal{L}_{con} + \\lambda_I \\cdot (\\mathcal{L}_{iou} + \\mathcal{L}_{dfl})\\)</p> <ul> <li>\\(\\mathcal{L}_{con}\\) : region-text contrastive loss</li> <li>\\(\\mathcal{L}_{dfl}\\) : distributed focal loss</li> <li>\\(\\lambda_I \\in \\{0, 1\\}\\) <ul> <li>1 if \\(I\\) is from detection or grounding data</li> <li>0 if \\(I\\) is from the image-text data</li> </ul> </li> </ul>"},{"location":"cv/detection/YOLO/","title":"YOLO","text":""},{"location":"cv/detection/YOLO/#yolo","title":"YOLO","text":"<p>It does not have region proposal network and also it does not have the fully-connected layer. </p>  You Only Look Once"},{"location":"cv/detection/YOLO/#process","title":"Process","text":"<ol> <li>Divide the image into grid (SxS cells).</li> <li>Predict B anchor box at the center of each box along with the confidence score</li> <li>Predict C classes for each grid cell.</li> </ol> <p>YOLO-Tensor : SxS(Bx5 + C) </p> <p>where, - SxS : number of grid</p> <ul> <li> <p>B : Number of bbox \\((P_c, b_x, b_y, b_h, b_w)\\)</p> </li> <li> <p>C : Number of classes</p> </li> </ul>  Predicting bounding box and confidence score for each cell.   Predicting class probability for each cell."},{"location":"cv/detection/YOLO/#loss-function","title":"Loss Function","text":"<p>\\(L_{total} = L_{localization} + L_{confidence} + L_{classification}\\)</p> <p>$ L_{localization} = \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 + (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2]$</p> <p>\\(L_{confidence} =  \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 + \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} (C_i - \\hat{C}_i)^2\\)</p> <p>\\(L_{classification} = \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum_{c\\in classes} (p_i(c) - \\hat{p}_i(c))^2\\)</p> <p>\\(\\lambda_{coord} = 5\\)</p> <p>\\(\\lambda_{noobj} = 0.5\\)</p>"},{"location":"cv/detection/YOLO/#yolo-papers","title":"YOLO Papers","text":""},{"location":"cv/detection/YOLO/#yolo-survey","title":"YOLO Survey","text":""},{"location":"cv/detection/YOLO/#yolo-v1-slides","title":"YOLO-v1 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v2-slides","title":"YOLO-v2 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v3","title":"YOLO-v3","text":""},{"location":"cv/detection/YOLO/#yolo-v4","title":"YOLO-v4","text":""},{"location":"cv/detection/YOLO/#yolo-v5-colab","title":"YOLO-v5 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v6-colab","title":"YOLO-v6 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v7-colab","title":"YOLO-v7 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v8","title":"YOLO-v8","text":""},{"location":"cv/detection/YOLO/#yolo-nas","title":"YOLO-NAS","text":""},{"location":"cv/detection/YOLOE/","title":"YOLOE","text":""},{"location":"cv/faceDetection/","title":"Overview","text":""},{"location":"cv/faceDetection/#face-detection","title":"Face Detection","text":""},{"location":"cv/faceDetection/#haar-cascade","title":"Haar Cascade","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#hog-svm","title":"HoG + SVM","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#facenet","title":"FaceNet","text":"FaceNet   Triplet Loss"},{"location":"cv/faceDetection/#loss","title":"Loss","text":"<p>\\(L = \\sum_{i}^{N} \\left[\\|f(x_i^a) - f(x_i^p)\\|_2^2 - \\|f(x_i^a) - f(x_i^n)\\|_2^2 + \\alpha\\right]_+\\)</p>"},{"location":"cv/nerf/","title":"Overview","text":""},{"location":"cv/objectTracking/","title":"Overview","text":""},{"location":"cv/objectTracking/#object-tracking","title":"Object Tracking","text":"<p>To continuously locate and maintain the identity of target objects as they move through video frames.</p> <p>Tracking is similarity measurement, correlation, correspondence, Matching/retrieval &amp; data association.</p>"},{"location":"cv/objectTracking/#learining","title":"Learining","text":""},{"location":"cv/objectTracking/#appearance","title":"Appearance","text":"<p>we need to know how the target looks like - Single object tracking - Re-identification</p>"},{"location":"cv/objectTracking/#motion","title":"Motion","text":"<p>To make predictions of where the targets goes - Trajectory prediction</p>"},{"location":"cv/objectTracking/#single-target-tracking","title":"Single Target Tracking","text":""},{"location":"cv/objectTracking/#as-a-matchingcorrespondence-problem","title":"As a matching/correspondence problem","text":"<ul> <li>GOTURN: no online appearance modeling</li> </ul> <p>Input: what to track?</p> <p>Architecture: conv + concatenate + FC</p> <p>Pros: - No training, very fast as it is juct template matching problem</p> <p>Cons: - Does not work if the object moves very fast and goes out of search window.</p>"},{"location":"cv/objectTracking/#as-an-appearance-learning-problem","title":"As an appearance learning problem","text":"<ul> <li>MDNet: quick online finetuning of the network</li> <li>Slow: not suitable for real-time applications</li> <li>Solution: train as few layers as possible</li> </ul> <p>At test time, we need to train fc6 (up to fc4 if wanted)</p> <p>Pros: - No previous location assumption, the object can move anywhere in the image</p> <p>Cons: - Not as fast as GOTURN</p>"},{"location":"cv/objectTracking/#as-a-temporal-prediction-problem","title":"As a (temporal) prediction problem","text":"<ul> <li>ROLO = CNN + LSTM</li> </ul> <p>LSTM receives the heatmap for the object\u2019s position and the 4096 descriptor of the image</p>"},{"location":"cv/objectTracking/#challanges","title":"Challanges","text":"<ul> <li>Occlusions</li> <li>Viewpoint/pose/blur/illumination variations (in a few frames of a sequence)</li> <li>Background clutter</li> </ul>"},{"location":"cv/objectTracking/#multiple-object-tracing","title":"Multiple Object Tracing","text":""},{"location":"cv/objectTracking/#online-tracking","title":"Online Tracking","text":"<ul> <li>Processes two frames at a time</li> <li>For real-time applications</li> <li>Prone to drifting \u00e0 hard to recover from errors or occlusions</li> </ul> <p>Process - Track initialization (e.g. using a detector) - Prediction of the next position (motion model)     - Kalman filter     - Recurrent architecture     - constant velocity model (works really well at high framerates and without occlusions!)</p> <ul> <li>Matching predictions with detections (appearance model)<ul> <li>Bipartite matching</li> </ul> </li> </ul>"},{"location":"cv/objectTracking/#track-initialization","title":"Track initialization","text":"<p>Making a detector into a tracktor - Tracktor: a method trained as a detector but with tracking capabilities. - Where did the detection with ID1 go in the next frame? </p> <p>Two-Step Detector - Region Proposal - Regression</p> <p>Pros - Tracktor are online - We can train our model on still images - We can reuse an extremely well-trained regressor</p> <p>Cons - Confusion in crowded places as there is no notion of identification. - The track is killed if the target becomes occluded. - Will not work if the object or the camera has large motions.</p> <p>1st &amp; 2nd can be solved using ReID (Re-Identification). While 2nd &amp; 3rd can be solved using Motion model.</p> <p>Modeling Appearence : Re-ID Modeling Motion : Model Motion </p>"},{"location":"cv/objectTracking/#prediction-of-the-next-position","title":"Prediction of the next position","text":""},{"location":"cv/objectTracking/#matching-predictions-with-detections-appearance-model","title":"Matching predictions with detections (appearance model)","text":""},{"location":"cv/objectTracking/#offline-tracking","title":"Offline Tracking","text":"<ul> <li>Processes a batch of frames</li> <li>Good to recover from occlusions</li> <li>Not suitable for real-time applications</li> <li>Suitable for video analysis</li> </ul>"},{"location":"cv/objectTracking/#challanges_1","title":"Challanges","text":"<ul> <li>Multiple objects of the same type</li> <li>Heavy occlusions</li> <li>Appearance is often very similar</li> </ul>"},{"location":"cv/objectTracking/#applications","title":"Applications","text":"<ul> <li>Surveillance &amp; Security</li> <li>Traffic Monitoring</li> <li>Autonomous Vehicles</li> <li>Sports Analytics</li> <li>Human-Computer Interaction</li> <li>Medical Imaging</li> <li>Robotics</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/","title":"Bipartite Matching","text":""},{"location":"cv/objectTracking/BipartiteMatching/#bipartite-matching","title":"Bipartite Matching","text":"<ul> <li>Links detections with predictions using distance metrics</li> <li>Uses IoU, Pixel, or 3D distances between boxes</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#process","title":"Process","text":"<ul> <li>Calculate distances between boxes</li> <li>Apply Hungarian algorithm for optimal matching</li> <li>Set thresholds to handle missing/unsuitable matches</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#edge-cases","title":"Edge Cases","text":"<ul> <li>Missing prediction: Add dummy nodes</li> <li>No suitable match: Use cost threshold</li> <li>Unmatched boxes: Create new tracks</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#output","title":"Output","text":"<ul> <li>Matched pairs</li> <li>New tracks from unmatched detections</li> <li>Lost tracks from unmatched predictions</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#hungarian-algorithm","title":"Hungarian algorithm","text":"<p>Demo</p>"},{"location":"cv/objectTracking/KalmanFilter/","title":"Kalman Filter","text":""},{"location":"cv/opticalFlow/","title":"Overview","text":""},{"location":"cv/opticalFlow/#optical-flow","title":"Optical Flow","text":"<p>Optical flow refers to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer (camera) and the scene. It helps understand how objects move in a sequence of images.</p> <p>The optical flow problem involves estimating a dense vector field where each vector represents the displacement of a pixel from one frame to the next. This displacement field captures both the magnitude and direction of motion for each point in the image.</p> <p>Key characteristics of optical flow: 1. Dense Motion Field: Unlike feature tracking which follows specific points, optical flow estimates motion for every pixel in the image 2. Temporal Consistency: Assumes that pixel intensities remain constant between consecutive frames (brightness constancy assumption) 3. Spatial Smoothness: Nearby pixels tend to move in similar ways (smoothness constraint)</p>"},{"location":"cv/opticalFlow/#methods","title":"Methods:","text":"<ul> <li>Patched Based<ol> <li>Lucas-Kanade</li> <li>Horn-Shunck</li> </ol> </li> <li>NN Based<ol> <li>FlowNet</li> </ol> </li> </ul>"},{"location":"cv/opticalFlow/#perception-of-motion","title":"Perception of Motion","text":"Figure 1: Illustration of optical flow and motion perception. The top left image shows a person running with a static camera (no camera motion), while the top right image shows the same person running but with the camera moving in the opposite direction. The bottom diagrams visualize the resulting optical flow vectors for each scenario. <p>Assuming Image intensity is constant.</p> <p>Brightness Constancy Equation:</p> <p>\\(I(x,y,t) \\approx I(x+dx,y+dy,t+dt)\\)</p> <p>Using Taylor Series Expansion:</p> <p>\\(I(x(t)+u.\\Delta t,y+v.\\Delta t) - I(x(t),y(t),t) \\approx 0\\)</p> <p>\\(I_x \\cdot u +  I_y \\cdot v + I_t = 0\\) (Brightness Constancy Constraint)</p> <p>\\([u, v]\\) is the optical flow.</p>"},{"location":"cv/opticalFlow/#lucas-and-kanade","title":"Lucas and Kanade","text":"<p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\(2(I_xu+ I_yv+ I_t)I_x = 2(I_xu+ I_yv+ I_t)I_y = 0\\)</p> <p>\\(\\begin{bmatrix} \\sum I_{x}^2 &amp; \\sum I_{x}I_{y} \\\\ \\sum I_{x}I_{y} &amp; \\sum I_{y}^2 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} \\sum I_{x}I_{t} \\\\ \\sum I_{y}I_{t} \\end{bmatrix}\\)</p> <p>Structural Tensor representation:</p> <p>\\(\\begin{bmatrix} T_{xx} &amp; T_{xy} \\\\ T_{xy} &amp; T_{yy} \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} T_{xt} \\\\ T_{yt} \\end{bmatrix}\\)</p> <p>$u = \\frac{T_{yt}T_{xy} - T_{xt}T_{yy}}{T_{xx}T_{yy} - T_{xy}^2} \\text{ and } v = \\frac{T_{xt}T_{xy} - T_{yt}T_{xx}}{T_{xx}T_{yy} - T_{xy}^2} $</p>"},{"location":"cv/opticalFlow/#issues","title":"Issues","text":"<ul> <li>Brightness constancy is not satisfied (Correlation based method could be used)</li> <li>A point may not move like its neighbors (Regularization based methods)</li> <li>The motion may not be small (Taylor does not hold!) (Multi-scale estimation could be used)</li> </ul>"},{"location":"cv/opticalFlow/#horn-schunck","title":"Horn &amp; Schunck","text":"<p>Global method with smoothness constraint to solve aperture problem</p> <p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 + \\alpha^2(|\\nabla u|^2 + |\\nabla v|^2) dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\((I_xu+ I_yv+ I_t)I_x - \\alpha^2(|\\nabla u|)= (I_xu+ I_yv+ I_t)I_y + \\alpha^2(|\\nabla u|) = 0\\)</p>"},{"location":"cv/opticalFlow/#flownet","title":"FlowNet","text":"<p>End-to-end frame work to for optical flow prediction.</p>"},{"location":"cv/opticalFlow/#simplenet","title":"SimpleNet","text":"<p>Both input images together and feed them through a rather generic network, allowing the network to decide itself how to process the image pair to extract the motion information.</p>"},{"location":"cv/opticalFlow/#flownetcorr","title":"FlowNetCorr","text":"<p>First produce meaningful representations of the two images separately and then combine them on a higher level. <code>correlation layer</code> performs multiplicative patch comparisons between two feature maps.</p> <p>Correlation of 2 patches of size \\(K \\times K\\) is given by :</p> <p>\\(c(\\mathbf{x}_1, \\mathbf{x}_2) = \\sum_{\\mathbf{o} \\in [-k,k] \\times [-k,k]} \\langle \\mathbf{f}_1(\\mathbf{x}_1 + \\mathbf{o}), \\mathbf{f}_2(\\mathbf{x}_2 + \\mathbf{o}) \\rangle\\)</p> <p>where \\(K = 2k+1\\). Above equation is similar to convolution but it is convolution of one data with another instead of filter.</p>"},{"location":"cv/opticalFlow/#flownetrefine","title":"FlowNetRefine","text":"<p>The main ingredient are \u2018upconvolutional\u2019 layers, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. To perform the refinement, we apply the \u2018upconvolution\u2019 to feature maps, and concatenate it with corresponding feature maps from the \u2019contractive\u2019 part of the network and an upsampled coarser flow prediction (if available). </p> <p>This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps.</p>"},{"location":"cv/opticalFlow/#application","title":"Application","text":"<ol> <li>Motion based segmentation</li> <li>SfM</li> <li>Alignment (e.g., UAV analysis)</li> <li>Video Compression</li> <li>Object Tracking</li> <li>Deformation Analysis</li> </ol>"},{"location":"cv/poseEstimation/","title":"Overview","text":""},{"location":"cv/poseEstimation/#pose-estimation","title":"Pose Estimation","text":"<ul> <li>Estimate a 2D pose (x,y) coordinates for each joint from a RGB image.</li> <li>17 joints</li> <li>Challanges<ul> <li>occlusions</li> <li>clothing</li> <li>extreme poses</li> <li>viewpoint changes etc.</li> </ul> </li> </ul>"},{"location":"cv/poseEstimation/#direct-regression","title":"Direct Regression","text":"DeepPose"},{"location":"cv/poseEstimation/#heatmap-predicion","title":"HeatMap Predicion","text":"<ul> <li>Instead of prediction by regression, for each joint one predicts a full image with a heatmap of the joint location</li> <li>Powerful representation, easier to predict a confidence per location, rather than regress a value for the position</li> <li>Ground truth (GT) heatmap is constructed by placing a 2D Gaussian around the joint position (e.g. variance 1.5 pixels)</li> <li>Loss: MSE between predicted and GT heatmap</li> </ul>  Newell predicted heatmap  <p>Bringing the structure of the problem - Body parts are linked to each other - Body symmetries - Joint limits, e.g., elbow cannot bend backwards - Physical connectivity: elbow connected to wrist</p> <p>Using graphical models also allows us to find the pose of several targets</p>  DeepCut  <p>Alternatively one ca do two stage process</p> <ol> <li>Object Detection</li> <li>Pose Estimation</li> </ol>"},{"location":"cv/segmentation/","title":"Overview","text":""},{"location":"cv/segmentation/#semantic-segmentation","title":"Semantic Segmentation","text":""},{"location":"cv/segmentation/#instace-based-segmentation","title":"Instace-Based Segmentation","text":""},{"location":"cv/segmentation/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Region-based<ul> <li>e.g., IoU or Jaccard index, F-measure  or Dice\u2019s coefficient,  weighted F-measure,</li> </ul> </li> <li>Boundary-based <ul> <li>CM, boundary F-measure, boundary IoU, boundary displacement error (BDE), Hausdorff distances, </li> </ul> </li> <li>Structure-based<ul> <li>S-measure, E-measure, </li> </ul> </li> <li>Confidence-based <ul> <li>MAE </li> </ul> </li> </ul>"},{"location":"cv/segmentation/#dichotomous-image-segmentation","title":"Dichotomous Image Segmentation","text":""},{"location":"cv/segmentation/#dis","title":"DIS","text":""},{"location":"cv/segmentation/#birefnet","title":"BiRefNet","text":""},{"location":"cv/segmentation/#methods","title":"Methods","text":""},{"location":"cv/segmentation/#pdfnet","title":"PDFNet","text":""},{"location":"cv/segmentation/#ben","title":"BEN","text":""},{"location":"cv/segmentation/BiRefNet/","title":"BiRefNet","text":""},{"location":"cv/segmentation/BiRefNet/#birefnet","title":"BiRefNet","text":"<p>Bilateral Reference for High-Resolution Dichotomous Image Segmentation</p> <ul> <li>Swin-Transformer Based</li> <li>inward reference(InRef) and an outward reference (OutRef)</li> </ul> <p>Two essential Module:</p> <ol> <li>Localization module (LM) : <ul> <li>object localization using global semantic information</li> <li>extract hierarchical features from vision transformer backbone, which are combined and squeezed to obtain corase predictions in low resolution in deep layers.</li> </ul> </li> <li>Reconstruction module (RM) : <ul> <li>hierarchical patches of images provide the source reference, and gradient maps serve as the target reference.</li> <li>the inward and outward references as bilateral references (BiRef), in which the source image and the gradient map are fed into the decoder at different stages.</li> </ul> </li> </ol>  BiRefNet Comparison"},{"location":"cv/segmentation/BiRefNet/#localization-module","title":"Localization Module","text":"BiRefNet Architecture  <ul> <li>Transformer Encoder extract the fearures at different stages i.e., \\(F_1^e, F_2^e, F_3^e\\) with resolution at 4,8,16,32. </li> <li>The features of the first four \\(\\{F_i^e\\}_{i=1}^3\\) are transferred to the corresponding decoder stages with lateral connections (1\u00d71 convolution layers).</li> <li>These features are stacked and concatenated in the last encoder block to generate \\(F^e\\) then fed into a classification module.</li> </ul> <p>To enlarge the receptive fields to cover features of large objects and focus on local features for high precision simultaneously Atrous Spatial Pyramid Pooling (ASPP)  is used for multi-context fusion.</p>"},{"location":"cv/segmentation/BiRefNet/#reconstruction-module","title":"Reconstruction Module","text":"BiRef Blocks  <ul> <li> <p>Small receptive field (RFs) lead to inadequate context information to locate the right target on a large background, whereas large RFs often result in insufficient feature extraction in detailed areas.</p> </li> <li> <p>To achieve balance, using reconstruction block (RB) in each BiRef block as a replacement for the vanilla residual blocks.</p> </li> <li> <p>In RB, we employ deformable convolutions with hierarchical receptive fields (i.e., 1\u00d71, 3\u00d73, 7\u00d77) and an adaptive average pooling layer to extract features with RFs of various scales.</p> </li> <li> <p>These features extracted by different RFs are then concatenated as \\(F_i^{\\theta}\\), followed by a 1\u00d71 convolution layer and a batch normalization layer to generate the output feature of RM \\(F_i^{d'}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#bilateral-reference","title":"Bilateral Reference","text":"<ul> <li> <p>inward reference(InRef) and an outward reference (OutRef)</p> </li> <li> <p>In InRef, images \\(I\\) with original high resolution are cropped to patches \\(\\{P_{k=1}^N\\}\\) of consistent size with the output features of the corresponding decoder stage.</p> </li> <li> <p>These patches are stacked with the original feature \\(F_i^{d+}\\) to be fed into the RM.</p> </li> <li> <p>In OutRef, we use gradient labels to draw more attention to areas of richer gradient information which is essential for the segmentation of fine structures.</p> </li> <li> <p>First, we extract the gradient maps of the input images as \\(G_i^{gt}\\). Meanwhile, \\(F_i^{\\theta}\\) is used to generate the feature \\(F_i^G\\) to produce the predicted gradient maps \\(\\hat{G}^i\\)</p> </li> <li> <p>It passes through a conv and a sigmoid layer and is used to generate the gradient referring attention \\(A_i^G\\), which is then multiplied by \\(F_i^{d'}\\) to generate output of the BiRef block as \\(F_{i\u22121}^{d}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#loss","title":"Loss","text":"<p>\\(L = L_{pixel} + L_{region} + L_{boundary} + L_{semantic} \\\\ = \\lambda_1 L_{BCE} + \\lambda_2 L_{IoU} + \\lambda_3 L_{SSIM} + \\lambda_4 L_{CE}\\)</p> <p>\\(L_{BCE} = -\\sum_{(i,j)} [G(i,j) \\log(M(i,j)) + (1-G(i,j)) \\log(1-M(i,j))]\\)</p> <p>\\(L_{IoU} = 1 - \\frac{\\sum_{r=1}^H \\sum_{c=1}^W M(i,j)G(i,j)}{\\sum_{r=1}^H \\sum_{c=1}^W [M(i,j)+G(i,j)-M(i,j)G(i,j)]}\\)</p> <p>\\(L_{SSIM} = 1 - \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\\)</p> <p>\\(L_{CE} = -\\sum_{c=1}^N y_{o,c}\\log(p_{o,c})\\)</p>"},{"location":"cv/segmentation/DIS/","title":"DIS","text":""},{"location":"cv/segmentation/DIS/#dis","title":"DIS","text":"<p>Dichotomous Image Segmentation (DIS) proposed IS-Net. IS-Net as 3 components</p> <ol> <li>ground truth (GT) encoder,</li> <li>image segmentation component (\\(U^2\\)-Net with an input convolution layer before its first encoder stage.)</li> <li>intermediate supervision strategy</li> </ol>"},{"location":"cv/segmentation/DIS/#1st-stage","title":"1st Stage","text":"<p>Self-supervised training of the GT-encoder</p> <p>\\(L_{gt} = \\sum_{d=1}^{D} \\lambda_{d}^{gt} BCE(F_{gt}(\\theta_{gt}, G)_d, G)\\)</p> <p>GT encoder will be frozen.</p>"},{"location":"cv/segmentation/DIS/#2nd-stage","title":"2nd Stage","text":"<p>High dimensional intermediate features</p> <p>\\(f_{D}^{G} = F_{gt}^{-}(\\theta_{gt}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <ul> <li> <p>\\(F_{gt}^{-}\\) represents the \\(F_{gt}\\)  without the last convolution layers for generating the probability maps.</p> </li> <li> <p>\\(F_{gt}^{-}\\) is to supervise those corresponding features \\(f_{D}^{I}\\) from the segmentation model \\(F_{sg}\\)</p> </li> </ul> <p>High dimensional intermediate features from image segmentation component</p> <p>\\(f_{I}^{G} = F_{sg}^{-}(\\theta_{sg}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <p>Feature Consistency Loss (intermediate supervision)</p> <p>\\(L_{fs} = \\sum_{d=1}^{D} \\lambda_{d}^{fs} ||f_{d}^{I} - f_{d}^{G}||^2\\)</p> <p>\\(L_{sg} = \\sum_{d=1}^{D} \\lambda_{d}^{sg} BCE(F_{sg}(\\theta_{sg}, I), G)\\)</p> <p>Loss for \\(F_{sg}\\) is  \\(L = L_{fs} + L_{sg}\\)</p>  IS-Net"},{"location":"cv/segmentation/DIS/#results","title":"Results","text":"Result"},{"location":"cv/segmentation/DIS/#meric-human-correction-efforts-hce","title":"Meric : Human Correction Efforts (HCE)","text":""},{"location":"ml/","title":"Machine Learning","text":""},{"location":"nlp/","title":"NLP","text":""}]}