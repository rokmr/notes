{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#notes","title":"Notes","text":""},{"location":"#explore-and-star-on-github","title":"Explore and star on GitHub","text":"<ul> <li> <p> rokmr/kick-start-ai</p> </li> <li> <p> rokmr/machine-learning</p> </li> <li> <p> rokmr/computer-vision</p> </li> <li> <p> rokmr/notes</p> </li> </ul>"},{"location":"LLM/","title":"A2A","text":""},{"location":"LLM/LLM/","title":"A2A","text":""},{"location":"LLM/LangChain/","title":"A2A","text":""},{"location":"LLM/Qdrant/","title":"A2A","text":""},{"location":"LLM/RAG/","title":"A2A","text":""},{"location":"agents/","title":"A2A","text":""},{"location":"agents/A2A/","title":"A2A","text":""},{"location":"agents/LangGraph/","title":"A2A","text":""},{"location":"agents/MCP/","title":"A2A","text":""},{"location":"basics/","title":"Basic","text":""},{"location":"basics/#basics","title":"Basics","text":""},{"location":"basics/#convex-and-concave-function","title":"Convex and Concave function","text":"<p>Convex Functions</p> <p>A function \\(f(x)\\) is convex if the line segment between any two points on the graph lies above or on the graph.</p> <p>For all \\(x_1, x_2\\) in the domain and \\(t \\in [0,1]\\):</p> \\[f(tx_1 + (1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2)\\] <p>The graph <code>curves upward</code></p> <p>Note</p> <ul> <li>Sum of convex functions is convex</li> <li>\\(f(E[X]) \\leq E[f(X)]\\)</li> <li>f is convex \u27fa f''(x) \u2265 0</li> <li>\\(g(f(x))\\) is convex if:<ul> <li>\\(g\\) is convex and non-decreasing, \\(f\\) is convex</li> <li>\\(g\\) is convex and non-increasing, \\(f\\) is concave</li> </ul> </li> <li>Examples: \\(x^2\\), \\(e^x\\), \\(|x|\\)</li> </ul> <p>Concave Functions</p> <p>A function \\(f(x)\\) is concave if the line segment between any two points on the graph lies below or on the graph.</p> <p>For all \\(x_1, x_2\\) in the domain and \\(t \\in [0,1]\\):</p> \\[f(tx_1 + (1-t)x_2) \\geq tf(x_1) + (1-t)f(x_2)\\] <p>The graph <code>curves downward</code></p> <p>Note</p> <ul> <li>Sum of concave functions is concave</li> <li>\\(f(E[X]) \\geq E[f(X)]\\)</li> <li>f is concave \u27fa f''(x) \u2264 0</li> <li>Examples: \\(\\log(x)\\), \\(\\sqrt{x}\\), \\(-x^2\\)</li> </ul>"},{"location":"basics/#parametric-and-non-parametric-model","title":"Parametric and Non-parametric model","text":"<p>Parametric</p> <ul> <li>Makes strong assumptions about the data distribution</li> <li>Has a fixed number of parameters</li> <li>Linear Regression, Logistic Regression, Neural Networks, LDA</li> </ul> <p>Non-parametric Model</p> <ul> <li>Makes fewer assumptions about data distribution</li> <li>Number of parameters grows with training data</li> <li>KNN, Decision Trees, Random Forests, SVM, Gaussian Processes</li> </ul>"},{"location":"basics/#generative-and-non-generative-model","title":"Generative and Non-generative model","text":"<p>Generative Model</p> <ul> <li>\\(P(Y|X) = P(X|Y)P(Y)/P(X)\\)</li> <li>\\(Y^{\\star} = argmax[P(X|Y)P(Y)]\\)<ul> <li>\\(X\\) : feature </li> <li>\\(Y\\) : class label</li> <li>\\(P(X|Y)\\) : Likelihood</li> </ul> </li> <li>Naive Bayes, GMM, LDA, GANs, HMM</li> </ul> <p>Non-generative Model</p> <ul> <li>Directly models \\(P(Y|X)\\)</li> <li>\\(Y^{\\star} = argmax[P(Y|X)]\\)</li> <li>Logistic Regression, SVM, Decision Tree, Random Forest, Neural Network </li> </ul>"},{"location":"basics/Activation/","title":"Activation","text":""},{"location":"basics/Activation/#activation","title":"Activation","text":""},{"location":"basics/Activation/#sigmoid","title":"Sigmoid","text":"<ul> <li> <p>\\(f(x) = \\frac{1}{1 + e^{-x}} = \\sigma(x)\\)</p> </li> <li> <p>\\(f'(x) = f(x)(1 - f(x)) = \\sigma(x)(1 - \\sigma(x))\\)</p> </li> </ul>"},{"location":"basics/Activation/#tanh","title":"Tanh","text":"<ul> <li> <p>\\(f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)</p> </li> <li> <p>\\(f'(x) = 1 - \\tanh^2(x) = 1 - f^2(x)\\)</p> </li> </ul>"},{"location":"basics/Activation/#relu","title":"ReLU","text":"<ul> <li> <p>\\(f(x) = \\max(0, x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul>"},{"location":"basics/Activation/#leaky-relu","title":"Leaky ReLU","text":"<ul> <li> <p>\\(f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ \\alpha &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul>"},{"location":"basics/Activation/#elu-exponential-linear-unit","title":"ELU (Exponential Linear Unit)","text":"<ul> <li> <p>\\(f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha(e^x - 1) &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ \\alpha e^x &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul>"},{"location":"basics/Activation/#swish","title":"Swish","text":"<ul> <li> <p>\\(f(x) = x \\cdot \\sigma(x)\\)</p> </li> <li> <p>\\(f'(x) = f(x) + \\sigma(x)(1 - f(x))\\)</p> </li> </ul>"},{"location":"basics/Activation/#prelu-parametric-relu","title":"PReLU (Parametric ReLU)","text":"<ul> <li> <p>\\(f(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> <li> <p>\\(f'(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ \\alpha &amp; \\text{if } x \\leq 0 \\end{cases}\\)</p> </li> </ul> <p>\\(\\alpha :\\) Learnable Parameter</p>"},{"location":"basics/Activation/#gelu","title":"GeLU","text":"<ul> <li> <p>\\(f(x) = x \\cdot \\Phi(x)\\)</p> </li> <li> <p>\\(f'(x) = \\Phi(x) + x\\Phi(x)\\)</p> </li> <li> <p>\\(\\Phi(x) : \\text{Cumulative Distribution Function of Gaussian}\\)</p> </li> <li> <p>\\(\\Phi(x) = \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right] = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-\\frac{t^2}{2}}dt\\)</p> </li> <li> <p>\\(\\Phi(x) = 0.5 * (1 + \\tanh(\\sqrt{\\frac{2}{\\pi}} * (x + 0.044715 * x^3)))\\)</p> </li> </ul>"},{"location":"basics/Activation/#linear","title":"Linear","text":"<ul> <li> <p>\\(f(x) = x\\)</p> </li> <li> <p>\\(f'(x) = 1\\)</p> </li> </ul>"},{"location":"basics/Attention/","title":"Attention","text":""},{"location":"basics/BiasVar/","title":"Bias & Variance","text":""},{"location":"basics/BiasVar/#bias-variance-tradeoff","title":"Bias-Variance tradeoff","text":"<ul> <li>Let \\(f(x)\\) be true model and \\(\\hat{f}(x)\\) be estimate of our model</li> </ul>"},{"location":"basics/BiasVar/#bias","title":"Bias","text":"<ul> <li>Bias measures the difference between the model's average prediction and the true value</li> <li>Bias(\\(\\hat{f}(x)\\)) = \\(E[\\hat{f}(x)] - f(x)\\)</li> <li>\\(E[\\hat{f}(x)]:\\) Average value of the model.</li> <li>For simple models, the average predicted value is often very far from the true value</li> <li>Simple models have very high bias &amp; complex models have very low bias</li> <li>Simple model has very high bias &amp; complex model has very low bias.</li> </ul>"},{"location":"basics/BiasVar/#variance","title":"Variance","text":"<ul> <li>Variance measures the model's sensitivity to fluctuations in the training set</li> <li>Variance(\\(\\hat{f}(x)\\)) = \\(E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]\\)</li> <li>Measures how much the predictions vary from one training set to another</li> <li>Simple models have low variance as they are more stable</li> <li>Complex models have high variance as they are more sensitive to training data</li> </ul> <p>Note</p> <ul> <li>Simple Model: high bias, low variance, underfitting</li> <li>Complex Model: low bias, high variance, overfitting</li> </ul>"},{"location":"basics/BiasVar/#trade-off","title":"Trade-off","text":"<ul> <li>The Bias-variance tradeoff is the relationship between bias and variance as you try to minimize each but by minimizing one the other increases, so there is  inherently a tradeoff. </li> <li>\\(E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{variance} + \\sigma \\text{(irreducible error)}\\)</li> </ul>"},{"location":"basics/BiasVar/#case-of-underfitting","title":"Case of underfitting","text":"<ol> <li>High loss for training and high for the test.</li> <li>Low accuracy for training and low for the test.</li> </ol>"},{"location":"basics/BiasVar/#prevention","title":"Prevention","text":"<ul> <li>More complex models</li> <li>Reducing regularization</li> <li>Increase training time</li> </ul>"},{"location":"basics/BiasVar/#case-of-overfitting","title":"Case of overfitting","text":"<ol> <li>Low loss for training and high for the test.</li> <li>Low accuracy for training and high for the test.</li> </ol>"},{"location":"basics/BiasVar/#prevention_1","title":"Prevention","text":"<ul> <li>Regularization</li> <li>Train with more data</li> <li>Data augmentation</li> <li>K-fold cross-validation</li> <li>Reduce the number of feature</li> <li>Change the model</li> </ul>"},{"location":"basics/Distance/","title":"Distance","text":""},{"location":"basics/Distance/#distance","title":"Distance","text":""},{"location":"basics/Distance/#euclidean-distance","title":"Euclidean Distance","text":"<ul> <li>Straight-line distance between two points</li> <li>Formula: \\(d(p,q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}\\)</li> </ul>"},{"location":"basics/Distance/#manhattan-distance","title":"Manhattan Distance","text":"<ul> <li>Sum of absolute differences between coordinates</li> <li>Formula: \\(d(p,q) = \\sum_{i=1}^{n}|p_i - q_i|\\)</li> </ul>"},{"location":"basics/Distance/#cosine-similarity","title":"Cosine Similarity","text":"<ul> <li>Measures cosine of angle between two vectors</li> <li>Range: [-1, 1] where 1 means vectors are identical</li> <li>Formula: \\(cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\\)</li> <li>Often converted to distance: \\(d(p,q) = 1 - cos(\\theta)\\)</li> </ul>"},{"location":"basics/Extra/","title":"Extra","text":""},{"location":"basics/Extra/#extra","title":"Extra","text":""},{"location":"basics/Extra/#handling-data-imbalance","title":"Handling Data Imbalance","text":"<ul> <li>over-sampling: sample the minority class significantly more such that the dataset is more balanced </li> <li>under-sampling: sample the majority class significantly less such that the dataset is more balanced </li> <li>SMOTE (synthetic minority oversampling technique) creates examples of the minority class by randomly sampling feature values from the current features </li> <li>Use another ML model: tree-based models perform well on imbalanced datasets</li> </ul>"},{"location":"basics/Extra/#residual-loss","title":"Residual Loss","text":""},{"location":"basics/Extra/#ssim-loss","title":"SSIM Loss","text":""},{"location":"basics/Extra/#perceptual-loss","title":"Perceptual Loss","text":""},{"location":"basics/Extra/#style-loss","title":"Style Loss","text":""},{"location":"basics/Extra/#content-loss","title":"Content Loss","text":""},{"location":"basics/Extra/#iou-loss","title":"IoU Loss","text":""},{"location":"basics/Extra/#negative-log-likelihood","title":"Negative Log Likelihood","text":""},{"location":"basics/FeatureEngg/","title":"Feature Engineering","text":""},{"location":"basics/FeatureEngg/#feature-engineering","title":"Feature Engineering","text":""},{"location":"basics/FeatureEngg/#feature-selection","title":"Feature Selection","text":"<ul> <li>Find a small subset of features that has best correlation with class labels.</li> <li>Transform the original feature vector into a new feature vector to improve the features (e.g., make them uncorrelated).</li> </ul>"},{"location":"basics/Inference/","title":"Inference","text":""},{"location":"basics/Inference/#inference","title":"Inference","text":""},{"location":"basics/Inference/#latency","title":"Latency","text":""},{"location":"basics/Inference/#bandwidth","title":"Bandwidth","text":""},{"location":"basics/Inference/#throughput","title":"Throughput","text":""},{"location":"basics/Inference/#model-compression","title":"Model Compression","text":""},{"location":"basics/Inference/#quantization","title":"Quantization","text":""},{"location":"basics/Inference/#pruning","title":"Pruning","text":""},{"location":"basics/Inference/#distillation","title":"Distillation","text":""},{"location":"basics/InfoTheory/","title":"Information Theory","text":""},{"location":"basics/InfoTheory/#information-theory","title":"Information Theory","text":""},{"location":"basics/InfoTheory/#entropy","title":"Entropy","text":"<p>\\(H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i)\\)</p>"},{"location":"basics/InfoTheory/#information-gain","title":"Information Gain","text":"<p>\\(IG(T,a) = H(T) - \\sum_{v \\in values(a)} \\frac{|T_v|}{|T|} H(T_v)\\)</p> <ul> <li>\\(IG(T,a)\\): Information Gain of dataset \\(T\\) when split on attribute \\(a\\)</li> <li>\\(H(T)\\): Entropy of the parent dataset</li> <li>\\(\\sum_{v \\in values(a)}\\): Sum over all possible values of attribute \\(a\\)</li> <li>\\(|T_v|\\) is the number of examples in subset \\(v\\)</li> <li>\\(|T|\\): is the total number of examples in the parent dataset</li> </ul>"},{"location":"basics/InfoTheory/#gini-impurity","title":"Gini Impurity","text":"<p>\\(Gini(T) = 1 - \\sum_{i=1}^{c} (p_i)^2\\)</p> <ul> <li>\\(c\\): Number of classes/categories in the dataset</li> <li>\\(p_i\\): Probability (proportion) of data points belonging to class \\(i\\)</li> </ul>"},{"location":"basics/InfoTheory/#cross-entropy","title":"Cross Entropy","text":"<p>\\(H(p,q) = -\\sum_{x} p(x) \\log q(x)\\)</p>"},{"location":"basics/InfoTheory/#kl-divergence","title":"KL-Divergence","text":"<p>\\(D_{KL}(P||Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}\\)</p>"},{"location":"basics/Loss/","title":"Loss","text":""},{"location":"basics/Loss/#loss","title":"Loss","text":""},{"location":"basics/Loss/#mean-square-loss","title":"Mean Square Loss","text":"<ul> <li><code>Regression problems</code></li> <li>\\(L_{MSE} = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2\\)</li> <li>Properties<ul> <li>Sensitive to outliers</li> </ul> </li> </ul>"},{"location":"basics/Loss/#binary-cross-entropy-loss","title":"Binary Cross Entropy Loss","text":"<ul> <li><code>Binary Classification</code></li> <li>\\(L_{BCE} = -\\sum_i y_i \\log(\\hat{y}_{i}) + (1 - y_i) \\log(1 - \\hat{y}_{i})\\)</li> <li>Properties<ul> <li>Penalizes confident wrong predictions heavily</li> </ul> </li> </ul>"},{"location":"basics/Loss/#cross-entropy-loss","title":"Cross Entropy Loss","text":"<ul> <li><code>Multi-class Classification</code></li> <li>\\(L_{CE} = -\\sum_i y_i \\log(\\hat{y}_i)\\)</li> <li>Properties<ul> <li>Good for mutually exclusive classes</li> <li>Penalizes wrong predictions proportionally</li> </ul> </li> </ul>"},{"location":"basics/Loss/#hinge-loss","title":"Hinge Loss","text":"<ul> <li><code>SVM Classification</code></li> <li>\\(L_{hinge}=\\max(0, 1-\\hat{y} \\cdot y)\\)</li> <li>Properties<ul> <li>Robust to outliers</li> <li>Used in maximum-margin classification</li> </ul> </li> </ul>"},{"location":"basics/Loss/#focal-loss","title":"Focal Loss","text":"<ul> <li><code>Object Detection/Imbalanced Classification</code></li> <li>\\(L_{focal} = - (1 - p_t)^{\\gamma} \\log(p_t)\\)</li> <li>\\(L_{CE} = - \\log(p_t)\\)</li> <li>Properties<ul> <li>\\(\\gamma\\) controls focus on hard examples</li> </ul> </li> </ul>"},{"location":"basics/Loss/#triplet-loss","title":"Triplet Loss","text":"<ul> <li><code>Similarity Learning/Embedding Learning</code></li> <li>\\(L_{triplet} = \\sum_i^N [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \\alpha]_{+}\\)</li> <li>Proeprties<ul> <li>Requires triplets (anchor, positive, negative)</li> <li>\\(\\alpha\\) : margin that is enforced between positive and negative pairs</li> <li>Used in face recognition</li> </ul> </li> </ul>"},{"location":"basics/Loss/#kl-divergence-loss","title":"KL Divergence Loss","text":"<ul> <li><code>Distribution Learning</code></li> <li>\\(KL(P||Q)=\\sum P(x) \\log \\frac{P(x)}{Q(x)}\\)</li> <li>Properties<ul> <li>Not symmetric</li> <li>Used in VAE</li> <li>Useful for probability distribution matching</li> </ul> </li> </ul>"},{"location":"basics/Loss/#contrastive-loss","title":"Contrastive Loss","text":"<ul> <li><code>Self-supervised/Multi-modal Learning</code></li> <li>\\(l_i^{(u \\rightarrow v)} = - \\log \\frac{\\exp(sim(u_i,v_i)/\\tau)}{\\sum_{k=1}^N \\exp(sim(u_i, v_k)/ \\tau)}\\)</li> <li>\\(L_{contrast} = \\frac{1}{N} \\sum_{i=1}^N (\\lambda l_i^{(v \\rightarrow u)} + (1-\\lambda) l_i^{(u \\rightarrow v)})\\)</li> <li>\\(\\lambda \\in [0,1]\\)</li> <li>Properties<ul> <li>Pulls similar pairs together</li> <li>Pushes dissimilar pairs apart</li> <li>Used in CLIP, SimCLR</li> </ul> </li> </ul>"},{"location":"basics/Loss/#question","title":"Question","text":"<ul> <li>Explain residual loss and how it can be generalized for training multiple hypotheses.</li> </ul> Draw graph of cross entropy."},{"location":"basics/MLEMAE/","title":"MAE &amp; MAE","text":""},{"location":"basics/MLEMAE/#mae-mae","title":"MAE &amp; MAE","text":""},{"location":"basics/MLEMAE/#what-is-mle-estimate-prior-and-map-estimate","title":"what is MLE estimate, prior and MAP estimate ?","text":""},{"location":"basics/Metric/","title":"Metric","text":""},{"location":"basics/Metric/#metric","title":"Metric","text":""},{"location":"basics/Metric/#basics","title":"Basics","text":"<ul> <li>True Positives (TP): actual observation is 1 (True) and prediction is 1 (True) </li> <li>True Negative (TN): actual observation is 0 (False) and prediction is 0 (False) </li> <li>False Positive (FP): actual observation is 0 (False) and prediction is 1 (True) </li> <li>False Negative (FN): actual observation is 1 (True) and prediction is 0 (False) </li> </ul>"},{"location":"basics/Metric/#accuracy","title":"Accuracy","text":"<p><code>Balanced</code></p> <p>\\(\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\)</p>"},{"location":"basics/Metric/#recall","title":"Recall","text":"<p><code>Balanced, Imbalanced</code></p> <p>Out of all the True <code>observations</code> how many were actually True. \\(Recall = \\frac{TP}{TP+FN}\\)</p> <ul> <li>When false positives are very costly</li> </ul> <p>Example</p> <ul> <li>Cancer Detection</li> <li>Criminal Detection</li> <li>Manufacturing Quality Control (catch all defects)</li> </ul>"},{"location":"basics/Metric/#precision","title":"Precision","text":"<p><code>Balanced, Imbalanced</code></p> <p>Out of all the True <code>predictions</code> how many were actually True.</p> <p>\\(Precision = \\frac{TP}{TP+FP}\\)</p> <p>Example</p> <ul> <li>Email Spam Filter (avoiding marking important emails as spam)</li> <li>Medical diagnostics  (avoiding unnecessary treatments)</li> <li>Fraud detection (avoiding blocking legitimate transactions)</li> </ul> <p>Info</p> <ul> <li>High threshold lead to high precision while recall decreases</li> </ul>"},{"location":"basics/Metric/#pr-curve","title":"PR Curve","text":""},{"location":"basics/Metric/#f1-score","title":"F1-score","text":"<p><code>Balanced, Imbalanced</code></p> <p>Harmonic Mean of Precision(P) and Recall(R)</p> <p>\\(F1-score = \\frac{2*PR}{P+R}\\)</p>"},{"location":"basics/Metric/#micro-f1-vs-macro-f1","title":"Micro F1 Vs macro F1","text":""},{"location":"basics/Metric/#mean-squared-error","title":"Mean Squared Error","text":""},{"location":"basics/Metric/#r2-score","title":"R2 Score","text":"<p>\\(R^2\\) Score</p> <ul> <li>range : [0, 1]</li> <li>Having 1 is good model and 0 is bad model </li> </ul>"},{"location":"basics/Metric/#receiver-operating-characteristic-roc","title":"Receiver Operating Characteristic (ROC)","text":"<p>The ROC is a probability curve and the area under  the curve can be thought of as the degree of separability between the two classes. </p>"},{"location":"basics/Metric/#pixel-accuracy","title":"Pixel Accuracy","text":"<p>A metric that calculates the percentage of correctly classified pixels in an image.</p> <p>\\(Pixel Accuracy = \\frac{Number\\ of\\ Correctly\\ Classified\\ Pixels}{Total\\ Number\\ of\\ Pixels}\\)</p> <p>Limitation: Can be misleading for imbalanced datasets. For example, if an image has only 2 pixels of interest out of 100 pixels, predicting no objects would still result in 98% accuracy.</p>"},{"location":"basics/Metric/#intersection-over-union-iou","title":"Intersection over Union (IoU)","text":"<p>A more robust metric that measures the overlap between predicted and ground truth segments.</p> <p>\\(IoU = \\frac{Area\\ of\\ Overlap}{Area\\ of\\ Union} = \\frac{Intersection}{Union}\\)</p> <p>Where,</p> <ul> <li>Intersection = Area shared between prediction and ground truth</li> <li>Union = Total area covered by both prediction and ground truth</li> </ul> <p>If two bbox ahve both Intersection and union high then the IoU will be high. Assume two case :</p> <ol> <li>Intersion is large but the bbox are also very large bbox this leads to very high union depicting same same object.</li> <li>Intersion is large but the bbox are small bbox then possibly they are dipicting different object.</li> </ol>"},{"location":"basics/Metric/#dice-coefficient","title":"Dice Coefficient","text":"<p>Similar to IoU but less harsh on misclassifications. It has a monotonic relationship with IoU.</p> <p>\\(Dice = \\frac{2 \\times Area\\ of\\ Overlap}{Sum\\ of\\ Areas} = \\frac{2 \\times Intersection}{Area\\ of\\ Prediction + Area\\ of\\ Ground\\ Truth}\\)</p> <p>Relationship with IoU: \\(Dice = \\frac{2 \\times IoU}{1 + IoU}\\)</p>"},{"location":"basics/Metric/#non-maxium-supression-nms","title":"Non-Maxium Supression (NMS)","text":"<p>We  don't need all the object proposals. We only want to keep the best one.</p>"},{"location":"basics/Metric/#nms","title":"\u03bb<sub>NMS</sub>","text":"<p>Do not allow the bbos if they are overlapping more than \\(\\lambda_{NMS}\\) threshold.</p> <ul> <li>Narrrow Threshold (High IoU) : Low Precision (More False Positive)</li> <li>Wide Threshold (Low IoU): Low Recall (More False Negative)</li> </ul>"},{"location":"basics/Metric/#question","title":"Question","text":"Metrics for multi-class classification? <ul> <li>Confusion Matrix</li> <li>Multi-class ROC-AUC<ul> <li>One-vs-Rest</li> <li>One-vs-One</li> </ul> </li> </ul>"},{"location":"basics/Optimization/","title":"Optimization","text":""},{"location":"basics/Optimization/#optimization","title":"Optimization","text":""},{"location":"basics/Optimization/#sgd","title":"SGD","text":"<p>\\(w_{t+1} = w_t - \\eta \\nabla w_t\\)</p>"},{"location":"basics/Optimization/#momentum-based-gd","title":"Momentum Based GD","text":"<p>\\(update_t = \\gamma \\cdot update_{t-1} + \\eta \\nabla w_t\\)</p> <p>\\(w_{t+1} = w_t - update_t\\)</p>"},{"location":"basics/Optimization/#nesterov-accelerated-gd","title":"Nesterov Accelerated GD","text":"<p>\\(update_t = \\gamma \\cdot update_{t-1} + \\eta \\nabla (w_t- \\gamma \\cdot update_{t-1})\\)</p> <p>\\(w_{t+1} = w_t - update_t\\)</p>"},{"location":"basics/Optimization/#adagrad","title":"AdaGrad","text":"<p>\\(v_t = v_{t-1} + (\\nabla w_t)^2\\)</p> <p>\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon }}* \\nabla w_t\\)</p> <p>effective_lr = initial_lr / sqrt(accumulated_squared_gradients + eps)</p>"},{"location":"basics/Optimization/#rmsprop","title":"RMSProp","text":"<p>\\(v_t = \\alpha * v_{t-1} + (1- \\alpha) *(\\nabla w_t)^2\\)</p> <p>\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon }}* \\nabla w_t\\)</p>"},{"location":"basics/Optimization/#adam","title":"Adam","text":"<p>\\(m_t = \\beta_1 * v_{t-1} + (1- \\beta_1) * \\nabla w_t\\)</p> <p>\\(v_t = \\beta_2 * v_{t-1} + (1- \\beta_2) *(\\nabla w_t)^2\\)</p> <p>Bias Correction</p> <p>\\(\\hat{m_t} = \\frac{m_t}{1-\\beta_1^t}\\)</p> <p>\\(\\hat{v_t} = \\frac{v_t}{1-\\beta_2^t}\\)</p> <p>\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v_t} + \\epsilon }} * \\nabla \\hat{m_t}\\)</p>"},{"location":"basics/Optimization/#question","title":"Question","text":"<ul> <li>Derive the backpropagation equations for a 3-layer neural network with the following specifications:<ul> <li>Each layer has exactly one neuron</li> <li>Network structure: Input \u2192 Hidden Layer 1 (1 neuron) \u2192 Hidden Layer 2 (1 neuron) \u2192 Output Layer (1 neuron)</li> </ul> </li> </ul> Despite using single samples for gradient estimation, how does Stochastic Gradient Descent (SGD) manage to converge? <ul> <li>Noisy but unbiased estimate of true gradient</li> <li>\\(E[\\nabla L_i] = \\nabla L\\)</li> <li>Converges in expectation</li> </ul>"},{"location":"basics/PositionEncoding/","title":"Position Encoding","text":""},{"location":"basics/Probability/","title":"Probability","text":""},{"location":"basics/Probability/#probability","title":"Probability","text":""},{"location":"basics/Probability/#notations","title":"Notations","text":"<p>\\(\\Omega\\) : Sample Space</p> <p>\\(\\mathcal{F}\\) : set of all possible event</p> <p>\\(P\\) : Probability</p>"},{"location":"basics/Probability/#basics","title":"Basics","text":""},{"location":"basics/Probability/#axioms-and-properties","title":"Axioms and Properties","text":"<p>Axioms</p> <p>\\(P : \\mathcal{F} \\rightarrow R\\) satisfying</p> <ul> <li>Non-negativity : \\(P(A)\\) \u2265 0, \\(\\forall A \\in \\mathcal{F}\\)</li> <li>Normalization: \\(P(\\Omega) =1\\)</li> <li>\\(\\sigma\\)-additivity: If \\(A_i \\cap A_j = \\phi\\), \\(\\forall i \\neq j\\) then \\(P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty}P(A_i)\\)</li> </ul> <p>Properties</p> <p>If \\(A \\subseteq B \\Longrightarrow P(A) \\le P(B).\\)</p> <ul> <li> <p>\\(P(A \\cap B) \\le min(P(A), P(B))\\)</p> </li> <li> <p>\\(P(A \\cup B) \\le P(A) + P(B)\\)</p> </li> <li> <p>\\(P( \\Omega / A) = 1- P(A)\\)</p> </li> </ul> <p>If \\(A_1, \\cdots, A_k\\) are a set of disjoint events such that \\(\\cup_{i=1}^{k} A_i = \\Omega\\), then \\(\\sum_{i=1}^{k} P(A_k) =1.\\)</p>"},{"location":"basics/Probability/#independent","title":"Independent","text":"<ul> <li> <p>\\(P(AB) = P(A) P(B)\\)</p> </li> <li> <p>\\(P(ABC) = P(A) P(B) P(C)\\)</p> </li> </ul>"},{"location":"basics/Probability/#conditional-independence","title":"Conditional Independence","text":"<ul> <li>\\(P(AB|C) = P(A|C)P(B|C)\\)</li> </ul> <p>Event may be conditionally independent but not independent.</p>"},{"location":"basics/Probability/#chain-rule","title":"Chain Rule","text":"\\[f(x_1, x_2, \\cdots , x_n) = f(x_1)f(x_2|x_1) f(x_3|x_1, x_2) \\cdots f(x_n| x_1,x_2, \\cdots , x_{n-1}) = f(x_1) \\Pi_{i=2}^{n} f(x_i | x_1, \\cdots , x_{i-1})\\]"},{"location":"basics/Probability/#conditional-probability","title":"Conditional Probability","text":"<ul> <li> <p>\\(P(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(AB)}{P(B)}\\)</p> </li> <li> <p>\\(P(AB) = P(A|B) P(B)\\)</p> </li> <li> <p>\\(P(A|B,C) = P(A|BC) = \\frac{P(ABC)}{P(BC)}\\)</p> </li> </ul>"},{"location":"basics/Probability/#total-probability-rule","title":"Total probability rule","text":"<ul> <li>\\(P(A) = \\sum_{i} P(A|B_i)P(B_i)\\)</li> </ul>"},{"location":"basics/Probability/#bayes-rule","title":"Bayes Rule","text":"<ul> <li>\\(P(A|B) = \\frac{P(AB)}{P(B)}= \\frac{P(B|A)P(A)}{P(B)}\\)</li> </ul>"},{"location":"basics/Probability/#random-variable","title":"Random Variable","text":"<p>\\(X:\\Omega \\rightarrow R\\), here \\(X(\\omega)\\) is denoted as \\(X\\).</p>"},{"location":"basics/Probability/#cdf","title":"CDF","text":"<p>\\(F_X : R \\rightarrow [0,1]\\)</p> <p>\\(F_X(x) = P(X &lt; x)\\)</p> <p>Properties</p> <ul> <li> <p>\\(0 \\le F_X(x) \\le 1.\\)</p> </li> <li> <p>\\(\\lim_{x \\to  - \\infty} F_X(x) = 0\\)</p> </li> <li> <p>\\(\\lim_{x \\to  \\infty} F_X(x) = 1\\)</p> </li> <li> <p>\\(x \\le y \\Longrightarrow  F_X(x) \\le F_X(y).\\)</p> </li> </ul>"},{"location":"basics/Probability/#pmf","title":"PMF","text":"<p>\\(X\\) is a discrete random variable</p> <p>\\(p_X : \\Omega \\rightarrow R\\)</p> <p>\\(p_X(x) = P(X=x)\\)</p> <p>\\(Val(X):\\) set of possible values of random variable \\(X\\).</p> <p>Properties:</p> <ul> <li> <p>\\(0 \\le p_X(x) \\le 1\\)</p> </li> <li> <p>\\(\\sum_{x \\in Val(X)} p_X(x) = 1\\)</p> </li> <li> <p>\\(\\sum_{x \\in A} p_X(x) = P(X \\in A)\\)</p> </li> </ul>"},{"location":"basics/Probability/#pdf","title":"PDF","text":"<p>For some continuous random variable, \\(F_x(x)\\) is differentiable everywhere. Then PDF can be defined as.</p> <p>\\(f_X(x) = \\frac{dF_X(x)}{dx}\\)</p> <p>So, PDF for continuous random variable may not always exits.</p> <p>PDF at any given point x is not the probability of that event, i.e., \\(f_X(x) \\neq P(X=x)\\) as it can take value larger than 1.</p> <p>Properties:</p> <ul> <li> <p>\\(f_X(x) \\ge 1\\)</p> </li> <li> <p>\\(\\int_{-\\infty}^{\\infty} f_X(x) =1\\)</p> </li> <li> <p>\\(\\int_{x \\in A} f_X(x) dx = P(X =A)\\)</p> </li> </ul>"},{"location":"basics/Probability/#expectation","title":"Expectation","text":"<ul> <li> <p>\\(E[g(X)] = \\sum_{x \\in Val(X)} g(x) p_X(x)\\), \\(X\\) is discrete random variable</p> </li> <li> <p>\\(E[g(X)] = \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx\\), \\(X\\) is continuous random variable</p> </li> </ul> <p>Properties</p> <ul> <li> <p>\\(E[a] = a\\)</p> </li> <li> <p>\\(E[af(X)]=aE[f(X)]\\), for any constant \\(a \\in R\\)</p> </li> <li> <p>\\(E[f(X) + g(X)] = E[f(X)] + E[g(X)]\\)</p> </li> </ul> <p>For discrete random variable \\(X, E[1\\{X=k\\}] = P(X=k)\\)</p>"},{"location":"basics/Probability/#variance","title":"Variance","text":"<ul> <li> <p>\\(Var(X) = E[(X - E(X))^2]\\)</p> </li> <li> <p>\\(E[(X - E(X))^2] = E[X^2] - E[X]^2\\)</p> </li> </ul> <p>Properties</p> <ul> <li> <p>\\(Var[a] = 0\\), for any constant \\(a \\in R\\).</p> </li> <li> <p>\\(Var[af(x)] = a^2 Var[f(x)]\\)</p> </li> </ul>"},{"location":"basics/Probability/#compilation","title":"Compilation","text":""},{"location":"basics/Probability/#two-random-variable","title":"Two Random Variable","text":""},{"location":"basics/Probability/#cdf_1","title":"CDF","text":"<ul> <li>\\(F_{XY}(x, y) = P(X \\le x, Y \\le y)\\)</li> </ul> <p>Marginal CDF</p> <ul> <li> <p>\\(F_X(x) = \\lim_{y \\to \\infty}F_{XY}(x, y) dy\\)</p> </li> <li> <p>\\(F_Y(y) = \\lim_{x \\to \\infty}F_{XY}(x, y) dx\\)</p> </li> </ul> <p>Properties</p> <ul> <li> <p>\\(0 \\le F_{XY}(x, y) \\le 1\\)</p> </li> <li> <p>\\(\\lim_{x,y \\to \\infty} F_{XY}(x, y) = 1\\)</p> </li> <li> <p>\\(\\lim_{x,y \\to -\\infty} F_{XY}(x, y) = 0\\)</p> </li> <li> <p>\\(F_X(x) = \\lim_{y \\to \\infty}F_{XY}(x, y)\\)</p> </li> </ul>"},{"location":"basics/Probability/#pmf_1","title":"PMF","text":"<ul> <li> <p>\\(p_{XY}:R \\times R \\rightarrow [0,1]\\)</p> </li> <li> <p>\\(p_{XY}(x,y) = P(X=x, Y=y)\\)</p> </li> <li> <p>\\(p_X(x) = \\sum_{y} p_{XY}(x,y)\\), marginal probability function.</p> </li> </ul>"},{"location":"basics/Probability/#expectation_1","title":"Expectation","text":"<ul> <li> <p>\\(g: R^2 \\rightarrow R\\)</p> </li> <li> <p>\\(E[g(X,Y)] =  \\sum_{x \\in Val(X)}\\sum_{y \\in Val(Y)} g(X,Y) p_{XY}(x,y)\\), \\(X, Y\\) are discrete rv.</p> </li> <li> <p>\\(E[g(X,Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y) f_{XY}(x,y)dxdy\\), \\(X, Y\\) are continuous  rv.</p> </li> </ul> <p>Properties</p> <ul> <li>\\(E[f(X,Y) +g(X,Y)] = E[f(X,Y)] + E[g(X,Y)]\\)</li> </ul> <p>If X and Y are independent, then \\(E[f(X)g(Y)] = E[f(X)] E[g(Y)]\\)</p>"},{"location":"basics/Probability/#covariance","title":"Covariance","text":"<ul> <li> <p>\\(Cov(X,Y) = E[(X - E[X]) (Y - E[Y])]\\)</p> </li> <li> <p>\\(Cov (X, Y) = E[XY] - E[X]E[Y]\\)</p> </li> </ul> <p>Properties</p> <ul> <li>\\(Var[X+Y] = Var[X] + Var[Y] + 2Cov[X,Y]\\)</li> </ul> <p>If X and Y are independent, then \\(Cov(X,Y) = 0\\)</p>"},{"location":"basics/Probability/#random-vectors","title":"Random Vectors","text":"<p>Consider data in the d-dimensional Euclidean Space</p> <ul> <li> <p>\\(x = (x^1, ..., x^d) \\in R^d\\)</p> </li> <li> <p>\\(\\langle x, y \\rangle = \\sum_{i=1}^{d} x^iy^i\\)</p> </li> <li> <p>\\(||x|| =\\sqrt{\\langle x,x \\rangle}\\)</p> </li> </ul> <p>Consider RV \\(X \\in R^d\\) with PDF \\(p_X : R^d \\rightarrow R_{\\ge 0}\\) providing event A with probability.</p> <ul> <li>\\(P(X\\in A) = \\int_{A} p_X(x)dx\\) where \\(\\int p_X(x)dx =1\\)</li> </ul> <p>To keep notation concise.</p> <ul> <li>\\(p_{X_t}\\) as \\(p_t\\)</li> <li>\\(X \\sim p(X)\\) as \\(X \\sim p\\)</li> </ul>"},{"location":"basics/Probability/#expectation_2","title":"Expectation","text":"<ul> <li> <p>Expected value minimizes the mean squared error.</p> </li> <li> <p>\\(E[X] = {\\arg \\min}_{z \\in R^d} \\int {||x-z ||}^2 p_X(x)dx = \\int x p_X(x)dx\\)</p> </li> <li> <p>\\(E[f(X)] = \\int f(x) p_X(x)dx\\) : Law of the Unconscious Statistician</p> </li> </ul> <p>Conditional densities and expectations</p> <ul> <li> <p>\\(X, Y \\in R^d\\)</p> </li> <li> <p>\\(p_{X,Y}(x,y)\\) :  Joint PDF</p> </li> <li> <p>\\(p_Y(y) = \\int p_{X,Y}(x,y) dx\\)  : Marginal PDF</p> </li> <li> <p>\\(p_X(x) = \\int p_{X,Y}(x,y) dy\\) : Marginal PDF</p> </li> <li> <p>\\(P_{X|Y}(x|y) := \\frac{p_{X,Y}(x,y)}{p_Y(y)}\\) : Conditional PDF that describes the PDF of the RV \\(X\\) when conditioned on \\(Y=y\\) with density \\(p_Y(y)&gt;0\\).</p> </li> </ul> <p>Using Bayes Rule</p> <ul> <li> <p>\\(P_{Y|X}(y|x) = \\frac{p_{X|Y}(x|y) p_Y(y)}{p_X(x)}\\) where \\(p_{X} &gt; 0\\).</p> </li> <li> <p>\\(E[X|Y=y] := g_{\\star}(y) = \\int x p_{X|Y}(x|y) dx\\) : It is function of y, \\(g_{\\star}(y) : R^d \\rightarrow R^d\\)</p> </li> <li> <p>\\(E[X|Y]\\) is a RV assuming values in \\(R^d\\)</p> </li> </ul>"},{"location":"basics/Probability/#tower-property","title":"Tower property","text":"<ul> <li> <p>\\(E[E[X|Y]] = E[X]\\) : Detail on variable \\(E_{Y}[E_{X}[X|Y]] = E_{X}[X]\\)</p> </li> <li> <p>\\(E[f(X,Y)|Y=y)] = \\int f(x,y) p_{X|Y}(x|y) dx\\) : Law of the Unconscious Statistician</p> </li> </ul>"},{"location":"basics/Probability/#covariance_1","title":"Covariance","text":"<ul> <li>\\(\\Sigma = E[(X-E[X])(X-E[X])^T]\\)</li> </ul>"},{"location":"basics/Probability/#change-of-variables","title":"Change of Variables","text":"<p>Jacobian matrix</p> <ul> <li> <p>\\(J =  \\frac{\\partial(x_1, x_2)}{\\partial(y_1, y_2)}=\\begin{bmatrix} \\frac{\\partial{x_1}}{\\partial{y_1}} &amp; \\frac{\\partial{x_1}}{\\partial{y_2}} \\\\ \\frac{\\partial{x_2}}{\\partial{y_1}} &amp; \\frac{\\partial{x_2}}{\\partial{y_2}} \\end{bmatrix}\\)</p> </li> <li> <p>\\(Y = g(X)\\) and \\(X = h(Y)\\). \\(h\\) is inverse of \\(g\\)</p> </li> <li> <p>\\(p_Y(y) =  |J| \\cdot p_X(h(y))\\)</p> </li> </ul> <p>Example use case ( Flow Matching )</p> <ul> <li> <p>\\(X \\sim p_X\\)</p> </li> <li> <p>\\(\\psi : R^d \\rightarrow R^d\\)</p> </li> <li> <p>\\(Y = \\psi (X)\\)</p> </li> </ul> <p>Calculate \\(p_Y(y)\\), \\(E[f(Y)]\\)</p> <p>Solution :</p> <ul> <li> <p>\\(p_Y(y) = |J| p_X(h(y))\\)</p> </li> <li> <p>\\(x = h(y) := \\psi^{-1}(y)\\)</p> </li> </ul> <p>\\(Here, J = \\begin{bmatrix} \\frac{\\partial{x}}{\\partial{y}}\\end{bmatrix} =\\begin{bmatrix} \\frac{\\partial{ \\psi^{-1}(y)}}{\\partial{y}}\\end{bmatrix}\\)</p> <p>Therefore, \\(p_Y(y) = | \\det \\partial_{y}\\psi^{-1}(y) | \\cdot p_X(\\psi^{-1}(y))\\)</p> <p>Now,</p> <ul> <li>\\(E[f(Y)] = \\int f(y) p_Y(y) dy = \\int f(y) | \\det \\partial_{y}\\psi^{-1}(y) | \\cdot p_X(\\psi^{-1}(y)) dy\\)</li> </ul>"},{"location":"basics/Regularization/","title":"Regularization","text":""},{"location":"basics/Regularization/#regularization","title":"Regularization","text":""},{"location":"basics/Regularization/#lasso","title":"Lasso","text":"<ul> <li>It is also called <code>L1 regularization</code></li> <li>Lasso has \u201cbuilt-in\u201d feature selection since it shrinks the least important features\u2019 coefficient to zero, creating sparse outputs. </li> <li>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda |\\sum_{i=0}^{M}w_j|\\)</li> </ul>"},{"location":"basics/Regularization/#ridge","title":"Ridge","text":"<ul> <li>It is also called <code>L2 regularization</code></li> <li>Ridge regression is more computationally efficient due to being differentiable at 0 (can be used easily with gradient descent) while lasso is undefined at 0. </li> <li>\\(\\tilde{\\mathcal{L}}(w) = \\mathcal{L}(w) + \\frac{\\lambda}{2} ||w||^2\\)<ul> <li>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\frac{\\lambda}{2} \\sum_{i=0}^{M}w_j^2\\)</li> </ul> </li> <li>\\(\\nabla \\tilde{\\mathcal{L}}(w) = \\nabla \\mathcal{L}(w) + \\lambda w\\)</li> <li>\\(w_{t+1} = w_t - \\eta \\cdot \\nabla \\tilde{\\mathcal{L}}(w_t)\\)</li> </ul>"},{"location":"basics/Regularization/#elastic-net","title":"Elastic-net","text":"<ul> <li>It is also called <code>L1+L2 regularization</code></li> <li>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda_1 |\\sum_{i=0}^{M}w_j| + \\lambda_2 \\sum_{i=0}^{M}w_j^2\\)</li> </ul>"},{"location":"basics/Regularization/#dataset-augmentation","title":"Dataset Augmentation","text":""},{"location":"basics/Regularization/#parameter-sharing-and-tying","title":"Parameter Sharing and tying","text":""},{"location":"basics/Regularization/#adding-noise-to-the-input","title":"Adding Noise to the input","text":""},{"location":"basics/Regularization/#adding-noise-to-the-output","title":"Adding Noise to the output","text":""},{"location":"basics/Regularization/#early-stopping","title":"Early Stopping","text":""},{"location":"basics/Regularization/#ensemble-methods","title":"Ensemble Methods","text":""},{"location":"basics/Regularization/#dorpout","title":"Dorpout","text":""},{"location":"basics/Regularization/#inverted-dropout","title":"Inverted Dropout","text":""},{"location":"basics/Regularization/#question","title":"Question","text":"How L1 helps in feature selection? <ul> <li>L1 regularization adds absolute value of weights (|w|) to loss function</li> <li>Pushes less important feature weights exactly to zero</li> <li>Creates sparse solutions where many features have zero coefficients</li> </ul> Why Exactly Zero? <ul> <li> <p>\\(\\mathcal{L} = \\sum_{i=0}^{N}(y_i - \\sum_{i=0}^{M}x_{ij}w_j)^2 + \\lambda |\\sum_{i=0}^{M}w_j|\\)</p> </li> <li> <p>derivative of \\(|W|\\)</p> <ul> <li>+1  when w &gt; 0</li> <li>-1  when w &lt; 0</li> <li>[-1,1] at w = 0 (subgradient)</li> <li>Function is non-differentiable at w = 0</li> </ul> </li> <li> <p>update equation</p> <ul> <li> <p>w = w - \u03b1(\u2202L/\u2202w + \u03bb\u22c5sign(w)) where, sign(w) = {-1,0,1}</p> </li> <li> <p>\\(\\text{For } w &gt; 0:     \\begin{cases}     w_{t+1} = w_t - \\alpha(\\frac{\\partial L}{\\partial w} + \\lambda) \\     \\text{Term } +\\lambda \\text{ pushes w toward zero}     \\end{cases}\\)</p> <p>\\(\\text{For } w &lt; 0: \\begin{cases} w_{t+1} = w_t - \\alpha(\\frac{\\partial L}{\\partial w} - \\lambda) \\ \\text{Term } -\\lambda \\text{ pushes w toward zero} \\end{cases}\\)</p> <p>\\(\\text{At } w = 0: \\begin{cases} \\text{Stays at zero if } |\\frac{\\partial L}{\\partial w}| &lt; \\lambda \\ \\text{Escapes zero if } |\\frac{\\partial L}{\\partial w}| &gt; \\lambda \\end{cases}\\)</p> </li> </ul> </li> </ul> Dropout and how it can be used to obtain probability distribution of response variable. <ul> <li>During Inference:<ul> <li>Keep dropout active (don't disable)</li> <li>Run multiple forward passes (e.g., 100 times)</li> <li>Each pass gives different prediction</li> </ul> </li> <li>Use the prediction:<ul> <li>Mean prediction (average of all passes)</li> <li>Uncertainty estimate (variance of predictions)</li> <li>Approximate probability distribution</li> </ul> </li> </ul>"},{"location":"basics/Scheduler/","title":"Scheduler","text":""},{"location":"basics/Scheduler/#scheduler","title":"Scheduler","text":""},{"location":"basics/Scheduler/#steplr","title":"StepLR","text":"<p>\\(lr_{epoch} = \\begin{cases} \\gamma * lr_{epoch-1}, &amp; \\text{if epoch % step_size = 0} \\\\ lr_{epoch-1}, &amp; \\text{otherwise} \\end{cases}\\)</p>"},{"location":"basics/Scheduler/#multisteplr","title":"MultiStepLR","text":"<p>\\(lr_{epoch} = \\begin{cases} \\gamma * lr_{epoch-1}, &amp; \\text{if epoch in milestones} \\\\ lr_{epoch-1}, &amp; \\text{otherwise} \\end{cases}\\)</p>"},{"location":"basics/Scheduler/#exponentiallr","title":"ExponentialLR","text":"<p>\\(lr_{epoch} = \\gamma * lr_{epoch-1}\\)</p>"},{"location":"basics/VectorCalculus/","title":"Vector Calculus","text":""},{"location":"basics/VectorCalculus/#vector-calculus","title":"Vector Calculus","text":""},{"location":"basics/VectorCalculus/#basics","title":"Basics","text":""},{"location":"basics/VectorCalculus/#vector","title":"Vector","text":""},{"location":"basics/VectorCalculus/#notations","title":"Notations","text":"<ul> <li> <p>\\(\\bar{A} = A_x \\hat{i} + A_y \\hat{j} + A_z \\hat{k}\\)</p> </li> <li> <p>\\(\\bar{B} = B_x \\hat{i} + B_y \\hat{j} + B_z \\hat{k}\\)</p> </li> </ul> <p>Position vector for point \\(P(x,y,z) : \\bar{OP} = \\bar{r} = x \\hat{i} + y \\hat{j} + z \\hat{k}\\)</p> <ul> <li>\\(r = \\sqrt{x^2 + y^2 + z^2}\\)</li> </ul>"},{"location":"basics/VectorCalculus/#properties","title":"Properties","text":"<ol> <li>Magnitude</li> <li>Direction</li> <li>Follows vector law of addition</li> </ol>"},{"location":"basics/VectorCalculus/#representations","title":"Representations","text":"<ul> <li> <p>\\(\\bar{A} = |\\bar{A}| \\hat{a}_A\\)</p> </li> <li> <p>\\(\\hat{a}_A = \\frac{\\bar{A} }{ |\\bar{A}| }\\) : unit vector</p> </li> <li> <p>\\(\\bar{AB} = \\bar{B} - \\bar{A}\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#angle-between-two-vectors","title":"Angle between two vectors","text":"<ul> <li>\\(0 \\le \\theta_{AB} \\le 180^{\\circ}\\)</li> </ul>"},{"location":"basics/VectorCalculus/#dot-product","title":"Dot Product","text":"<ul> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = |\\bar{A}| |\\bar{B}| \\cos \\theta_{AB} \\rightarrow \\text{scalar}\\)</p> </li> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = 0 \\rightarrow \\text{perpendicular}\\)</p> </li> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = |\\bar{A}| |\\bar{B}|  \\rightarrow \\text{parallel}\\)</p> </li> <li> <p>\\(\\bar{A} \\cdot \\bar{B} = - |\\bar{A}| |\\bar{B}| \\rightarrow \\text{anti-parallel}\\)</p> </li> </ul> <p>Example:</p> <ul> <li>\\(\\bar{A} \\cdot \\bar{B} = A_xB_x + A_yB_y +A_zB_z\\)</li> </ul> <p>Projection</p> <p>Projection of \\(\\bar{B}\\) in the direction of \\(\\bar{A} = \\bar{B} \\cdot \\hat{a}_A = |\\bar{B}| |\\hat{a}| \\cos \\theta_{AB} = |\\bar{B}| \\cos \\theta_{AB}\\)</p>"},{"location":"basics/VectorCalculus/#cross-product","title":"Cross Product","text":"<p>\\(\\bar{A} \\times \\bar{B} = |\\bar{A}| |\\bar{B}| \\sin \\theta_{AB} \\hat{a}_n\\)</p> <p>$\\hat{a}_n $$: normal perpendicular to plane AB while taking curl from \\(\\(\\bar{A}\\)\\) to towards $\\(\\bar{B}\\)</p> <p>Example:</p> <ul> <li>\\(\\bar{A} \\times \\bar{B} = \\begin{bmatrix} \\hat{i} &amp; \\hat{j} &amp; \\hat{k} \\\\ A_x &amp; A_y &amp; A_z \\\\ B_x &amp; B_y &amp;B_z \\end{bmatrix}\\)</li> </ul> <p>Physical Significance of curl:</p> <ul> <li> <p>\\(\\text{Area of }\\triangle ABC = \\frac{1}{2} |\\bar{AB} \\times \\bar{AC} |\\)</p> </li> <li> <p>\\(\\text{Area of }\\square ABCD = |\\bar{AB} \\times \\bar{AD} | = \\frac{1}{2} |\\bar{AC} \\times \\bar{BD} |\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#co-ordinate-system","title":"Co-ordinate System","text":""},{"location":"basics/VectorCalculus/#cartesian-coordinate-system","title":"Cartesian Coordinate System","text":"<p>Variables: \\((x, y, z)\\)</p> <p>Properties</p> <ul> <li>\\(-\\infty &lt; x &lt; \\infty\\)</li> <li>\\(-\\infty &lt; y &lt; \\infty\\)</li> <li>\\(-\\infty &lt; z &lt; \\infty\\)</li> </ul>"},{"location":"basics/VectorCalculus/#cylindrical-coordinate-system","title":"Cylindrical Coordinate System","text":"<p>Variables: \\((r, \\phi, z)\\)</p> <p>Properties</p> <ul> <li>\\(0 \\le r &lt; \\infty\\)</li> <li>\\(0 \\le \\phi &lt; 2\\pi\\)</li> <li>\\(-\\infty &lt; z &lt; \\infty\\)</li> </ul>"},{"location":"basics/VectorCalculus/#spherical-coordinate-system","title":"Spherical Coordinate System","text":"<p>Variables : \\((r, \\theta,  \\phi)\\)</p> <p>Properties</p> <ul> <li>\\(0 \\le r &lt; \\infty\\)</li> <li>\\(0 \\le \\phi &lt; 2\\pi\\)</li> <li>\\(0 \\le \\theta \\le \\pi\\)</li> </ul>"},{"location":"basics/VectorCalculus/#vector-operator","title":"Vector Operator","text":"<p>Del/Nabla operator</p> <ul> <li>\\(\\nabla = \\frac{\\partial{}}{\\partial{x}}\\bf{\\hat{i}} +  \\frac{\\partial{}}{\\partial{y}} \\bf{\\hat{j}} +  \\frac{\\partial{}}{\\partial{z}} \\bf{\\hat{k}}\\): vector</li> </ul> <p>Laplacian Operator</p> <ul> <li>\\(\\nabla \\cdot \\nabla = \\nabla^2 = \\frac{\\partial^2}{\\partial{x^2}}+  \\frac{\\partial^2}{\\partial{y^2}} +  \\frac{\\partial^2}{\\partial{z^2}}\\) : scalar</li> </ul> <p>Scalar-point function</p> <ul> <li>\\(f(x,y,z) : R^3 \\rightarrow R\\) (output scalar)</li> </ul> <p>Level-surface equation : \\(f(x,y,z) = c\\)</p> <p>Vector-point function</p> <ul> <li> <p>\\(\\bar{F}(x,y,z) : R^3 \\rightarrow R^3\\)(output vector)</p> </li> <li> <p>\\(\\bar{F} = F_x\\hat{i} + F_y\\hat{j} + F_z\\hat{k}\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#differential","title":"Differential","text":""},{"location":"basics/VectorCalculus/#gradient","title":"Gradient","text":"<p>Calculated for scalar-point function \\(f\\)</p> <p>Caution</p> <p>Convert function \\(f\\) into the form \\(f = 0\\) before calculating the gradient</p> <p>\\(\\text{grad} f  \\rightarrow \\text{vector}\\)</p> <p>\\(\\text{grad} f = \\nabla f = \\nabla f(x,y,z) = (\\frac{\\partial{f}}{\\partial{x}}, \\frac{\\partial{f}}{\\partial{y}}, \\frac{\\partial{f}}{\\partial{z}}) = \\frac{\\partial{f}}{\\partial{x}}\\bf{i} +  \\frac{\\partial{f}}{\\partial{y}} \\bf{j} +  \\frac{\\partial{f}}{\\partial{z}} \\bf{k}\\) ( direction is in the dir \\(f &gt; c\\) )</p> <p>Properties of gradient</p> <ul> <li>\\(|\\nabla f|\\): Maximum rate of change in the normal direction of \\(f\\)</li> <li>\\(\\nabla(r) = \\frac{\\bar{r}}{r}\\)</li> <li>\\(\\nabla(f(r)) = f'(r)\\frac{\\bar{r}}{r}\\)</li> <li>\\(\\nabla^2[f(r)] = f''(r) + \\frac{2}{r}f'(r)\\)</li> </ul> <p>Application of gradient</p> <ol> <li>Directional derivative (D.D.) of \\(f\\) at point \\(P\\) in the direction of \\(\\bar{M} = (\\nabla f)_{P} \\cdot \\hat{a}_M\\)</li> <li>Angle between to level surface \\(f_1\\) and \\(f_2\\) : \\(\\nabla f_1 \\cdot \\nabla f_2 = |\\nabla f_1| |\\nabla f_2| \\cos \\theta\\)</li> </ol>"},{"location":"basics/VectorCalculus/#divergence","title":"Divergence","text":"<p>Calculated for vector point function \\(\\bar{F}\\)</p> <ul> <li> <p>\\(\\nabla \\cdot \\bar{F} \\rightarrow \\text{Scalar}\\)</p> </li> <li> <p>\\(\\text{div} \\bar{F} =  \\nabla \\cdot \\bar{F} = (\\frac{\\partial{F_x}}{\\partial{x}} + \\frac{\\partial{F_y}}{\\partial{y}} + \\frac{\\partial{F_z}}{\\partial{z}})\\)</p> </li> </ul> <p>Physical significance of divergence</p> <p>1 . It is only understood at a point. i.e., \\((\\nabla \\cdot \\bar{F})_P\\)</p> <p>If \\((\\nabla \\cdot \\bar{F})_P\\)  := positive, \\(P\\) is acting as a source.</p> <p>If \\((\\nabla \\cdot \\bar{F})_P\\)  := negative, \\(P\\) is acting as a sink.</p> <ol> <li>Solenoidal Vector Field</li> </ol> <p>\\(\\nabla \\bar{F} = 0\\),  at all points \\((x,y,z)\\)</p> <p>Example: magnetic field, any field lines that makes closed loop</p> <p>Properties of divergence</p> <ul> <li>\\(\\nabla\\cdot \\bar{r} = 3\\)</li> </ul>"},{"location":"basics/VectorCalculus/#curl","title":"Curl","text":"<p>\\(\\nabla \\times \\bar{F} \\rightarrow \\text{Vector}\\)</p> <p>\\(\\nabla \\times \\bar{F} = \\begin{bmatrix} \\hat{i} &amp; \\hat{j} &amp; \\hat{k} \\\\ \\frac{\\partial}{\\partial{x}} &amp; \\frac{\\partial}{\\partial{y}} &amp;  \\frac{\\partial}{\\partial{z}} \\\\ F_x &amp; F_y &amp; F_z \\end{bmatrix}\\)</p> <p>Physical significance of curl</p> <ol> <li>It is only understood at a point. i.e., \\((\\nabla \\times \\bar{F})_P\\)</li> </ol> <p>It represents the capacity of vector field to rotate the point P</p> <ol> <li>Irrotational Vector Field/Conservative Vector Field : \\((\\nabla \\times \\bar{F})_P = \\bar{0}, \\forall P\\)</li> </ol> <p>\\((\\nabla \\times \\bar{E}) = \\bar{0}\\) , where \\(\\bar{E}\\) is static electric field.</p> <p>Properties of curl</p> <ul> <li>\\(\\nabla \\times  \\bar{r} = \\bar{0}\\), where \\(\\bar{r}\\) is positional vector.</li> </ul> <p>Null Identity</p> <ul> <li> <p>Scalar: \\(\\nabla \\cdot [\\nabla \\times \\bar{A}] = 0\\)</p> </li> <li> <p>Vector: \\(\\nabla \\times [\\nabla \\phi] = \\bar{0}\\)</p> </li> </ul>"},{"location":"basics/VectorCalculus/#laplacian","title":"Laplacian","text":"<p>\\(\\nabla^2 f = \\nabla \\cdot (\\nabla f) = \\frac{\\partial^2 f}{\\partial{x}^2} + \\frac{\\partial^2 f}{\\partial{y}^2} + \\frac{\\partial^2 f}{\\partial{z}^2}\\)</p> <p>Properties of laplacian</p> <ul> <li>Rotation invariant</li> <li>\\(\\nabla^2(fg) = f \\nabla^2 g + 2(\\nabla f \\cdot \\nabla g) + g \\nabla^2 f\\)</li> <li>Laplace's equation: \\(\\nabla^2 f = 0\\)</li> <li>Poisson's equation: \\(\\nabla^2 f = - \\rho\\)</li> </ul>"},{"location":"basics/VectorCalculus/#integral","title":"Integral","text":""},{"location":"basics/VectorCalculus/#line-integral","title":"Line Integral","text":"<p>\\(\\int_C \\bar{F} \\cdot \\bar{d}l\\)</p> <p>Closed line encircling an Area.</p> <p>Non-conservative Vector Field</p> <ul> <li>Depends on endpoints</li> <li>Path dependent</li> <li>\\(\\oint_C \\bar{F} \\cdot \\bar{d}l \\neq 0\\)</li> </ul> <p>Conservative Vector Field</p> <ul> <li>Depends on endpoints</li> <li>Path independent</li> <li>\\(\\oint_C \\bar{F} \\cdot \\bar{d}l = 0\\)</li> <li>\\(F = \\nabla f\\) for some scalar field</li> <li>\\(\\int_A^B \\bar{F} \\cdot \\bar{d}l = f(B) - f(A)\\)</li> </ul>"},{"location":"basics/VectorCalculus/#surface-integral","title":"Surface Integral","text":"<p>\\(\\int_S \\bar{F} \\cdot \\bar{d}s\\) or \\(\\int \\int_S \\bar{F} \\cdot \\bar{d}s\\)</p> <p>Open Surface: Circle, Rectangle etc.</p> <p>Closed Surface: Closed Surface Enclosing a Volume (Hollow).</p> <p>Level Surface: \\(f(x,y,z) =constant\\)</p> <p>Area Vector: \\(\\bar{S} =|\\bar{S}|\\hat{a}_S\\)</p> <p>\\(\\hat{a}_S \\rightarrow\\) always normal to the surface</p> <p>Open Surface Integral of Vector Field</p> <p>\\(\\int \\int_S \\bar{F} \\cdot \\bar{d}s = \\psi\\)</p> <p>Measure of field lines of Vector \\(bar{F}\\) crossing surface \\(S\\), perpendicularly = Flux</p> <p>\\(\\psi\\) is flux of \\(\\bar{F}\\) through \\(S\\)</p>"},{"location":"basics/VectorCalculus/#volume-integral","title":"Volume Integral","text":"<p>\\(\\int \\int \\int_V V dv\\)</p>"},{"location":"basics/VectorCalculus/#stokes-theorem","title":"Stoke's Theorem","text":"<p>\\(\\oint_C \\bar{A} \\cdot \\bar{dl} = \\int\\int_S (\\nabla \\times \\bar{A}) \\cdot \\bar{ds}\\)</p> <p>A is continuous and differential at every point inside C</p> <p>If Vector Field is irrotational then it has to be Conservative vector field. But converse is not necessarily true.</p> <p>If : \\(\\nabla \\times \\bar{A} = \\bar{0}\\) then \\(\\oint_C \\bar{A} \\cdot \\bar{dl} = 0\\)</p> <p>Inverse Stoke's Theorem :</p> <p>\\(\\int\\int_S (\\nabla \\times \\bar{A}) \\cdot \\bar{ds} =\\oint_C \\bar{A} \\cdot \\bar{dl}\\)</p>"},{"location":"basics/VectorCalculus/#greens-theorem","title":"Green's Theorem","text":"<p>\\(\\oint_C M(x,y) dx + N(x,y) dy = \\oint M dx + N dy = \\int \\int_R (\\frac{\\partial N}{\\partial x} - \\frac{\\partial M}{\\partial y})dxdy\\) if C is in anti-clockwise direction and if C is in clock-wise direction \\(-\\int \\int_R (\\frac{\\partial N}{\\partial x} - \\frac{\\partial M}{\\partial y})dxdy\\)</p> <p>M, N are continuous and differential at every point C.</p>"},{"location":"basics/VectorCalculus/#divergence-theorem","title":"Divergence Theorem","text":"<p>\\(\\oint_S \\bar{A} \\cdot \\bar{ds} = \\int \\int \\int_V (\\nabla \\cdot \\bar{A}) dv\\)</p>"},{"location":"basics/bayesian/","title":"Bayesian","text":""},{"location":"basics/bayesian/#bayesian","title":"Bayesian","text":""},{"location":"basics/bayesian/#question","title":"Question","text":"<ul> <li>what is bayesian inference ?</li> </ul>"},{"location":"casestudy/","title":"Case Study","text":""},{"location":"casestudy/#case-study","title":"Case Study","text":"<ul> <li> <p>When a user adds an item to their cart, we need to calculate the probability that they will return the product (given that they complete the order). Using this probability, the company can adjust shipping charges accordingly. For example, if the probability of return is high, the company may increase shipping charges to discourage potential returns. How would you calculate this probability?</p> </li> <li> <p>Can you explain what A/B Testing is? How would you take a uniform sample set of 10M users from streaming data of approximately 10B users, ensuring that every user has equal probability of being in the final sampled set?</p> </li> <li> <p>In a scenario where you have a small sample size of approximately 20 observations, which statistical test would you use and why?</p> <ul> <li>This tests understanding of:<ul> <li>Small sample size statistics</li> <li>When to use parametric vs non-parametric tests</li> <li>Knowledge of t-test vs z-test vs other tests</li> </ul> </li> </ul> </li> <li> <p>In a credit card fraud detection system, how would you determine the optimal classification threshold? While ROC-AUC is one metric, what other considerations and metrics should be taken into account given that:</p> <ul> <li>False positives (legitimate transactions flagged as fraud) cause customer inconvenience</li> <li>False negatives (missed fraud) cause direct financial loss</li> </ul> </li> <li> <p>You need to design a recommendation system to replace an existing rule-based job search system. Provide a detailed plan</p> </li> </ul>"},{"location":"cv/","title":"Computer Vision","text":""},{"location":"cv/#overview","title":"Overview","text":""},{"location":"cv/#core","title":"Core","text":"<ul> <li> Basics</li> <li> Image Operations</li> <li> Image Annotations</li> <li> Image Enhancement</li> <li> Image Filtering</li> <li> Image Feature</li> <li> Image Alignment</li> <li> Panorama</li> <li> HDR</li> <li> Image Classification</li> <li> Image Segmentation</li> <li> Edge Detection &amp; Contours</li> <li> Object Detection</li> <li> Face Detection</li> <li> Optical Flow</li> <li> Object Tracking</li> <li> Pose Estimation</li> <li> OCR</li> </ul>"},{"location":"cv/#advance","title":"Advance","text":"<ul> <li> AutoEncoder</li> <li> ViT</li> <li> NeRF</li> </ul>"},{"location":"cv/#generative","title":"Generative","text":"<ul> <li> Variational Encoder</li> </ul>"},{"location":"cv/classification/","title":"Classification","text":""},{"location":"cv/classification/#classification","title":"Classification","text":""},{"location":"cv/classification/#question","title":"Question","text":"Given a lot of unlabelled data along with a small amount of labelled data, how will you use this unlabelled data to improve your classification model on labelled data. <ul> <li>TACLE</li> </ul>"},{"location":"cv/cnn/","title":"CNN","text":""},{"location":"cv/cnn/#convolutional-neural-network","title":"Convolutional Neural Network","text":"<p>\\(W_{new} = \\frac{W_{old} - F + 2P}{S} +1\\)</p> <p>\\(H_{new} = \\frac{H_{old} - F + 2P}{S} +1\\)</p> <p>How is learning kernel is different from regular feed-forward NN? - Weight Sharing - Sparse Network</p> <p>CNN tries to learn ecific characterstics of inputs, different neurons can fire for different characterstics.</p>"},{"location":"cv/cnn/#question","title":"Question","text":"<ul> <li>why cnn is more robust then dnn ? how is it translation invariant ?</li> </ul> Given two CNN with the same number of parameters, but one is of single layer while other is a 2-layer network. Which one you will choose and why? <ul> <li>2-layer CNN has better feature learning capability and expressiveness.<ul> <li>First layer learns basic features (edges, corners) and second layer combines these to learn more complex patterns</li> <li>Additional ReLU between layers adds more non-linearity</li> </ul> </li> </ul> Analyze the differentiability of the MaxPool function commonly used in CNNs? <ul> <li> <p>MaxPool Properties:</p> <ul> <li>Non-linear function</li> <li>Not differentiable at points where maximum changes</li> <li>Piece-wise continuous</li> <li>Only one input contributes to output</li> </ul> </li> <li> <p>Derivative Behavior:</p> <ul> <li>Gradient = 1 for max element</li> <li>Gradient = 0 for all other elements</li> <li>Not defined when two elements are equal</li> </ul> </li> </ul>"},{"location":"cv/detection/","title":"Object Detection","text":""},{"location":"cv/detection/#object-detection","title":"Object Detection","text":""},{"location":"cv/detection/#introduction","title":"Introduction","text":"<p>The goal of object detection is to predict a set of bounding boxes(x,y,w,h) and category labels for each object of interest.</p>"},{"location":"cv/detection/#traditional","title":"Traditional","text":""},{"location":"cv/detection/#template-matching-sliding-window","title":"Template Matching + Sliding Window","text":"<p>For every position you evaluate how much do the pixels in the image and template correlate.</p> <p>Cons</p> <ol> <li>Does not handle occlusions.</li> <li>Works with instance of object but not with class of it.</li> <li>Does not work if pose changes.</li> <li>Does not work if position, scale and aspect ratio changes.</li> </ol>"},{"location":"cv/detection/#feature-extraction-and-classification","title":"Feature Extraction and Classification","text":"<p>Learn multiple weak classifier to build a strong final decision.</p>"},{"location":"cv/detection/#feature-extraction","title":"Feature Extraction","text":"<p>Viola-Jones Detector</p> <p>Haar Features</p> <p>Histogram of Oriented Gradients(HOGs) Compute gradients in dense grids, compute gradients and create a histogram based on gradient direction</p> <p>Deformable Part Model (DPM) Based on HOG features but based on body part detection. More robust to different body poses.</p>"},{"location":"cv/detection/#classification","title":"Classification","text":"<p>It is done with the help of SVM.</p>"},{"location":"cv/detection/#general-object-detection","title":"General Object Detection","text":"<ul> <li>Class agnostic</li> <li>Object Proposals / Region of Intrest<ul> <li>Selective search</li> <li>Edge boxes</li> </ul> </li> </ul> <p>Localization</p>"},{"location":"cv/detection/#two-stage-detector","title":"Two-Stage Detector","text":"<ul> <li>R-CNN, Fast R-CNN, Faster R-CNN</li> <li>SPP-Net, R-FCN, FPN</li> </ul> <ul> <li>Overfeat</li> <li>R-CNN, Fast R-CNN, Faster R-CNN, SPP-Net</li> </ul>"},{"location":"cv/detection/#one-stage-detector","title":"One-Stage Detector","text":"<p>No need of Region Proposal Network</p> <p>They are very fast</p> <ul> <li>YOLO, SSD, RetinaNet</li> <li>CenterNet, CornerNet, ExtremeNet</li> </ul> <ul> <li>YOLO</li> <li>RetinaNet</li> <li>CornerNet</li> <li>CenterNet</li> <li>ExtremeNet</li> </ul>"},{"location":"cv/detection/#transformer-based-detector","title":"Transformer-Based Detector","text":"<ul> <li>DETR</li> </ul>"},{"location":"cv/detection/#methods","title":"Methods","text":"<ul> <li>Swin Transformer</li> <li>DINO</li> <li>InternImage</li> <li>OWL</li> </ul>"},{"location":"cv/detection/CenterNet/","title":"CenterNet","text":""},{"location":"cv/detection/CenterNet/#centernet","title":"CenterNet","text":"<ul> <li>Focus on the center of the object to infer its class.</li> <li>Use the corners as proposals, and the center to verify the class of the object and filter out outliers.</li> </ul>  CenterNet Architecture   Center Pooling Module"},{"location":"cv/detection/CornerNet/","title":"CornerNet","text":""},{"location":"cv/detection/CornerNet/#cornernet","title":"CornerNet","text":"<p>Bounding box cordinates as top-left and bottom-right corner.</p>  Hourglass network   Corner pooling"},{"location":"cv/detection/CornerNet/#issues","title":"Issues","text":"<ul> <li>Many incorrect bounding boxes (especially small) \\(\\rightarrow\\) too many False Positives</li> <li>Hypothesis: It is hard to infer the class of the box if the network is focused on the boundaries</li> </ul>"},{"location":"cv/detection/DETR/","title":"DETR","text":""},{"location":"cv/detection/DETR/#detr","title":"DETR","text":"<p>A direct set prediction approach to bypass the surrogate tasks (like proposals, anchors, window centers, non-maximum suppression).</p> <p>A encoder-decoder based architecure.</p> <p>It predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.</p>"},{"location":"cv/detection/DETR/#components","title":"Components","text":""},{"location":"cv/detection/DETR/#set-prediction","title":"Set Prediction","text":"<p><code>Bipartite matching loss</code></p> <p>The usual solution is to design a loss based on the Hungarian algorithm, to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match.</p>"},{"location":"cv/detection/DETR/#transformers-and-parallel-decoding","title":"Transformers and Parallel Decoding","text":"<p>Combine transformers and parallel decoding for their suitable trade-off between computational cost and the ability to perform the global computations required for set prediction.</p>"},{"location":"cv/detection/DETR/#object-detection","title":"Object detection","text":"<ol> <li>Two-Stage detectors</li> <li>One-Stage detectors</li> </ol> <p>The final performance of above systems heavily depends on the exact way these initial guesses are set.</p>"},{"location":"cv/detection/DETR/#model","title":"Model","text":""},{"location":"cv/detection/DETR/#object-detection-set-prediction-loss","title":"Object detection set prediction loss","text":"<p>DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image.</p> <ul> <li> <p>\\(y:\\) ground-truth (\\(y_i = (c_i, b_i)\\), \\(c_i\\) target class label and \\(b_i \\in [0,1]^4\\) is ground-truth box.)</p> </li> <li> <p>\\(\\hat{y} = \\{\\hat{y}_i\\}_{i=1}^{N}:\\) set of N predictions</p> </li> <li> <p>\\(y\\) also as a set of size \\(N\\) padded with \\(\\varnothing\\) (no object)</p> </li> </ul> <p>Permutation of element with lowest cost</p> <ul> <li>\\(\\hat{\\sigma} = \\arg\\min_{\\sigma\\in\\mathfrak{G}_N} \\sum_{i}^N \\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)})\\)</li> </ul> <p>Pair-wise matching cost</p> <p>\\(\\mathcal{L}_{\\text{match}}(y_i, \\hat{y}_{\\sigma(i)}) = -\\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\hat{p}_{\\sigma(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}}\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)})\\)</p> <p>\\(\\hat{p}_{\\sigma_i}(c_i):\\) prediction with index \\(\\sigma(i)\\) we define probability of class \\(c_i\\)</p> <p>Hungarian loss</p> <p>\\(\\mathcal{L}_{\\text{Hungarian}}(y, \\hat{y}) = \\sum_{i=1}^N \\left[-\\log \\hat{p}_{\\hat{\\sigma}(i)}(c_i) + \\mathbb{1}_{\\{c_i\\neq\\varnothing\\}} \\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\hat{\\sigma}(i)})\\right]\\)</p> <p>Box-Loss</p> <p>\\(\\mathcal{L}_{\\text{box}}(b_i, \\hat{b}_{\\sigma(i)}) = \\lambda_{\\text{iou}}\\mathcal{L}_{\\text{iou}}(b_i, \\hat{b}_{\\sigma(i)}) + \\lambda_{\\text{L1}}\\|b_i - \\hat{b}_{\\sigma(i)}\\|_1\\)</p>"},{"location":"cv/detection/DETR/#detr-architecture","title":"DETR architecture","text":"<p>CNN Backbone</p> <p>As a feature extractor</p> <p>Transformer encoder</p> <p>Since the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings that are added to the input of each attention layer.</p> <p>Transformer decoder</p> <p>The difference with the original transformer is that our model decodes the N objects in parallel at each decoder layer, while transformer use an autoregressive model that predicts the output sequence one element at a time.</p> <p>Prediction feed-forward networks (FFNs)</p> <p>The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d, and a linear projection layer.</p> <p>Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total N = 100 prediction slots in DETR decoder. </p> <p>This shows that the prediction box focuses on different part of the image.</p>"},{"location":"cv/detection/DETR/#rt-detr","title":"RT-DETR","text":""},{"location":"cv/detection/DETR/#fast-detr","title":"Fast-DETR","text":""},{"location":"cv/detection/DETR/#detrs-fastdetr","title":"DETRs-FastDETR","text":""},{"location":"cv/detection/DETR/#sam-det","title":"SAM-Det","text":""},{"location":"cv/detection/DETR/#ultra-detr","title":"ULTRA-DETR","text":""},{"location":"cv/detection/DINO/","title":"DINO","text":""},{"location":"cv/detection/DINO/#dino","title":"DINO","text":""},{"location":"cv/detection/DINO/#grounding-dino","title":"Grounding DINO","text":""},{"location":"cv/detection/ExtremeNet/","title":"ExtremeNet","text":""},{"location":"cv/detection/ExtremeNet/#extremenet","title":"ExtremeNet","text":"<ul> <li>Precticting the corner where object does not lies is hard for CNNs.</li> <li>Represent objects by their extreme points.</li> <li>No need to predict embeddings for the box computation.</li> </ul>  ExtremeNet Arch."},{"location":"cv/detection/ExtremeNet/#applications","title":"Applications","text":"<ul> <li>Extreme points are used commonly for annotation</li> </ul>"},{"location":"cv/detection/InternImage/","title":"InternImage","text":""},{"location":"cv/detection/OWL/","title":"OWLv2","text":""},{"location":"cv/detection/Overfeat/","title":"Overfeat","text":""},{"location":"cv/detection/Overfeat/#overfeat","title":"Overfeat","text":"<p>Slidingwindow + bbox regression + classification</p>"},{"location":"cv/detection/Overfeat/#sliding-window","title":"Sliding Window","text":"<p>Implicity encoded in the CNN architecture. Use sliding widow at different scale.</p>"},{"location":"cv/detection/Overfeat/#localization","title":"Localization","text":"<p>Regression</p>"},{"location":"cv/detection/Overfeat/#detection","title":"Detection","text":"<p>Classification</p>"},{"location":"cv/detection/Overfeat/#cons","title":"Cons","text":"<ul> <li>Needs fixed sized window as the fully-connected layer need to have fixed input.</li> <li>Expensive to try out all the possible positions, scales and aspect ratio. (Choose only the potential location)</li> </ul>"},{"location":"cv/detection/RCNN/","title":"RCNNs","text":""},{"location":"cv/detection/RCNN/#rcnn","title":"RCNN","text":""},{"location":"cv/detection/RCNN/#rcnn_1","title":"RCNN","text":"<p>Steps</p> <ol> <li> <p>Scan the input image for possible objects using an algorithm called Selective Search, generating ~2000 region proposals</p> </li> <li> <p>Warp to a fix size 227 x 227</p> </li> <li> <p>Run a convolutional neural net (CNN) on top of each of these region proposals</p> </li> <li> <p>Make the output of each CNN and feed it into:</p> <p>a) an SVM to classify the region and </p> <p>b) a linear regressor to tighten the bounding box of the object, if such an object exists.</p> </li> </ol>"},{"location":"cv/detection/RCNN/#training","title":"Training","text":"<ol> <li>Pre-train the CNN on ImageNet</li> <li>Finetune the CNN on the number of classes the detector is aiming to classify (softmax loss).</li> <li>Train a linear Support Vector Machine classifier to classify image regions. One SVM per class! (hinge loss)</li> <li>Train the bounding box regressor (L2 loss)</li> </ol> <p>Cons 1. If we have overlapping window then we will do ConvNet computation for each of the pixels more than 1 times. This increases extra computation.</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#spp-net","title":"SPP Net","text":"<p>Makes the RCNN fast at test time.</p> <p>Issues</p> <ol> <li> <p>Training is slow and complex(no end-to-end)</p> </li> <li> <p>Region Proposal region is fixed</p> </li> </ol>"},{"location":"cv/detection/RCNN/#fast-rcnn","title":"Fast RCNN","text":"<ol> <li>Performing feature extraction over the image before proposing regions, thus only running one CNN over the entire image instead of 2000 CNN\u2019s over 2000 overlapping regions.</li> <li>After conv5 there is FC layer we need to make all the deature size need to be of same size using RoI Pooling layer.</li> <li>Replacing the SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn","title":"Faster RCNN","text":"<ul> <li>Removes region proposal network Can we reuse our CNN feature and still be able to create this proposal.</li> </ul> <p>How to extract proposals.</p> <ul> <li> <p>How many proposals?</p> <ul> <li>Decide a fix number</li> <li>set of 9 anchor box per location (3 scales, 3 aspect ratio)</li> </ul> </li> <li> <p>Where are they placed?</p> <ul> <li>Densly</li> </ul> </li> <li> <p>For each of the location get the descriptor of 256-d by 3X3 filtermap</p> </li> <li> <p>Pass the descriptor to the classification layer and regression layer</p> </li> </ul>"},{"location":"cv/detection/RCNN/#rpn-training","title":"RPN Training","text":"Region Proposal Network  <p>Classification Ground-Truth</p> <p>\\(p^{*}\\) Amount of anchor box overlapping with the Ground-Truth.</p> <p>\\(p^{*} = 1\\) if IoU &gt; 0.7 (anchor is in foreground)</p> <p>\\(p^{*} = 0\\) if IoU &lt; 0.3 (anchor is in background)</p> <p>For training only consider above two case.</p> <ol> <li>Randomly sample 256 sample, form mini-batch.</li> <li>Calculate binary CE loss</li> <li>Anchor box containing object will go through regression box</li> </ol> <p>Anchor box \\((x_a, y_a, w_a, h_a)\\), \\(x_a, y_a\\) is center of box and rest width and height respectively.</p> <ol> <li>Network actually predicts are \\((t_x, t_y, t_w, t_h)\\) which are relative.</li> </ol> <p>\\(t_x = (x-x_a)/w_a\\)</p> <p>\\(t_y = (y-y_a)/h_a\\)</p> <p>\\(t_w = \\log(w/w_a)\\)</p> <p>\\(t_h = \\log(h/h_a)\\)</p> <ol> <li>Smooth L1 loss on regression targets</li> </ol>"},{"location":"cv/detection/RCNN/#faster-rcnn-training","title":"Faster RCNN Training","text":"<p>Can be train jointly. But in paper it is trained in following manner.     - RPN classification (object/non-object)     - RPN regression (anchor -&gt; proposal)     - Fast R-CNN classification (type of object)     - Fast R-CNN regression (proposal -&gt; box)</p> <p>Pros</p> <ol> <li>10x faster at test time wrt Fast R-CNN</li> <li>Trained end-to-end including feature extraction, region proposals, classifier and regressor.</li> <li>More accurate, since proposals are learned. RPN is fully convolutional.</li> </ol>"},{"location":"cv/detection/RCNN/#conclusion","title":"Conclusion","text":"R-CNN Fast RCNN Faster RCNN Test time per image (sec) 50 2 0.2 Speeed-Up 1X 25X 250X mAP (VOC 2007) 66.0 66.9 66.9"},{"location":"cv/detection/RetinaNet/","title":"RetinaNet","text":""},{"location":"cv/detection/RetinaNet/#retinanet","title":"RetinaNet","text":"<p>Since there are lots of anchor box in which there is no object and very few of them object. We need to incorporate this information in the loss function which is done with the weighting of the loss function.</p>  Focal Loss  <p>As \\(\\gamma\\) increases the easy sample weight decreases.</p>"},{"location":"cv/detection/RetinaNet/#key-point","title":"Key Point","text":"<ul> <li>Proposed: Focal loss</li> <li>Powerful feature extraction: ResNet</li> <li>Multi-scale prediction</li> <li>9 anchors per level, each one with a classification and regression target</li> </ul>"},{"location":"cv/detection/SSD/","title":"SSD","text":""},{"location":"cv/detection/SelectiveSearch/","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#selective-search","title":"Selective Search","text":""},{"location":"cv/detection/SelectiveSearch/#wip","title":"WIP","text":""},{"location":"cv/detection/SwinTransformer/","title":"Swin Transformer","text":""},{"location":"cv/detection/YOLO-World/","title":"YOLO-World","text":""},{"location":"cv/detection/YOLO-World/#yolo-world","title":"YOLO-World","text":"<ul> <li> <p>YOLO with openvocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets.</p> </li> <li> <p>Propose a new Re-parameterizable VisionLanguage Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information.</p> </li> <li> <p>\\(\\Omega = \\{B_i, t_i \\}_{i=1}^{N}\\) where \\(t_i\\) is the corresponding text for the region \\(B_i\\)</p> </li> </ul>  YOLO-World Architecture   YOLO-World Architecture  <p>Architecture 1. YOLO Detector      - YOLOv8 2. Text Encoder      - CLIP 3. Text Contrastive Head      - object-text similarity     - \\(s_{k,j} = \\alpha \\cdot \\text{L2-Norm}(e_k) \\cdot \\text{L2-Norm}(w_j)^T + \\beta,\\)     - \\(e_k\\) object embedding and \\(w_j\\) word embedding     - \\(\\alpha\\) and \\(\\beta\\) are learnable.</p>"},{"location":"cv/detection/YOLO-World/#re-parameterizable-vision-language-pan","title":"Re-parameterizable Vision-Language PAN","text":"<ul> <li>establish the feature pyramids {P3,P4,P5} with the multi-scale image features {C3,C4,C5}</li> <li>propose the Text-guided CSPLayer (T-CSPLayer) and Image-Pooling Attention (I-Pooling Attention) to further enhance the interaction between image features and text features, which can improve the visual-semantic representation for open-vocabulary capability</li> </ul>  RepVL-PAN"},{"location":"cv/detection/YOLO-World/#text-guided-csplayer","title":"Text-guided CSPLayer","text":"<p>Text-guided cross-stage partial layers</p> <ul> <li>text embeddings W</li> <li>Image features \\(X_l \\in R^{H \\times W \\times D} (l \\in \\{3, 4, 5 \\})\\)</li> <li>\\(\\delta\\) sigmoid function \\(X_l' = X_l \\cdot \\delta(\\max_{j \\in \\{1..C\\}} (X_lW_j^{\\top}))\\)</li> </ul>"},{"location":"cv/detection/YOLO-World/#image-pooling-attention","title":"Image-Pooling Attention","text":"<ul> <li>Max-Pooling output \\(\\tilde{X}\\)</li> </ul> <p>\\(W' = W + \\text{MultiHead-Attention}(W, \\tilde{X}, \\tilde{X})\\)</p>"},{"location":"cv/detection/YOLO-World/#training","title":"Training","text":"<p>\\(\\mathcal{L}(I) = \\mathcal{L}_{con} + \\lambda_I \\cdot (\\mathcal{L}_{iou} + \\mathcal{L}_{dfl})\\)</p> <ul> <li>\\(\\mathcal{L}_{con}\\) : region-text contrastive loss</li> <li>\\(\\mathcal{L}_{dfl}\\) : distributed focal loss</li> <li>\\(\\lambda_I \\in \\{0, 1\\}\\) <ul> <li>1 if \\(I\\) is from detection or grounding data</li> <li>0 if \\(I\\) is from the image-text data</li> </ul> </li> </ul>"},{"location":"cv/detection/YOLO/","title":"YOLO","text":""},{"location":"cv/detection/YOLO/#yolo","title":"YOLO","text":"<p>It does not have region proposal network and also it does not have the fully-connected layer. </p>  You Only Look Once"},{"location":"cv/detection/YOLO/#process","title":"Process","text":"<ol> <li>Divide the image into grid (SxS cells).</li> <li>Predict B anchor box at the center of each box along with the confidence score</li> <li>Predict C classes for each grid cell.</li> </ol> <p>YOLO-Tensor : SxS(Bx5 + C) </p> <p>where, - SxS : number of grid</p> <ul> <li> <p>B : Number of bbox \\((P_c, b_x, b_y, b_h, b_w)\\)</p> </li> <li> <p>C : Number of classes</p> </li> </ul>  Predicting bounding box and confidence score for each cell.   Predicting class probability for each cell."},{"location":"cv/detection/YOLO/#loss-function","title":"Loss Function","text":"<p>\\(L_{total} = L_{localization} + L_{confidence} + L_{classification}\\)</p> <p>\\(L_{localization} = \\lambda_{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 + (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2]\\)</p> <p>\\(L_{confidence} =  \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} (C_i - \\hat{C}_i)^2 + \\lambda_{noobj} \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{noobj} (C_i - \\hat{C}_i)^2\\)</p> <p>\\(L_{classification} = \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj} \\sum_{c\\in classes} (p_i(c) - \\hat{p}_i(c))^2\\)</p> <p>\\(\\lambda_{coord} = 5\\)</p> <p>\\(\\lambda_{noobj} = 0.5\\)</p>"},{"location":"cv/detection/YOLO/#yolo-papers","title":"YOLO Papers","text":""},{"location":"cv/detection/YOLO/#yolo-survey","title":"YOLO Survey","text":""},{"location":"cv/detection/YOLO/#yolo-v1-slides","title":"YOLO-v1 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v2-slides","title":"YOLO-v2 || Slides","text":""},{"location":"cv/detection/YOLO/#yolo-v3","title":"YOLO-v3","text":""},{"location":"cv/detection/YOLO/#yolo-v4","title":"YOLO-v4","text":""},{"location":"cv/detection/YOLO/#yolo-v5-colab","title":"YOLO-v5 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v6-colab","title":"YOLO-v6 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v7-colab","title":"YOLO-v7 || colab","text":""},{"location":"cv/detection/YOLO/#yolo-v8","title":"YOLO-v8","text":""},{"location":"cv/detection/YOLO/#yolo-nas","title":"YOLO-NAS","text":""},{"location":"cv/detection/YOLOE/","title":"YOLOE","text":""},{"location":"cv/faceDetection/","title":"Face Detection","text":""},{"location":"cv/faceDetection/#face-detection","title":"Face Detection","text":""},{"location":"cv/faceDetection/#haar-cascade","title":"Haar Cascade","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#hog-svm","title":"HoG + SVM","text":"<p>Detail are present here</p>"},{"location":"cv/faceDetection/#facenet","title":"FaceNet","text":"FaceNet   Triplet Loss"},{"location":"cv/faceDetection/#loss","title":"Loss","text":"<p>\\(L = \\sum_{i}^{N} \\left[\\|f(x_i^a) - f(x_i^p)\\|_2^2 - \\|f(x_i^a) - f(x_i^n)\\|_2^2 + \\alpha\\right]_+\\)</p>"},{"location":"cv/nerf/","title":"NeRF","text":""},{"location":"cv/nerf/#nerf-video","title":"NeRF Video","text":""},{"location":"cv/nerf/#code","title":"Code","text":""},{"location":"cv/objectTracking/","title":"Object Tracking","text":""},{"location":"cv/objectTracking/#object-tracking","title":"Object Tracking","text":"<p>To continuously locate and maintain the identity of target objects as they move through video frames.</p> <p>Tracking is similarity measurement, correlation, correspondence, Matching/retrieval &amp; data association.</p>"},{"location":"cv/objectTracking/#learining","title":"Learining","text":""},{"location":"cv/objectTracking/#appearance","title":"Appearance","text":"<p>we need to know how the target looks like - Single object tracking - Re-identification</p>"},{"location":"cv/objectTracking/#motion","title":"Motion","text":"<p>To make predictions of where the targets goes - Trajectory prediction</p>"},{"location":"cv/objectTracking/#single-target-tracking","title":"Single Target Tracking","text":""},{"location":"cv/objectTracking/#as-a-matchingcorrespondence-problem","title":"As a matching/correspondence problem","text":"<ul> <li>GOTURN: no online appearance modeling</li> </ul> <p>Input: what to track?</p> <p>Architecture: conv + concatenate + FC</p> <p>Pros: - No training, very fast as it is juct template matching problem</p> <p>Cons: - Does not work if the object moves very fast and goes out of search window.</p>"},{"location":"cv/objectTracking/#as-an-appearance-learning-problem","title":"As an appearance learning problem","text":"<ul> <li>MDNet: quick online finetuning of the network</li> <li>Slow: not suitable for real-time applications</li> <li>Solution: train as few layers as possible</li> </ul> <p>At test time, we need to train fc6 (up to fc4 if wanted)</p> <p>Pros: - No previous location assumption, the object can move anywhere in the image</p> <p>Cons: - Not as fast as GOTURN</p>"},{"location":"cv/objectTracking/#as-a-temporal-prediction-problem","title":"As a (temporal) prediction problem","text":"<ul> <li>ROLO = CNN + LSTM</li> </ul> <p>LSTM receives the heatmap for the object\u2019s position and the 4096 descriptor of the image</p>"},{"location":"cv/objectTracking/#challanges","title":"Challanges","text":"<ul> <li>Occlusions</li> <li>Viewpoint/pose/blur/illumination variations (in a few frames of a sequence)</li> <li>Background clutter</li> </ul>"},{"location":"cv/objectTracking/#multiple-object-tracing","title":"Multiple Object Tracing","text":""},{"location":"cv/objectTracking/#online-tracking","title":"Online Tracking","text":"<ul> <li>Processes two frames at a time</li> <li>For real-time applications</li> <li>Prone to drifting \u00e0 hard to recover from errors or occlusions</li> </ul> <p>Process - Track initialization (e.g. using a detector) - Prediction of the next position (motion model)     - Kalman filter     - Recurrent architecture     - constant velocity model (works really well at high framerates and without occlusions!)</p> <ul> <li>Matching predictions with detections (appearance model)<ul> <li>Bipartite matching</li> </ul> </li> </ul>"},{"location":"cv/objectTracking/#track-initialization","title":"Track initialization","text":"<p>Making a detector into a tracktor - Tracktor: a method trained as a detector but with tracking capabilities. - Where did the detection with ID1 go in the next frame? </p> <p>Two-Step Detector - Region Proposal - Regression</p> <p>Pros - Tracktor are online - We can train our model on still images - We can reuse an extremely well-trained regressor</p> <p>Cons - Confusion in crowded places as there is no notion of identification. - The track is killed if the target becomes occluded. - Will not work if the object or the camera has large motions.</p> <p>1st &amp; 2nd can be solved using ReID (Re-Identification). While 2nd &amp; 3rd can be solved using Motion model.</p> <p>Modeling Appearence : Re-ID Modeling Motion : Model Motion </p>"},{"location":"cv/objectTracking/#prediction-of-the-next-position","title":"Prediction of the next position","text":""},{"location":"cv/objectTracking/#matching-predictions-with-detections-appearance-model","title":"Matching predictions with detections (appearance model)","text":""},{"location":"cv/objectTracking/#offline-tracking","title":"Offline Tracking","text":"<ul> <li>Processes a batch of frames</li> <li>Good to recover from occlusions</li> <li>Not suitable for real-time applications</li> <li>Suitable for video analysis</li> </ul>"},{"location":"cv/objectTracking/#challanges_1","title":"Challanges","text":"<ul> <li>Multiple objects of the same type</li> <li>Heavy occlusions</li> <li>Appearance is often very similar</li> </ul>"},{"location":"cv/objectTracking/#applications","title":"Applications","text":"<ul> <li>Surveillance &amp; Security</li> <li>Traffic Monitoring</li> <li>Autonomous Vehicles</li> <li>Sports Analytics</li> <li>Human-Computer Interaction</li> <li>Medical Imaging</li> <li>Robotics</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/","title":"Bipartite Matching","text":""},{"location":"cv/objectTracking/BipartiteMatching/#bipartite-matching","title":"Bipartite Matching","text":"<ul> <li>Links detections with predictions using distance metrics</li> <li>Uses IoU, Pixel, or 3D distances between boxes</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#process","title":"Process","text":"<ul> <li>Calculate distances between boxes</li> <li>Apply Hungarian algorithm for optimal matching</li> <li>Set thresholds to handle missing/unsuitable matches</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#edge-cases","title":"Edge Cases","text":"<ul> <li>Missing prediction: Add dummy nodes</li> <li>No suitable match: Use cost threshold</li> <li>Unmatched boxes: Create new tracks</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#output","title":"Output","text":"<ul> <li>Matched pairs</li> <li>New tracks from unmatched detections</li> <li>Lost tracks from unmatched predictions</li> </ul>"},{"location":"cv/objectTracking/BipartiteMatching/#hungarian-algorithm","title":"Hungarian algorithm","text":"<p>Demo</p>"},{"location":"cv/objectTracking/KalmanFilter/","title":"Kalman Filter","text":""},{"location":"cv/opticalFlow/","title":"Optical Flow","text":""},{"location":"cv/opticalFlow/#optical-flow","title":"Optical Flow","text":"<p>Optical flow refers to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer (camera) and the scene. It helps understand how objects move in a sequence of images.</p> <p>The optical flow problem involves estimating a dense vector field where each vector represents the displacement of a pixel from one frame to the next. This displacement field captures both the magnitude and direction of motion for each point in the image.</p> <p>Key characteristics of optical flow: 1. Dense Motion Field: Unlike feature tracking which follows specific points, optical flow estimates motion for every pixel in the image 2. Temporal Consistency: Assumes that pixel intensities remain constant between consecutive frames (brightness constancy assumption) 3. Spatial Smoothness: Nearby pixels tend to move in similar ways (smoothness constraint)</p>"},{"location":"cv/opticalFlow/#methods","title":"Methods:","text":"<ul> <li>Patched Based<ol> <li>Lucas-Kanade</li> <li>Horn-Shunck</li> </ol> </li> <li>NN Based<ol> <li>FlowNet</li> </ol> </li> </ul>"},{"location":"cv/opticalFlow/#perception-of-motion","title":"Perception of Motion","text":"Figure 1: Illustration of optical flow and motion perception. The top left image shows a person running with a static camera (no camera motion), while the top right image shows the same person running but with the camera moving in the opposite direction. The bottom diagrams visualize the resulting optical flow vectors for each scenario. <p>Assuming Image intensity is constant.</p> <p>Brightness Constancy Equation:</p> <p>\\(I(x,y,t) \\approx I(x+dx,y+dy,t+dt)\\)</p> <p>Using Taylor Series Expansion:</p> <p>\\(I(x(t)+u.\\Delta t,y+v.\\Delta t) - I(x(t),y(t),t) \\approx 0\\)</p> <p>\\(I_x \\cdot u +  I_y \\cdot v + I_t = 0\\) (Brightness Constancy Constraint)</p> <p>\\([u, v]\\) is the optical flow.</p>"},{"location":"cv/opticalFlow/#lucas-and-kanade","title":"Lucas and Kanade","text":"<p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\(2(I_xu+ I_yv+ I_t)I_x = 2(I_xu+ I_yv+ I_t)I_y = 0\\)</p> <p>\\(\\begin{bmatrix} \\sum I_{x}^2 &amp; \\sum I_{x}I_{y} \\\\ \\sum I_{x}I_{y} &amp; \\sum I_{y}^2 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} \\sum I_{x}I_{t} \\\\ \\sum I_{y}I_{t} \\end{bmatrix}\\)</p> <p>Structural Tensor representation:</p> <p>\\(\\begin{bmatrix} T_{xx} &amp; T_{xy} \\\\ T_{xy} &amp; T_{yy} \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix} = - \\begin{bmatrix} T_{xt} \\\\ T_{yt} \\end{bmatrix}\\)</p> <p>$u = \\frac{T_{yt}T_{xy} - T_{xt}T_{yy}}{T_{xx}T_{yy} - T_{xy}^2} \\text{ and } v = \\frac{T_{xt}T_{xy} - T_{yt}T_{xx}}{T_{xx}T_{yy} - T_{xy}^2} $</p>"},{"location":"cv/opticalFlow/#issues","title":"Issues","text":"<ul> <li>Brightness constancy is not satisfied (Correlation based method could be used)</li> <li>A point may not move like its neighbors (Regularization based methods)</li> <li>The motion may not be small (Taylor does not hold!) (Multi-scale estimation could be used)</li> </ul>"},{"location":"cv/opticalFlow/#horn-schunck","title":"Horn &amp; Schunck","text":"<p>Global method with smoothness constraint to solve aperture problem</p> <p>\\(E(u,v) = \\int_{x,y} (I_xu+ I_yv+ I_t)^2 + \\alpha^2(|\\nabla u|^2 + |\\nabla v|^2) dxdy\\)</p> <p>\\(\\frac{\\partial E(u, v)}{\\partial u} = \\frac{\\partial E(u, v)}{\\partial v}  = 0\\)</p> <p>\\((I_xu+ I_yv+ I_t)I_x - \\alpha^2(|\\nabla u|)= (I_xu+ I_yv+ I_t)I_y + \\alpha^2(|\\nabla u|) = 0\\)</p>"},{"location":"cv/opticalFlow/#flownet","title":"FlowNet","text":"<p>End-to-end frame work to for optical flow prediction.</p>"},{"location":"cv/opticalFlow/#simplenet","title":"SimpleNet","text":"<p>Both input images together and feed them through a rather generic network, allowing the network to decide itself how to process the image pair to extract the motion information.</p>"},{"location":"cv/opticalFlow/#flownetcorr","title":"FlowNetCorr","text":"<p>First produce meaningful representations of the two images separately and then combine them on a higher level. <code>correlation layer</code> performs multiplicative patch comparisons between two feature maps.</p> <p>Correlation of 2 patches of size \\(K \\times K\\) is given by :</p> <p>\\(c(\\mathbf{x}_1, \\mathbf{x}_2) = \\sum_{\\mathbf{o} \\in [-k,k] \\times [-k,k]} \\langle \\mathbf{f}_1(\\mathbf{x}_1 + \\mathbf{o}), \\mathbf{f}_2(\\mathbf{x}_2 + \\mathbf{o}) \\rangle\\)</p> <p>where \\(K = 2k+1\\). Above equation is similar to convolution but it is convolution of one data with another instead of filter.</p>"},{"location":"cv/opticalFlow/#flownetrefine","title":"FlowNetRefine","text":"<p>The main ingredient are \u2018upconvolutional\u2019 layers, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. To perform the refinement, we apply the \u2018upconvolution\u2019 to feature maps, and concatenate it with corresponding feature maps from the \u2019contractive\u2019 part of the network and an upsampled coarser flow prediction (if available). </p> <p>This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps.</p>"},{"location":"cv/opticalFlow/#application","title":"Application","text":"<ol> <li>Motion based segmentation</li> <li>SfM</li> <li>Alignment (e.g., UAV analysis)</li> <li>Video Compression</li> <li>Object Tracking</li> <li>Deformation Analysis</li> </ol>"},{"location":"cv/poseEstimation/","title":"Pose Estimation","text":""},{"location":"cv/poseEstimation/#pose-estimation","title":"Pose Estimation","text":"<ul> <li>Estimate a 2D pose (x,y) coordinates for each joint from a RGB image.</li> <li>17 joints</li> <li>Challanges<ul> <li>occlusions</li> <li>clothing</li> <li>extreme poses</li> <li>viewpoint changes etc.</li> </ul> </li> </ul>"},{"location":"cv/poseEstimation/#direct-regression","title":"Direct Regression","text":"DeepPose"},{"location":"cv/poseEstimation/#heatmap-predicion","title":"HeatMap Predicion","text":"<ul> <li>Instead of prediction by regression, for each joint one predicts a full image with a heatmap of the joint location</li> <li>Powerful representation, easier to predict a confidence per location, rather than regress a value for the position</li> <li>Ground truth (GT) heatmap is constructed by placing a 2D Gaussian around the joint position (e.g. variance 1.5 pixels)</li> <li>Loss: MSE between predicted and GT heatmap</li> </ul>  Newell predicted heatmap  <p>Bringing the structure of the problem - Body parts are linked to each other - Body symmetries - Joint limits, e.g., elbow cannot bend backwards - Physical connectivity: elbow connected to wrist</p> <p>Using graphical models also allows us to find the pose of several targets</p>  DeepCut  <p>Alternatively one ca do two stage process</p> <ol> <li>Object Detection</li> <li>Pose Estimation</li> </ol>"},{"location":"cv/segmentation/","title":"Image Segmentatio","text":""},{"location":"cv/segmentation/#semantic-segmentation","title":"Semantic Segmentation","text":""},{"location":"cv/segmentation/#instace-based-segmentation","title":"Instace-Based Segmentation","text":""},{"location":"cv/segmentation/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Region-based<ul> <li>e.g., IoU or Jaccard index, F-measure  or Dice\u2019s coefficient,  weighted F-measure,</li> </ul> </li> <li>Boundary-based <ul> <li>CM, boundary F-measure, boundary IoU, boundary displacement error (BDE), Hausdorff distances, </li> </ul> </li> <li>Structure-based<ul> <li>S-measure, E-measure, </li> </ul> </li> <li>Confidence-based <ul> <li>MAE </li> </ul> </li> </ul>"},{"location":"cv/segmentation/#dichotomous-image-segmentation","title":"Dichotomous Image Segmentation","text":""},{"location":"cv/segmentation/#dis","title":"DIS","text":""},{"location":"cv/segmentation/#birefnet","title":"BiRefNet","text":""},{"location":"cv/segmentation/#methods","title":"Methods","text":""},{"location":"cv/segmentation/#pdfnet","title":"PDFNet","text":""},{"location":"cv/segmentation/#ben","title":"BEN","text":""},{"location":"cv/segmentation/BiRefNet/","title":"BiRefNet","text":""},{"location":"cv/segmentation/BiRefNet/#birefnet","title":"BiRefNet","text":"<p>Bilateral Reference for High-Resolution Dichotomous Image Segmentation</p> <ul> <li>Swin-Transformer Based</li> <li>inward reference(InRef) and an outward reference (OutRef)</li> </ul> <p>Two essential Module:</p> <ol> <li>Localization module (LM) : <ul> <li>object localization using global semantic information</li> <li>extract hierarchical features from vision transformer backbone, which are combined and squeezed to obtain corase predictions in low resolution in deep layers.</li> </ul> </li> <li>Reconstruction module (RM) : <ul> <li>hierarchical patches of images provide the source reference, and gradient maps serve as the target reference.</li> <li>the inward and outward references as bilateral references (BiRef), in which the source image and the gradient map are fed into the decoder at different stages.</li> </ul> </li> </ol>  BiRefNet Comparison"},{"location":"cv/segmentation/BiRefNet/#localization-module","title":"Localization Module","text":"BiRefNet Architecture  <ul> <li>Transformer Encoder extract the fearures at different stages i.e., \\(F_1^e, F_2^e, F_3^e\\) with resolution at 4,8,16,32. </li> <li>The features of the first four \\(\\{F_i^e\\}_{i=1}^3\\) are transferred to the corresponding decoder stages with lateral connections (1\u00d71 convolution layers).</li> <li>These features are stacked and concatenated in the last encoder block to generate \\(F^e\\) then fed into a classification module.</li> </ul> <p>To enlarge the receptive fields to cover features of large objects and focus on local features for high precision simultaneously Atrous Spatial Pyramid Pooling (ASPP)  is used for multi-context fusion.</p>"},{"location":"cv/segmentation/BiRefNet/#reconstruction-module","title":"Reconstruction Module","text":"BiRef Blocks  <ul> <li> <p>Small receptive field (RFs) lead to inadequate context information to locate the right target on a large background, whereas large RFs often result in insufficient feature extraction in detailed areas.</p> </li> <li> <p>To achieve balance, using reconstruction block (RB) in each BiRef block as a replacement for the vanilla residual blocks.</p> </li> <li> <p>In RB, we employ deformable convolutions with hierarchical receptive fields (i.e., 1\u00d71, 3\u00d73, 7\u00d77) and an adaptive average pooling layer to extract features with RFs of various scales.</p> </li> <li> <p>These features extracted by different RFs are then concatenated as \\(F_i^{\\theta}\\), followed by a 1\u00d71 convolution layer and a batch normalization layer to generate the output feature of RM \\(F_i^{d'}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#bilateral-reference","title":"Bilateral Reference","text":"<ul> <li> <p>inward reference(InRef) and an outward reference (OutRef)</p> </li> <li> <p>In InRef, images \\(I\\) with original high resolution are cropped to patches \\(\\{P_{k=1}^N\\}\\) of consistent size with the output features of the corresponding decoder stage.</p> </li> <li> <p>These patches are stacked with the original feature \\(F_i^{d+}\\) to be fed into the RM.</p> </li> <li> <p>In OutRef, we use gradient labels to draw more attention to areas of richer gradient information which is essential for the segmentation of fine structures.</p> </li> <li> <p>First, we extract the gradient maps of the input images as \\(G_i^{gt}\\). Meanwhile, \\(F_i^{\\theta}\\) is used to generate the feature \\(F_i^G\\) to produce the predicted gradient maps \\(\\hat{G}^i\\)</p> </li> <li> <p>It passes through a conv and a sigmoid layer and is used to generate the gradient referring attention \\(A_i^G\\), which is then multiplied by \\(F_i^{d'}\\) to generate output of the BiRef block as \\(F_{i\u22121}^{d}\\).</p> </li> </ul>"},{"location":"cv/segmentation/BiRefNet/#loss","title":"Loss","text":"<p>\\(L = L_{pixel} + L_{region} + L_{boundary} + L_{semantic} \\\\ = \\lambda_1 L_{BCE} + \\lambda_2 L_{IoU} + \\lambda_3 L_{SSIM} + \\lambda_4 L_{CE}\\)</p> <p>\\(L_{BCE} = -\\sum_{(i,j)} [G(i,j) \\log(M(i,j)) + (1-G(i,j)) \\log(1-M(i,j))]\\)</p> <p>\\(L_{IoU} = 1 - \\frac{\\sum_{r=1}^H \\sum_{c=1}^W M(i,j)G(i,j)}{\\sum_{r=1}^H \\sum_{c=1}^W [M(i,j)+G(i,j)-M(i,j)G(i,j)]}\\)</p> <p>\\(L_{SSIM} = 1 - \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\\)</p> <p>\\(L_{CE} = -\\sum_{c=1}^N y_{o,c}\\log(p_{o,c})\\)</p>"},{"location":"cv/segmentation/DIS/","title":"DIS","text":""},{"location":"cv/segmentation/DIS/#dis","title":"DIS","text":"<p>Dichotomous Image Segmentation (DIS) proposed IS-Net. IS-Net as 3 components</p> <ol> <li>ground truth (GT) encoder,</li> <li>image segmentation component (\\(U^2\\)-Net with an input convolution layer before its first encoder stage.)</li> <li>intermediate supervision strategy</li> </ol>"},{"location":"cv/segmentation/DIS/#1st-stage","title":"1st Stage","text":"<p>Self-supervised training of the GT-encoder</p> <p>\\(L_{gt} = \\sum_{d=1}^{D} \\lambda_{d}^{gt} BCE(F_{gt}(\\theta_{gt}, G)_d, G)\\)</p> <p>GT encoder will be frozen.</p>"},{"location":"cv/segmentation/DIS/#2nd-stage","title":"2nd Stage","text":"<p>High dimensional intermediate features</p> <p>\\(f_{D}^{G} = F_{gt}^{-}(\\theta_{gt}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <ul> <li> <p>\\(F_{gt}^{-}\\) represents the \\(F_{gt}\\)  without the last convolution layers for generating the probability maps.</p> </li> <li> <p>\\(F_{gt}^{-}\\) is to supervise those corresponding features \\(f_{D}^{I}\\) from the segmentation model \\(F_{sg}\\)</p> </li> </ul> <p>High dimensional intermediate features from image segmentation component</p> <p>\\(f_{I}^{G} = F_{sg}^{-}(\\theta_{sg}, G), D= \\{1,2,3,4,5,6\\}\\) </p> <p>Feature Consistency Loss (intermediate supervision)</p> <p>\\(L_{fs} = \\sum_{d=1}^{D} \\lambda_{d}^{fs} ||f_{d}^{I} - f_{d}^{G}||^2\\)</p> <p>\\(L_{sg} = \\sum_{d=1}^{D} \\lambda_{d}^{sg} BCE(F_{sg}(\\theta_{sg}, I), G)\\)</p> <p>Loss for \\(F_{sg}\\) is  \\(L = L_{fs} + L_{sg}\\)</p>  IS-Net"},{"location":"cv/segmentation/DIS/#results","title":"Results","text":"Result"},{"location":"cv/segmentation/DIS/#meric-human-correction-efforts-hce","title":"Meric : Human Correction Efforts (HCE)","text":""},{"location":"ml/","title":"Machine Learning","text":""},{"location":"ml/#machine-learning","title":"Machine Learning","text":"<ul> <li> Linear Regression</li> <li> Logistic Regression</li> <li> Naive Bayes</li> <li> Principal Component Analysis (PCA)</li> <li> Linear Discriminant Analysis (LDA)</li> <li> k-Nearest Neighbors (k-NN)</li> <li> k-means Clustering</li> <li> Support Vector Machine (SVM)</li> <li> Decision Tree</li> <li> Ensemble Method</li> <li> Perceptron</li> <li> Neural Network</li> <li> Convolutional Neural Network (CNN)</li> </ul>"},{"location":"ml/DTQn/","title":"Decision Tree","text":""},{"location":"ml/DTQn/#decision-tree","title":"Decision Tree","text":"<ul> <li> <p>explain random forest, bagging.</p> </li> <li> <p>what is random in random forest ?</p> </li> <li> <p>what is a weak learner ?</p> </li> <li> <p>what is a decision tree?</p> </li> <li> <p>How does split takes place in decision tree?</p> </li> <li> <p>how random forest is better than decision tree ?</p> </li> <li> <p>how to avoid overfitting ? Are decision tree in random forest overfitting ?</p> </li> <li> <p>how bias/variance helps random forest ?</p> </li> <li> <p>what is boosting, how does bias/variance work in it ?</p> </li> <li> <p>Gradient boosting tree, how the variance is low ?</p> </li> <li> <p>Given a single DT with accuracy of 70% what will be the accuracy of an ensemble model made with this tree(assume 3 trees in the ensemble model).</p> </li> <li> <p>Intuition behind increasing weights of examples in boosting.</p> </li> <li> <p>Percentage of unique samples in bagging.</p> </li> <li> <p>What is XGboost?</p> </li> <li> <p>Why boosting reduces bias?</p> </li> <li> <p>Why bagging reduces variance? Math and intuition</p> </li> <li> <p>How does boosting and random forest affect bias and variance.</p> </li> </ul>"},{"location":"ml/DecisionTree/","title":"Decision Tree","text":""},{"location":"ml/DecisionTree/#decision-tree","title":"Decision Tree","text":"<p>A supervised learning method used for: 1. Regression 2. Classification</p>"},{"location":"ml/DecisionTree/#terminology","title":"Terminology","text":"<ol> <li>Root Node: Starting point of the tree</li> <li>Decision Node: Internal node where a split occurs</li> <li>Leaf Node: Terminal node containing predictions</li> <li>Sub Tree: Part of the tree below a node</li> <li>Splitting: Process of dividing a node into two or more sub-nodes</li> </ol>"},{"location":"ml/DecisionTree/#core-concepts","title":"Core Concepts","text":"<ol> <li>Recursive Binary Splitting</li> <li>Greedy approach at each step</li> <li>Non-parametric method</li> <li>Can handle non-linear relationships</li> </ol>"},{"location":"ml/DecisionTree/#tree-construction","title":"Tree Construction","text":""},{"location":"ml/DecisionTree/#feature-selection-measures","title":"Feature Selection Measures","text":"<p>For Classification: 1. Gini Index: \\(\\sum_{k=1}^K p\u0302_k(1-p\u0302_k)\\)    - Measures node purity    - Range: [0,0.5] for binary    - 0 = pure node, 0.5 = equal distribution</p> <ol> <li>Entropy: -\\(\\sum_{k=1}^K p\u0302_k\\log(p\u0302_k)\\)</li> <li>Measures information gain</li> <li>Range: [0,log(k)]</li> <li> <p>0 = pure node</p> </li> <li> <p>Misclassification Error: 1 - max(p\u0302\u1d62)</p> </li> <li>Less used in practice</li> <li>Not sensitive enough for tree growth</li> </ol> <p>For Regression:</p> <p>For region \\(R_j\\), 1. RSS (Residual Sum of Squares) : \\(RSS = \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i -\\hat{y}_i)^2\\) 2. MSE (Mean Squared Error) : \\(MSE = \\frac{1}{N} \\sum_{j=1}^J \\sum_{i \\in R_j} (y_i -\\hat{y}_i)^2\\) 3. MAE (Mean Absolute Error) : \\(MAE = \\frac{1}{N} \\sum_{j=1}^J \\sum_{i \\in R_j} |y_i -\\hat{y}_i|\\)</p>"},{"location":"ml/DecisionTree/#building-process","title":"Building Process","text":"<ul> <li>Uses top-down, greedy approach (binary splitting)</li> <li>At each step, makes locally optimal split</li> <li>Prediction: Mean of response variable in each leaf</li> </ul> <p>Splitting Process 1. For each feature:    - For numerical: Find best splitting threshold    - For categorical:       * Binary: Two groups      * Multi-class: Consider all possible groupings 2. Calculate impurity measure for each split 3. Select split that maximizes information gain:    \\(IG = I(parent) - \\sum_{j=1}^m \\frac{N_j}{N}I(j)\\)    where I = impurity measure</p> <p>Algorithm Steps 1. Consider all predictors and possible cut points 2. Calculate RSS for each potential split 3. Select split with minimum RSS 4. Repeat until stopping criteria met</p> <p>Stopping Criteria 1. Minimum samples at internal node 2. Minimum samples at leaf node 3. Maximum depth of tree 4. Maximum number of leaf nodes</p>"},{"location":"ml/DecisionTree/#pruning","title":"Pruning","text":"<p>Cost complexity function: \\(\\sum_{m=1}^{|T|} \\sum_{i:x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|\\)</p> <p>where \\(|T|\\) is tree size and \\(\\alpha\\) is complexity parameter</p> <p>Pruning Process 1. Grow maximum size tree 2. Prune back using cost complexity</p>"},{"location":"ml/DecisionTree/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":"<p>Advantages - Interpretable and visualizable - Handles both numerical and categorical data - Minimal preprocessing needed - Captures non-linear relationships - Handles missing values well</p> <p>Disadvantages - High variance (unstable) - Prone to overfitting - Biased towards dominant classes - May create biased trees with imbalanced datasets</p>"},{"location":"ml/Ensemble/","title":"Ensemble","text":""},{"location":"ml/Ensemble/#bagging-bootstrap-aggregating","title":"Bagging (Bootstrap Aggregating)","text":"<p>Bootsrapping : Sample with replacement</p> <ul> <li>Creates multiple trees using bootstrap samples</li> <li>Uses all features</li> <li>No pruning (reduces bias)</li> <li>Reduces variance through averaging</li> <li>Problem: Creates correlated trees</li> </ul>"},{"location":"ml/Ensemble/#random-forest","title":"Random Forest","text":"<ul> <li>Extension of bagging</li> <li>Randomly selects subset of features at each split</li> <li>Feature subset size:<ul> <li>Classification: \\(\\sqrt{M}\\)</li> <li>Regression: \\(M/3\\)</li> <li>Can be reduced for correlated features</li> </ul> </li> </ul>"},{"location":"ml/Ensemble/#boosting","title":"Boosting","text":"<ul> <li>Sequential tree growth</li> <li>Each tree learns from previous errors</li> <li>Controls tree depth</li> <li> <p>Types:</p> <ol> <li> <p>Gradient Boosting</p> <ul> <li>Fits trees to residuals</li> <li>Slow learning procedure</li> <li>Uses gradient descent</li> </ul> </li> <li> <p>AdaBoost</p> <ul> <li>Adjusts observation weights</li> <li>Focus on misclassified instances</li> <li>Adaptive learning rate</li> </ul> </li> <li> <p>XGBoost</p> <ul> <li>Regularized gradient boosting</li> <li>Better control over overfitting</li> <li>Advanced features:</li> <li>L1 (Lasso) &amp; L2 (Ridge) regularization</li> <li>Handling missing values</li> <li>Tree pruning</li> </ul> </li> </ol> </li> </ul>"},{"location":"ml/KNN/","title":"k-NN","text":""},{"location":"ml/KNN/#k-nearest-neighbor-knn","title":"k-nearest neighbor (KNN)","text":"<p>It is non-parametric learning algorithm. It is mainly used for classification but also can be used for regression by averaging out the nearest value based on distance.</p>"},{"location":"ml/KNN/#process","title":"Process","text":"<ol> <li>Choose the number of k and a distance metric(Euclidean, Manhattan, Cosine etc.)</li> <li>Find the k-nearest neighbors of the data record that we want to classify</li> <li>Assign the class label by majority vote</li> </ol> <p>The right choice of k is crucial to finding a good balance between overfitting and underfitting.</p>"},{"location":"ml/KNN/#k","title":"k","text":"<p><code>Effect of k:</code> As k increases, variance decreases while bias increases. Conversely, as k decreases, variance increases while bias decreases.</p> <p><code>Choice of k:</code> Choosig based on validation error:</p>"},{"location":"ml/KNN/#key-points","title":"Key Points","text":"<ul> <li>It is a memory-based approach immediately adapts as we collect new training data. </li> <li>The computational complexity for classifying new examples grows linearly with the number of examples in the training dataset in the worst-case scenario.</li> <li>KNN is very susceptible to overfitting due to the curse of dimensionality (the closest neighbors as being too far away in a high-dimensional space to give a good estimate.). Regularization method cannot be applied here.</li> <li>All the features should be scaled as we will be taking distnace based on features.</li> <li>Optimization can be done through the dimensionality reduction by using method like PCA, LDA etc.</li> </ul>"},{"location":"ml/KNN/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/LinearRegression/","title":"Linear Regression","text":""},{"location":"ml/LinearRegression/#linear-regression","title":"Linear Regression","text":"<p>Linear Regression would be appropriate since we are predicting a continuous value.</p> <p>Linear Regression works when these 4 assumtion being followed:</p> <ol> <li> <p>Linearity: this means that the relationship must be linear between the independent variables and dependent variables.  \\(y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) or \\(y= \\beta_0 + \\beta_1 sin(x) + \\beta_2 cosx(x)\\)</p> </li> <li> <p>Homoscedasticity: Constant variance of residuals. </p> </li> <li> <p>Independence: independent variables (observations) are not highly correlated.</p> </li> <li> <p>Normality: Residuals are normally distributed for any fixed value of our observations </p> </li> </ol> <p>Note</p> <ul> <li> <p>Find the collinearity by using Variance Inflation Factors (VIF). VIF &gt; 5 variable are dependent.</p> </li> <li> <p>Solve collinearity by either removing one of the features or linearly combine both features. </p> </li> </ul>"},{"location":"ml/LinearRegression/#metrics","title":"Metrics","text":"<ul> <li> <p>Root Mean Square Error (RMSE) : Calculates the average of the squared difference between the predicted and actual values. Thus, larger errors (outliers or poor prediction) are flagged  more than when using MAE due to squaring errors. \\(RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2}{N}}\\)</p> </li> <li> <p>Mean Absolute Error (MAE) : Calculates the average of the absolute difference between the predicted and actual values. As a result, it does not punish large errors as much as RMSE. \\(MAE = \\frac{\\sum_{i=1}^{N}|y_i - \\hat{y_i}|}{N}\\)</p> </li> </ul>"},{"location":"ml/LinearRegression/#methods","title":"Methods","text":"<ol> <li> <p>Closed form solution</p> <ul> <li>\\(XW=y\\)</li> <li>\\(X^TXW=X^Ty\\)</li> <li>\\(w = (X^TX)^{-1}X^Ty\\)</li> <li>Useful, when optimal solution is needed. Issue when inverse does notexist and computationally expensive when data is too large.</li> </ul> </li> <li> <p>Optimization algorithm, typically Gradient Descent (GD) or Stochastic Gradient Descent (SGD).</p> <ul> <li>\\(\\text{L} = \\frac{1}{2} ||\\hat{y} - y||^2\\) where \\(\\hat{y} = X*W + b\\) </li> <li>\\(\\frac{\\partial L}{\\partial W} = X*(\\hat{y}- y)\\)</li> <li>\\(\\frac{\\partial L}{\\partial W} = \\hat{y}- y\\)</li> </ul> </li> </ol>"},{"location":"ml/LinearRegression/#feature-importance","title":"Feature Importance","text":"<p>If the features are normalized then the coefficients are an indication of feature importance, i.e. features with higher coefficients are more useful for  prediction.</p>"},{"location":"ml/LinearRegression/#prediction","title":"Prediction","text":"<p>\\(y = \\sum_{i}w_ix_i + b\\)</p>"},{"location":"ml/LinearRegression/#question","title":"Question","text":"Why linear regression is called linear? <ul> <li>The relationship between the independent variables (X) and dependent variable (Y) is expressed as a linear combination of parameters (coefficients).</li> <li>The coefficients (\u03b2) appear in the equation in a linear way: Y = \u03b2\u2080 + \u03b2\u2081X\u2081 + \u03b2\u2082X\u2082 + ... + \u03b2\u2099X\u2099</li> <li>These parameters are not raised to powers or modified by other functions.</li> <li>The variables themselves (X) can be non-linear (like X\u00b2, log(X), etc.)</li> <li>For example: Y = \u03b2\u2080 + \u03b2\u2081X\u00b2 is still a linear regression model because the coefficient \u03b2\u2081 is linear</li> </ul> Explain the concept of correlation between features, its implications, and potential problems in machine learning models, particularly in regression. <ul> <li>Measures linear relationship between variables</li> <li>Range: -1 to +1</li> <li>Shows how variables move together</li> <li>Problems with Correlated Features:<ul> <li>Multicollinearity in regression</li> <li>Unstable coefficients</li> <li>Reduced model interpretability</li> </ul> </li> </ul>"},{"location":"ml/LinearRegression/#code","title":"Code","text":"<p>Numpy</p> Closed FormGradient Form <pre><code>class LinearRegressionClosedForm:\n    def __init__(self):\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        bias = np.ones((n_samples, 1))\n        X_new = np.column_stack((X, bias))\n        W = np.linalg.inv(X_new.T @ X_new) @ X_new.T @ y\n        self.weights = W[:-1]\n        self.bias = W[-1]\n\n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n</code></pre> <pre><code>class LinearRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # gradient descent\n        for _ in range(self.n_iters):\n            y_predicted = np.dot(X, self.weights) + self.bias\n            # compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n\n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        y_approximated = np.dot(X, self.weights) + self.bias\n        return y_approximated\n</code></pre> <p>Pytorch</p> Parameter BasedLinear Layer Based <pre><code>class LinearRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegression, self).__init__()\n        self.weights = nn.Parameter(torch.randn(input_dim, output_dim, requires_grad=True))\n        self.bias = nn.Parameter(torch.randn(1, output_dim, requires_grad=True))\n\n    def forward(self, x):\n        return  torch.matmul(x, self.weights) + self.bias\n</code></pre> <pre><code>class LinearRegressionV2(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearRegressionV2, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return  self.linear(x)\n</code></pre>"},{"location":"ml/LogisticRegression/","title":"Logistic Regression","text":""},{"location":"ml/LogisticRegression/#logistic-regression","title":"Logistic Regression","text":"<p>It is used for te classification problem. It uses linear regressing equaition to predict the class probabilities.</p> <p>Equation:</p> <p>\\(y = wx+b\\)</p> <p>This \\(y\\) is feed to the sigmoid function to get the output between 0 and 1 as probailities. So,</p> <p>\\(y = \\frac{1}{1+ e^{-(wx+b)}}\\)</p> <p>Logistic regression doesn't require: - Normality of residuals - Homoscedasticity</p> <p>Logistic regression specifically requires: - Binary/categorical outcome - Linear relationship with log odds (not the outcome itself)</p>"},{"location":"ml/LogisticRegression/#effect-of-outlier","title":"Effect of Outlier","text":"<p>Since here we focus on finding the decision boundry that linearly seperate the classes. So we mostly focus on the points which are closer to the boundry. Therefore, outlier will have very less effect here.</p>"},{"location":"ml/LogisticRegression/#logistic-regression-as-maximum-likelihood-estimationmle","title":"Logistic Regression as Maximum Likelihood Estimation(MLE)","text":"<p>Assuming the Bernoulli distribution (i.e., binary classification). Let, \\(y \\in \\{0,1\\}\\) if \\(p\\) is the probability of class as 1. Then according to MLE we need to maximize \\(p^y\\) if class is 1 and \\((1-p)^{(1-y)}\\) if class is 0.</p> <p>\\(L = \\Pi_{i=1}^{N} p_i^{y_i} (1-p_i)^{(1-y_i)}\\)</p> <p>Multipying such large number may result in the overflow. So take <code>log</code> on both side.</p> <p>\\(L = \\sum_{i=1}^{N} (y_i \\ln p_i + (1 - y_i) \\ln (1 - p_i))\\)</p> <p>\\(Loss = -Likelihood\\)</p> <p>This loss penelizes much more than MSE when prediciton is wrong.</p>"},{"location":"ml/LogisticRegression/#optimization","title":"Optimization","text":"<ul> <li>\\(z^{(i)} = wx^{(i)} + b\\)<ul> <li>\\(\\frac{\\partial z^{(i)}}{\\partial w} = x^{(i)}\\) </li> </ul> </li> <li>\\(\\hat{y}^{(i)} = \u03c3(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}\\)<ul> <li>\\(\\frac{\\partial \\hat{y}^{(i)}}{\\partial z^{(i)}} = \\hat{y}^{(i)}(1-\\hat{y}^{(i)})\\)</li> </ul> </li> <li>\\(J(w,b) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]\\)<ul> <li>\\(\\frac{\\partial J}{\\partial \\hat{y}^{(i)}} = -\\frac{1}{m} [\\frac{y^{(i)}}{\\hat{y}^{(i)}} - \\frac{1-y^{(i)}}{1-\\hat{y}^{(i)}}] = -\\frac{1}{m} [\\frac{y^{(i)} - \\hat{y}^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}]\\)</li> </ul> </li> <li> <p>\\(\\frac{\\partial J}{\\partial w} = \\frac{\\partial J}{\\partial \\hat{y}^{(i)}} \\cdot \\frac{\\partial \\hat{y}^{(i)}}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial w}\\)</p> </li> <li> <p>\\(\\frac{\\partial J}{\\partial w} =  -\\frac{1}{m} [\\frac{y^{(i)} - \\hat{y}^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}] \\cdot \\hat{y}^{(i)}(1-\\hat{y}^{(i)}) \\cdot x^{(i)}\\)</p> <ul> <li>\\(\\frac{\\partial J}{\\partial w} =  -\\frac{1}{m} (y^{(i)} - \\hat{y}^{(i)}) \\cdot x^{(i)}\\)</li> </ul> </li> <li>\\(\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})x^{(i)}\\)</li> </ul> <p>Similarly,</p> <ul> <li>\\(\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\\)</li> </ul>"},{"location":"ml/LogisticRegression/#prediction","title":"Prediction","text":"<p>Here, \\(pred = \\frac{1}{1+ e^{-(wx+b)}}\\), if \\(pred &gt; \\tau\\) then class 1 else class 0.</p> <p>\\(\\tau\\) is decided according to problem statement.</p>"},{"location":"ml/LogisticRegression/#multi-class-n","title":"Multi-Class (N)","text":"<ol> <li> <p>One-vs-all: We need to have N models </p> <p>\\(pred = \\text{argmax}_{i} f_{i}(x)\\)</p> </li> <li> <p>One-vs-one: We need to have \\(\\binom{N}{2}\\) models, where each model is trained to distinguish between a pair of classes. For N classes, this results in \\(\\frac{N(N-1)}{2}\\) binary classifiers. The prediction is made by majority voting across all pairwise comparisons:</p> <p>\\(pred = argmax_{i} \\sum_{j \\neq i} \\mathbb{I}(f_{ij}(x) = i)\\)</p> <p>where \\(f_{ij}(x)\\) is the binary classifier for classes i and j, and \\(\\mathbb{I}\\) is the indicator function.</p> </li> <li> <p>Mathematical: Use softmax instead of sigmoid and use cross-entropy loss.</p> </li> </ol>"},{"location":"ml/LogisticRegression/#question","title":"Question","text":"Why logistic regression is a classifier and not regression? <ul> <li>Logistic regression outputs probabilities between 0 and 1</li> <li>These probabilities are then converted to binary classes (0 or 1) using a threshold \\(\\tau\\)</li> </ul> Why do we use cross-entropy instead of mean square errors in logistic regression? <ul> <li>Cross-entropy gives larger gradients for wrong predictions, leading to faster and better learning, especially when predictions are far from actual values.</li> </ul> How to extend the sigmoid function for multi-class classification? <ul> <li>By using softmax</li> </ul> Suppose you have a logistic regression model as a black box. How can you determine the weights? <ul> <li>We can determine the weights using n+1 strategic queries (where n is the number of features).<ul> <li>Finding Bias Term (b)<ul> <li>First, input a zero vector (all features set to 0)</li> <li>The output will give us the bias term as there's no contribution from any weights</li> </ul> </li> <li>Finding Individual Weights<ul> <li>Use the columns of an identity matrix as inputs</li> <li>When we input [1,0,0,...], the output will reflect bias + weight1 and so on.</li> </ul> </li> </ul> </li> </ul> BCE loss is convex function? <ul> <li> <p>For Linear Model:</p> <ul> <li>BCE with respect to w and b is convex<ul> <li>Sigmoid is monotonic</li> <li>Log loss is convex</li> <li>Composition maintains convexity here</li> </ul> </li> </ul> </li> <li> <p>For Neural Networks:</p> <ul> <li>BCE is NOT convex<ul> <li>Non-linear activation functions</li> <li>Multiple layers</li> <li>Complex compositions</li> </ul> </li> </ul> </li> </ul>"},{"location":"ml/LogisticRegression/#code","title":"Code","text":"NumpyPyTorch <pre><code>class LogisticRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        # gradient descent\n        for _ in range(self.n_iters):\n            # approximate y with linear combination of weights and x, plus bias\n            linear_model = np.dot(X, self.weights) + self.bias\n            # apply sigmoid function\n            y_predicted = self._sigmoid(linear_model)\n\n            # compute gradients\n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n            # update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n\n    def predict(self, X):\n        linear_model = np.dot(X, self.weights) + self.bias\n        y_predicted = self._sigmoid(linear_model)\n        y_predicted_cls = [1 if i &gt; 0.5 else 0 for i in y_predicted]\n        return np.array(y_predicted_cls)\n\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n</code></pre> <pre><code>class LogisticRegression(nn.Module):\n    def __init__(self, input_features):\n        super(LogisticRegression, self).__init__()\n        self.layer1 = nn.Linear(input_features, 8)\n        self.layer2 = nn.Linear(8, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.bn = nn.BatchNorm1d(8)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = torch.sigmoid(self.layer2(x))\n        return x\n</code></pre>"},{"location":"ml/MATH/","title":"Co-Variance:","text":""},{"location":"ml/MATH/#co-variance","title":"Co-Variance:","text":"<p>Covariance is a measures the extent two features either increase or decrease with each other (range ( \u2212 \u221e , \u221e ) ). A covariance score of 0 indicates that both features are not related. If the covariance score is positive it means that both features increase in the same direction and a negative score indicates an inverse relationship between the two features. </p> <p>A Covariance matrix is used in PCA, Gaussian mixture models (GMMs) and Mahalanobis Distance. </p>"},{"location":"ml/MATH/#correlation","title":"Correlation","text":"<p>Correlation lets us know the strength and direction of the two features (range ( \u2212 1 , 1 ) ). If we say two features are correlated, then we can say that a change in one feature creates an impact/change in another variable. A positive correlation indicates that as one feature increases the other will increase, a correlation score of 0 indicates no relationship between variables and a negative correlation indicates that as one feature increases the other will  decrease. </p> <p>Correlation is just in large amounts of data is to find patterns, e.g. correlated features within a dataset. </p>"},{"location":"ml/MATH/#bayes-equation","title":"Bayes Equation","text":""},{"location":"ml/NaiveBayes/","title":"Naive Bayes","text":""},{"location":"ml/NaiveBayes/#naive-bayes","title":"Naive Bayes","text":"<p>Naive Bayes classifiers have a general assumption that the effect of an attribute value on a given class in independent of the values of the other attributes. This assumption is called class-conditional independence.</p> <p>\\(P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\\)</p> <p>\\(\\text{posterior} = \\frac{\\text{likelihood prob} * \\text{prior}}{\\text{marginal}}\\)</p> <p>In the context of Naive Bayes classification, we often work with log probabilities to avoid numerical underflow. The log posterior probability is calculated as:</p> <p>\\(\\log P(A|B) = \\log P(B|A) + \\log P(A) - \\log P(B)\\)</p> <p>The denominator \\(P(B)\\) is omitted since it's constant across classes and doesn't affect the \\(\\text{argmax}\\)</p> <p>\\(\\log P(A|B) = \\log P(B|A) + \\log P(A)\\)</p> <p>\\(posterior = posterior + prior\\)</p> <p>This matches the implementation shown in the code where: - \\(\\log P(A)\\) is the prior probability - \\(\\log P(B|A)\\) is calculated as the sum of log probabilities from the PDF</p>"},{"location":"ml/NaiveBayes/#additional-details","title":"Additional Details","text":"<p>Prior Probability[P(A)]: The prior probability represents our initial belief about the probability of each class before seeing any evidence. It's calculated by dividing the number of instances of a particular class by the total number of instances in the training dataset.</p> <p>Likelihood Probability[P(B|A)]: This represents the probability of observing the features given a particular class. It measures how likely we are to see these features if the class is true.</p> <p>Marginal Probability[P(B)]: This is the probability of observing the features regardless of the class. It acts as a normalizing constant to ensure our probabilities sum to 1.</p> <p>Posterior Probability[P(A|B)]: This is our final probability of a class given the observed features. It's calculated by multiplying the likelihood by the prior and dividing by the marginal probability.</p> <p>\\(P(x|c) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}} e^{-\\frac{(x-\\mu_c)^2}{2\\sigma_c^2}}\\)</p> <p>we assume that the data follows Gaussian distribution, due  to which the probability of an item, given a class label c, can be defined as</p>"},{"location":"ml/NaiveBayes/#advantages","title":"Advantages","text":"<ol> <li>Simple and fast to train and predict</li> <li>Works well with small datasets</li> <li>Can handle both continuous and discrete features</li> <li>Less prone to overfitting</li> </ol>"},{"location":"ml/NaiveBayes/#disadvantages","title":"Disadvantages","text":"<ol> <li>Assumes features are independent (naive assumption)</li> <li>May not perform well when features are correlated</li> <li>Requires features to be normally distributed for optimal performance</li> <li>Sensitive to feature scaling</li> </ol>"},{"location":"ml/NaiveBayes/#laplacian-smoothing","title":"Laplacian Smoothing","text":"<p>Laplacian smoothing (also known as additive smoothing) is a technique used to handle zero probability problems in Naive Bayes classification. It adds a small constant \u03b1 to the count of each feature-class combination to prevent zero probabilities.</p> <p>For a feature value x and class c, the smoothed probability is calculated as:</p> <p>\\(P(x|c) = \\frac{count(x,c) + \\alpha}{count(c) + \\alpha|V|}\\)</p> <p>Where</p> <ul> <li>count(x,c) is the number of times feature x appears with class c</li> <li>count(c) is the number of instances of class c</li> <li>\u03b1 is the smoothing parameter (typically 1)</li> <li>|V| is the size of the vocabulary (number of unique feature values)</li> </ul>"},{"location":"ml/NaiveBayes/#question","title":"Question","text":"Why Naive Bayes is called Naive? <ul> <li>Simplified assumption that all features in a dataset are independent of each other, even though in real-world scenarios this is rarely true.</li> <li>\\(P(y|x\u2081,x\u2082,...,x\u2099) = \\frac{P(x\u2081|y) \u00d7 P(x\u2082|y) \u00d7 ... \u00d7 P(x\u2099|y) \u00d7 P(y)}{P(X)} = \\frac{P(y) \\prod_{i=1}^{n} P(x_i|y)}{P(X)}\\)</li> </ul>"},{"location":"ml/NaiveBayes/#code","title":"Code","text":"NumpyPyTorch <pre><code>class NaiveBayes:\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self._classes = np.unique(y)\n        n_classes = len(self._classes)\n\n        # calculate mean, var, and prior for each class\n        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n        self._priors = np.zeros(n_classes, dtype=np.float64)\n\n        for idx, c in enumerate(self._classes):\n            X_c = X[y == c]\n            self._mean[idx, :] = X_c.mean(axis=0)\n            self._var[idx, :] = X_c.var(axis=0)\n            self._priors[idx] = X_c.shape[0] / float(n_samples)\n\n    def predict(self, X):\n        y_pred = [self._predict(x) for x in X]\n        return np.array(y_pred)\n\n    def _predict(self, x):\n        posteriors = []\n\n        # calculate posterior probability for each class\n        for idx, c in enumerate(self._classes):\n            prior = np.log(self._priors[idx])\n            posterior = np.sum(np.log(self._pdf(idx, x)))\n            posterior = prior + posterior\n            posteriors.append(posterior)\n\n        # return class with highest posterior probability\n        return self._classes[np.argmax(posteriors)]\n\n    def _pdf(self, class_idx, x):\n        mean = self._mean[class_idx]\n        var = self._var[class_idx]\n        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n        denominator = np.sqrt(2 * np.pi * var)\n        return numerator / denominator\n</code></pre> <pre><code>\n</code></pre>"},{"location":"ml/SVM/","title":"SVM","text":""},{"location":"ml/SVM/#svm","title":"SVM","text":"<p>Support Vector Machine</p> <ul> <li>Used for both classification and regression tasks.</li> <li>Core idea: Find optimal hyperplane that maximizes margin between classes.</li> </ul>"},{"location":"ml/SVM/#geometric-interpretation","title":"Geometric Interpretation","text":"<ul> <li>There are many hyperplanes that separate positive and negative points.</li> <li>We want to find the margin-maximizing hyperplane.</li> <li>Margin is the perpendicular distance between two hyperplanes \\(H^{+}\\) and \\(H^{-}\\).</li> <li>As margin increases, generalization ability increases.</li> <li>Points through which \\(H^{+}\\) or \\(H^{-}\\) pass are called support vectors.</li> </ul>"},{"location":"ml/SVM/#alternate-geometric-interpretation","title":"Alternate Geometric Interpretation","text":"<ul> <li>Find the convex hull for positive and negative points.</li> <li>Find the shortest line connecting these hulls.</li> <li>The plane bisecting this line perpendicularly gives the margin maximizer.</li> </ul>"},{"location":"ml/SVM/#derivation","title":"Derivation","text":"<ul> <li>\\(H\\): Margin Maximizing hyperplane</li> <li>\\(H\\): \\(w^T x + b = 0\\) </li> <li>\\(H^{+}\\): \\(w^T x + b = 1\\) (positive class boundary)</li> <li>\\(H^{-}\\): \\(w^T x + b = -1\\) (negative class boundary)</li> <li>Margin: \\(d = \\frac{2}{||w||}\\) (derived from geometric distance formula)</li> </ul> <p>Problem Statement</p> <ul> <li>For linearly separable data:</li> <li>Find \\(w^{\\star}\\) and \\(b^{\\star}\\) such that \\(\\frac{2}{||w||}\\) is maximized</li> <li>Subject to: \\(y_i(w^T x_i + b) \\ge 1 \\text{  } \\forall i\\) </li> <li> <p>\\(w^{\\star}, b^{\\star} = \\text{arg max}_{w,b} \\frac{2}{||w||} = \\text{arg min}_{w,b} \\frac{||w||^2}{2}\\)</p> </li> <li> <p>For non-linearly separable data:</p> </li> <li>Introduce slack variables \\(\\xi_i \\ge 0\\)</li> <li>\\(\\xi_i = 0\\) for correctly classified points</li> <li>\\(\\xi_i &gt; 0\\) measures violation of margin</li> <li>Objective: \\(w^{\\star}, b^{\\star} = \\text{arg min}_{w,b} \\frac{||w||^2}{2} + C \\sum_i^N \\xi_i\\)</li> <li>\\(C\\): regularization parameter (higher \\(C\\) \u2192 stricter margin)</li> </ul>"},{"location":"ml/SVM/#loss-minimization-hinge-loss","title":"Loss minimization: Hinge Loss","text":"<ul> <li>\\(L_{hinge}(y_i, f(x_i))=\\max(0, 1-y_i f(x_i))\\) where \\(f(x_i) = w^T x_i + b\\)</li> <li>When \\(y_i f(x_i) \\ge 1\\): Loss \\(= 0\\) (correct classification with margin)</li> <li>When \\(y_i f(x_i) &lt; 1\\): Loss \\(&gt; 0\\) (violation of margin)</li> <li>Complete objective: \\(\\mathcal{L} = \\min_{w,b} \\frac{1}{n}\\sum_i^N \\max(0, 1 - y_i(w^T x_i +b)) + \\lambda ||w||^2\\)</li> </ul>"},{"location":"ml/SVM/#optimization","title":"Optimization","text":"<ul> <li>Gradient descent can be used to minimize hinge loss</li> <li>Challenging due to non-differentiability at \\(y_i(w^T x_i + b) = 1\\)</li> <li>Sub-gradients are used at non-differentiable points</li> </ul> <p>Gradient Computation:</p> <p>\\(\\frac{\\partial J}{\\partial w} = \\lambda w - \\frac{1}{n}\\sum_{i=1}^n y_i x_i \\cdot I(y_i(w^T x_i + b) &lt; 1)\\)</p> <p>\\(\\frac{\\partial J}{\\partial b} = -\\frac{1}{n}\\sum_{i=1}^n y_i \\cdot I(y_i(w^T x_i + b) &lt; 1)\\)</p> <p>where \\(I()\\) is the indicator function.</p> <p>Gradient Updates:</p> <ol> <li>For points outside margin (\\(y_i(w^T x_i + b) \\geq 1\\)):</li> <li>Only regularization affects update</li> <li>\\(w = w - \\eta \\lambda w\\)</li> <li> <p>\\(b\\) remains unchanged</p> </li> <li> <p>For margin violations (\\(y_i(w^T x_i + b) &lt; 1\\)):</p> </li> <li>Both loss and regularization affect update</li> <li>\\(w = w - \\eta(\\lambda w - y_i x_i)\\)</li> <li>\\(b = b + \\eta y_i\\)</li> </ol> <p>where \\(\\eta\\) is the learning rate.</p>"},{"location":"ml/SVM/#dual-form-of-svm","title":"Dual form of SVM","text":"<p>Primal \\(\\min_{w,b,\\xi} \\frac{||w||^2}{2} + C \\sum_i^N \\xi_i\\) subject to: \\(y_i(w^T x_i + b) \\ge 1 - \\xi_i\\) and \\(\\xi_i \\ge 0\\) \\(\\forall i\\)</p> <p>Dual \\(\\max_{\\alpha} \\sum_i^N \\alpha_i - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j x_i^T x_j\\) subject to: \\(\\sum_i^N \\alpha_i y_i = 0\\) and \\(0 \\le \\alpha_i \\le C\\) \\(\\forall i\\)</p>"},{"location":"ml/SVM/#kernel-svm","title":"Kernel SVM","text":"<ul> <li>For non-linearly separable data in input space, map to higher dimensional feature space</li> <li>Explicit mapping \u03c6(x) is computationally expensive</li> <li>Kernel trick: K(x,y) = \u03c6(x)\u1d40\u03c6(y)</li> <li>Replace dot products with kernel function in dual form</li> </ul> <p>Dual form with Kernel \\(\\max_{\\alpha} \\sum_i^N \\alpha_i - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j K(x_i,x_j)\\) subject to: \\(\\sum_i^N \\alpha_i y_i = 0\\) and \\(0 \\le \\alpha_i \\le C\\)</p> <p>Prediction \\(f(x) = \\text{sign}(\\sum_i \\alpha_i y_i K(x_i,x) + b)\\)</p>"},{"location":"ml/SVM/#mercers-theorem","title":"Mercer's Theorem","text":"<ul> <li>Kernel \\(K(x,y)\\) is valid if and only if:</li> <li>Symmetric: \\(K(x,y) = K(y,x)\\)</li> <li>Positive semi-definite: \\(\\int\\int K(x,y)g(x)g(y)dxdy \\ge 0\\) for all g</li> <li>Ensures existence of feature space mapping \\(\\phi\\)</li> <li>Guarantees convergence of kernel optimization</li> </ul>"},{"location":"ml/SVM/#common-kernels","title":"Common Kernels","text":"<p>Linear Kernel</p> <ul> <li>\\(K(x,y) = x^T y\\)</li> <li>Equivalent to no transformation</li> <li>Used when data is linearly separable</li> </ul> <p>Polynomial Kernel</p> <ul> <li>\\(K(x,y) = (\\gamma x^Ty + r)^d\\)</li> <li>Parameters: degree \\(d\\), \\(\\gamma &gt; 0, r \\ge 0\\)</li> <li>Maps to space of polynomials up to degree d</li> <li>Captures feature interactions</li> </ul> <p>RBF (Gaussian) Kernel</p> <ul> <li>\\(K(x,y) = exp(-\\gamma||x-y||^2)\\)</li> <li>Parameter: \\(\\gamma &gt; 0\\) controls spread</li> <li>Maps to infinite dimensional space</li> <li>Most commonly used for non-linear data</li> <li>\\(\\gamma\\) large \\(\\rightarrow\\) high variance, low bias</li> <li>\\(\\gamma\\) small \\(\\rightarrow\\) low variance, high bias</li> </ul>"},{"location":"ml/SVM/#inference","title":"Inference","text":"<ul> <li>\\(f(x) = \\text{sign}(\\sum_{i \\in SV} \\alpha_i y_i K(x_i,x) + b)\\)</li> <li>Computational cost depends on number of SVs</li> </ul>"},{"location":"ml/SVM/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/TODO/","title":"TODO","text":"<ul> <li> Write Code for multiclass, SGD logistic regression.</li> <li> Variance Inflation Factors (VIF)</li> <li> (Overfitting) Reduce the number of features: We can use some feature selection methods \u2013 filter-based (chi-square), wrapper based (Recursive Feature Elimination) or embedded  like Lasso regularization. </li> <li> SMOTE(data imbalance)</li> <li>number of principle component (PCA)</li> <li> <p> GMM code</p> </li> <li> <p>Complete SVR in SVM etc</p> </li> </ul>"},{"location":"ml/clustering/","title":"Clustering","text":""},{"location":"ml/clustering/#clustering","title":"Clustering","text":""},{"location":"ml/clustering/#question","title":"Question","text":"<ul> <li>what is K-mean and gaussian mixture model ? what is hard/soft clustering ? what is EM ?</li> <li>how will you evaluate clustering ?</li> <li>How you will perform clustering in a distributed environment(simultaneously on multiple machines). (IMP)</li> <li>Example where K means fails and Spectral Works</li> </ul>"},{"location":"ml/clustering/DBSCAN/","title":"DBSCAN","text":""},{"location":"ml/clustering/DBSCAN/#dbscan","title":"DBSCAN","text":"<p>DBSCAN is a density-based clustering algorithm.</p>"},{"location":"ml/clustering/DBSCAN/#process","title":"Process:","text":"<p>Parameters: \u03b5 (epsilon), n (min points)</p> <ol> <li>Start with an arbitrary point p from the dataset</li> <li>Find all points within \u03b5 radius of point p (\u03b5-neighbors)</li> <li>If number of \u03b5-neighbors \u2265 n:</li> <li>Create a new cluster</li> <li>Add point p and its \u03b5-neighbors to the cluster</li> <li>For each neighbor point:<ul> <li>Find its \u03b5-neighbors</li> <li>If \u2265 n neighbors, add them to the cluster</li> <li>Continue expanding until no more points can be added</li> </ul> </li> <li>Mark processed points as visited</li> <li>Repeat steps 1-4 with unvisited points until all points are processed</li> </ol> <p>Pros: Arbitrary cluster shapes</p> <p>Cons: Two parameters to tune and fixed \u03b5 can't handle varying densities</p>"},{"location":"ml/clustering/DBSCAN/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/GMM/","title":"GMM","text":""},{"location":"ml/clustering/GMM/#gaussian-mixture-models-gmm","title":"Gaussian Mixture Models (GMM)","text":"<ul> <li>Soft clustering algorithm where data points can belong to multiple clusters with probability scores</li> <li>Models data as a mixture of K Gaussian distributions</li> <li>Each cluster is represented by a Gaussian distribution with its own parameters</li> </ul>"},{"location":"ml/clustering/GMM/#key-components","title":"Key Components","text":"<ol> <li>Parameters for each Gaussian:</li> <li>Mean (\u03bc)</li> <li>Covariance matrix (\u03a3)</li> <li>Weight/mixing coefficient (\u03c0)</li> </ol>"},{"location":"ml/clustering/GMM/#process","title":"Process","text":"<ol> <li>Initialization:</li> <li>Randomly initialize parameters for K Gaussians</li> <li> <p>Set initial weights, means, and covariance matrices</p> </li> <li> <p>Expectation-Maximization (EM):</p> </li> </ol> <p>a) Expectation Step (E-step):    - Calculate probability of each data point belonging to each cluster    - Compute posterior probabilities (responsibilities)</p> <p>b) Maximization Step (M-step):    - Update Gaussian parameters using weighted averages    - Recalculate means, covariances, and mixing coefficients</p> <ol> <li>Convergence:</li> <li>Repeat E-step and M-step until parameters converge</li> <li>Monitor log-likelihood for convergence criteria</li> </ol>"},{"location":"ml/clustering/GMM/#advantages","title":"Advantages","text":"<ul> <li>Provides soft assignments (probabilities)</li> <li>Can model elliptical clusters</li> <li>More flexible than K-means</li> <li>Handles overlapping clusters well</li> </ul>"},{"location":"ml/clustering/GMM/#limitations","title":"Limitations","text":"<ul> <li>Computationally more expensive than K-means</li> <li>Sensitive to initialization</li> <li>May converge to local optima</li> </ul>"},{"location":"ml/clustering/GMM/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/HDBSCAN/","title":"HDBSCAN","text":""},{"location":"ml/clustering/HDBSCAN/#hdbscan","title":"HDBSCAN:","text":"<ul> <li>Only needs n parameter</li> <li>Handles varying densities</li> <li>Slower than k-means but more versatile</li> <li>Recommended as first clustering approach</li> </ul>"},{"location":"ml/clustering/HDBSCAN/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/HierarchicalClustering/","title":"Hierarchical Clustering","text":""},{"location":"ml/clustering/HierarchicalClustering/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<ul> <li>Agglomerative (bottom-up): Similar to flood filling, starts with individual points and merges closest pairs</li> <li>Divisive (top-down): Starts with all points in one cluster and recursively splits</li> </ul>"},{"location":"ml/clustering/HierarchicalClustering/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/clustering/KMeans/","title":"k-Means","text":""},{"location":"ml/clustering/KMeans/#k-means-clustering","title":"K-means Clustering","text":""},{"location":"ml/clustering/KMeans/#process","title":"Process","text":"<ul> <li>Initialize number of k</li> <li>Randomly choose k points in the data as centroids </li> <li>While points do not change: (centroids do not change or max number of iter)<ul> <li>Assign each data point to its nearest centroid.</li> <li>Recompute the centroids of each cluster </li> </ul> </li> </ul>"},{"location":"ml/clustering/KMeans/#k","title":"K","text":""},{"location":"ml/clustering/KMeans/#optimal-k","title":"Optimal K","text":"<p>To find the optimal number of clusters (k), we use the Elbow Method: 1. Calculate the Within-Cluster Sum of Squares (WSS) for different values of k 2. Plot WSS vs k 3. Look for the \"elbow\" point - where increasing k starts giving diminishing returns 4. Choose k at this elbow point</p>"},{"location":"ml/clustering/KMeans/#advantages","title":"Advantages","text":"<ol> <li> <p>Simple and Intuitive: Easy to understand and implement; works well for basic clustering tasks; widely used in practice.</p> </li> <li> <p>Fast and Efficient: Linear time complexity O(nkd) where n is samples, k is clusters, d is dimensions; scales well with large datasets.</p> </li> <li> <p>Memory Efficient: Only stores centroids and cluster assignments; minimal memory requirements compared to other clustering methods.</p> </li> </ol>"},{"location":"ml/clustering/KMeans/#drawbacks","title":"Drawbacks","text":"<ol> <li> <p>Sensitive to Initialization: Results vary based on initial centroid positions, may get stuck in local optima; solution is to run multiple times with different initializations.</p> </li> <li> <p>Assumes Spherical Clusters: Works best with spherical, similarly sized clusters; struggles with elongated or irregular shapes; not suitable for varying densities.</p> </li> <li> <p>Requires Pre-specification of k: Number of clusters must be known beforehand; elbow method is subjective; different metrics may suggest different optimal k values.</p> </li> <li> <p>Sensitive to Outliers: Outliers can significantly influence centroid positions; may create clusters just for outliers; solution is to pre-process data.</p> </li> </ol>"},{"location":"ml/clustering/KMeans/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/dimReduction/","title":"Dimensionality Reduction","text":""},{"location":"ml/dimReduction/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>High dimensional data could be a problem</p> <ul> <li>An obvious reason would be more computing power/time needed to build a model </li> <li>The curse of dimensionality: When the number of dimensions (features) increases our model starts to become more complex and dependent on the data it was trained on and so it overfits thus reducing our model performance </li> <li>For models that use distance metrics, when the number of dimensions is too high, then each datapoint seems very similar to each other. This is because, with very large features, the distance between two data points are almost equal since they are all very far from each other. </li> </ul> <p>Method to resolve : PCA, LDA, t-SNE(non-linear correlation), AutoEncoder</p>"},{"location":"ml/dimReduction/#question","title":"Question","text":"What is curse of dimensionality? <ul> <li> <p>As dimensions increase, data analysis becomes exponentially harder.</p> <ul> <li>Data becomes sparse</li> <li>Distances become meaningless</li> <li>More data needed exponentially</li> <li>Computational cost increases</li> </ul> </li> <li> <p>Solution</p> <ul> <li>Dimensionality reduction</li> <li>Feature selection</li> </ul> </li> </ul> Explain different methods for feature selection and dimensionality reduction. <ul> <li> <p>Forward Feature Selection</p> <ul> <li>Start: Empty feature set</li> <li>Process: Iteratively add best performing feature</li> <li>Selection: Based on performance metric</li> <li>\\(F_{i+1} = F_i \\cup \\{\\text{argmax}_{f \\in F_i} Score(F_i \\cup {f})\\}\\)</li> </ul> </li> <li> <p>Backward Feature Selection</p> <ul> <li>Start: Full feature set</li> <li>Process: Iteratively remove worst feature</li> <li>Selection: Based on performance drop</li> <li>\\(F_{i+1} = F_i \\ \\{\\text{argmin}_{f \\in F_i} Score(F_i \\ {f})\\}\\)</li> </ul> </li> <li> <p>Information Gain</p> </li> <li>Lasso vs Ridge</li> </ul>"},{"location":"ml/dimReduction/LDA/","title":"LDA","text":""},{"location":"ml/dimReduction/LDA/#linear-discriminant-analysis-lda","title":"Linear Discriminant Analysis (LDA)","text":"<ul> <li>LDA is a <code>supervised dimensionality</code> reduction and classification algorithm. </li> <li>Unlike PCA which focuses on maximizing variance, LDA aims to find a linear combination of features that <code>maximizes class separation while minimizing within-class variance</code>.</li> </ul>"},{"location":"ml/dimReduction/LDA/#within-class-scatter-matrix-sw","title":"Within-Class Scatter Matrix (SW)","text":"<p>Measures the variance within each class: \\(S_W = \\sum_c S_c\\)</p> <p>Where for each class c: \\(S_c = \\sum_{i \\in c} (x_i - \\bar{x}_c) \\cdot (x_i - \\bar{x}_c)^T\\)</p>"},{"location":"ml/dimReduction/LDA/#between-class-scatter-matrix-sb","title":"Between-Class Scatter Matrix (SB)","text":"<p>Measures the variance between different classes: \\(S_B = \\sum_{c} n_c \\cdot (\\bar{x}_c - \\bar{x}) \\cdot (\\bar{x}_c - \\bar{x})^T\\)</p> <p>Where</p> <ul> <li>\\(n_c\\) is the number of samples in class c</li> <li>\\(\\bar{x}_c\\) is the mean of class c</li> <li>\\(\\bar{x}\\) is the overall mean</li> </ul>"},{"location":"ml/dimReduction/LDA/#fishers-linear-discriminant","title":"Fisher's Linear Discriminant","text":""},{"location":"ml/dimReduction/LDA/#objective-function","title":"Objective Function","text":"<p>To maximize class separation while minimizing within-class variance, we use Fisher's criterion: \\(J(w) = \\frac{w^TS_Bw}{w^TS_Ww}\\)</p> <p>Taking the derivative with respect to w and setting it to zero: \\(\\frac{\\partial}{\\partial w}J(w) = \\frac{2S_B w(w^T S_W w) - 2S_W w(w^T S_B w)}{(w^T S_W w)^2} = 0\\)</p> <p>Simplifying,</p> <p>\\(S_B w(w^T S_W w) = S_W w(w^T S_B w)\\)</p> <p>\\(S_W^{-1}S_B w = \\frac{(w^T S_B w)}{(w^T S_W w)} w\\)</p> <p>\\(S_W^{-1}S_B w = \\lambda w\\)</p>"},{"location":"ml/dimReduction/LDA/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Calculate the within-class scatter matrix (\\(S_W\\))</li> <li>Calculate the between-class scatter matrix (\\(S_B\\))</li> <li>Compute eigenvalues and eigenvectors of \\(S_W^{-1}S_B\\)</li> <li>Select top k eigenvectors based on eigenvalues</li> <li>Transform the data using selected eigenvectors</li> </ol>"},{"location":"ml/dimReduction/LDA/#advantages","title":"Advantages","text":"<ul> <li>Preserves class discriminatory information</li> <li>Reduces dimensionality while maintaining class separation</li> <li>Works well for normally distributed classes</li> </ul>"},{"location":"ml/dimReduction/LDA/#limitations","title":"Limitations","text":"<ul> <li>Assumes normal distribution of features</li> <li>May fail if within-class covariance is not equal across classes</li> <li>Limited by the number of classes (max components = number of classes - 1)</li> </ul>"},{"location":"ml/dimReduction/LDA/#question","title":"Question","text":"Does LDA has linear decision boundary? <ul> <li>Yes, the decision boundary is a straight line in 2D space, a plane in 3D space, or a hyperplane in higher dimensions.</li> </ul>"},{"location":"ml/dimReduction/LDA/#code","title":"Code","text":"NumpyPyTorch"},{"location":"ml/dimReduction/PCA/","title":"PCA","text":""},{"location":"ml/dimReduction/PCA/#pca","title":"PCA","text":"<p>It is <code>unspervised machine learning</code></p> <p>Assumption:</p> <ul> <li> <p>PCA needs a linear correlation  between all variables  (It should not have  non-linear correlations )</p> </li> <li> <p>It is a linear combination of variables that results in a line or axis/axes that explain a maximal amount of variance from the original dataset. More formally, the eigenvectors of the covariance matrix (of the data) are the principal components and the eigenvalues represent the amount of variance carried in each principal component. </p> </li> <li> <p>If the eigenvectors are, all the same, PCA would not be able to select which principal component since we select the top n eigenvectors and there would be no top n since they are all equal. </p> </li> <li> <p>It is not necessary to remove variables that are highly correlated because PCA would project all the correlated variables onto the same principal  component. </p> </li> <li> <p>Choose number of principle component such that it captures 95-99% or the variance.</p> </li> </ul>"},{"location":"ml/dimReduction/PCA/#process","title":"Process","text":"<ol> <li>Standardize our data (since, PCA is sensitive to the variance within features.)</li> <li>Calculate the covariance matrix </li> <li>Then using this matrix we calculate eigenvectors and eigenvalues and thus the principal components from the eigenvectors</li> </ol>"},{"location":"ml/dimReduction/PCA/#drawback","title":"Drawback","text":"<ol> <li>Computationally expensive </li> <li>Information is always lost </li> <li>Explainability becomes much more difficult.</li> </ol>"},{"location":"ml/dimReduction/PCA/#question","title":"Question","text":"How to determine the optimal number of principal components (hyperplanes) in PCA? <ul> <li>Variance explained = \\(\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^N \\lambda_i}\\) </li> <li>Scree Plot Components:<ul> <li>X-axis: Principal component number</li> <li>Y-axis: variance explained</li> </ul> </li> <li>Choose number of componet if the variance explained reaches the threshold<ul> <li>\\(k = min\\{n: \\frac{\\sum_{i=1}^n \\lambda_i}{\\sum_{i=1}^N \\lambda_i} \\geq \\text{threshold}\\}\\)</li> </ul> </li> </ul>"},{"location":"ml/dimReduction/PCA/#code","title":"Code","text":"NumpyPyTorch"},{"location":"mlops/","title":"MLOPs","text":""},{"location":"mlops/#mlops","title":"MLOps","text":"<p>MLOps combines Machine Learning with DevOps best practices to automate and manage the entire ML lifecycle. It helps in delivering reliable, scalable, and maintainable ML systems.</p>"},{"location":"mlops/#ml-lifecycle-stages-in-mlops","title":"ML Lifecycle Stages in MLOps","text":"<p>Data Collection &amp; Preparation</p> <ul> <li>Gather and clean raw data.</li> <li>Data versioning and feature engineering.</li> </ul> <p>Model Training</p> <ul> <li>Experimentation and tuning of models.</li> <li>Track experiments and hyperparameters.</li> </ul> <p>Model Validation &amp; Testing</p> <ul> <li>Evaluate accuracy, fairness, and robustness.</li> <li>Validate on new/unseen data.</li> </ul> <p>Model Deployment</p> <ul> <li>Deploy model to production environment.</li> <li>Use APIs, microservices, or edge devices.</li> </ul> <p>Monitoring &amp; Maintenance</p> <ul> <li>Monitor model performance and data drift.</li> <li>Trigger retraining or rollback when needed.</li> </ul>"},{"location":"mlops/#types-of-ml-development","title":"Types of ML Development","text":""},{"location":"mlops/#batch-processing","title":"Batch Processing","text":"<ul> <li>Process large datasets at scheduled intervals.</li> <li>Used for offline training and periodic updates.</li> <li>Examples: nightly retraining, report generation.</li> </ul>"},{"location":"mlops/#stream-processing","title":"Stream Processing","text":"<ul> <li>Continuous, near real-time data ingestion and processing.</li> <li>Enables live analytics and alerting.</li> <li>Examples: fraud detection, user behavior tracking.</li> </ul>"},{"location":"mlops/#real-time-inference","title":"Real-time Inference","text":"<ul> <li>Low latency predictions (&lt;100ms).</li> <li>Used for instant decisions like ad bidding, image recognition.</li> </ul>"},{"location":"mlops/#edge-deployment","title":"Edge Deployment","text":"<ul> <li>Model runs locally on devices like mobiles, drones, IoT.</li> <li>Benefits: offline use, privacy, reduced latency.</li> </ul>"},{"location":"mlops/#cicd","title":"CI/CD","text":"<ul> <li>Automate data ingestion, model training, testing, and deployment.</li> <li>Integrate version control for:</li> <li>Code</li> <li>Data sets</li> <li>Models</li> <li>Automate testing for data quality, model accuracy, and bias.</li> <li>Enable rollback or redeployment on model degradation or errors.</li> <li>Tools: Jenkins, GitHub Actions, GitLab CI, CircleCI.</li> </ul>"},{"location":"mlops/#monitoring-governance","title":"Monitoring &amp; Governance","text":"<p>Model Monitoring</p> <ul> <li>Track accuracy, latency, and detect drift; alert on anomalies.</li> </ul> <p>Data Governance</p> <ul> <li>Manage data lineage and ensure regulatory compliance.</li> </ul> <p>Model Governance</p> <ul> <li>Control model versions with access management and audits.</li> </ul>"},{"location":"mlops/API/","title":"API","text":""},{"location":"mlops/API/#api","title":"API","text":""},{"location":"mlops/API/#what-is-an-api","title":"What is an API?","text":"<p>API (Application Programming Interface) is a communication bridge between two software systems that allows them to exchange data and functionalities.</p>"},{"location":"mlops/API/#rest-restful-apis","title":"REST &amp; RESTful APIs","text":"<p>REST (REpresentational State Transfer) is a web architecture style for designing networked applications.</p>"},{"location":"mlops/API/#characteristics","title":"Characteristics","text":"<ul> <li>Stateless communication</li> <li>Client-server model</li> <li>Uniform interface (HTTP)</li> <li>Cacheable</li> <li>Layered system</li> </ul>"},{"location":"mlops/API/#restful-vs-rest","title":"RESTful vs REST","text":"<ul> <li>REST is the architectural concept.</li> <li>RESTful APIs are web services that adhere to REST principles.</li> </ul>"},{"location":"mlops/API/#crud-operations","title":"CRUD Operations","text":"Operation HTTP Method Description Example (AI Services) Create POST Add a new service Add <code>video-gen</code> service Read GET Fetch all services Get list of services Update PUT Modify a service Update <code>en-nep-translate</code> <code>en-hn-translate</code> Delete DELETE Remove a service Delete <code>text-gen</code>"},{"location":"mlops/API/#http-methods","title":"HTTP Methods","text":"Method Usage <code>GET</code> Retrieve data <code>POST</code> Submit new data <code>PUT</code> Replace entire resource <code>PATCH</code> Modify partial resource <code>DELETE</code> Remove resource <code>HEAD</code> Like GET but no body <code>OPTIONS</code> Communication options <code>CONNECT</code> Establish tunnel <code>TRACE</code> Loop-back message test"},{"location":"mlops/API/#api-response-status-codes","title":"API Response Status Codes","text":"Category Description 1xx Informational 2xx Successful 3xx Redirection 4xx Client error 5xx Server error"},{"location":"mlops/API/#http-response-status-codes","title":"HTTP Response Status Codes","text":"<ul> <li>200 OK: Request processed successfully.</li> <li>202 Accepted: Request accepted but not yet processed.</li> <li>204 No Content: Request processed successfully with no content returned.</li> <li>400 Bad Request: The request could not be processed due to client error.</li> <li>404 Not Found: The requested resource could not be found.</li> <li>500 Internal Server Error: A generic error occurred on the server.</li> <li>503 Service Unavailable: The server cannot handle the request at the moment.</li> </ul>"},{"location":"mlops/AWS/","title":"AWS","text":""},{"location":"mlops/AWS/#aws","title":"AWS","text":"<p>AWS Calculator</p>"},{"location":"mlops/AWS/#ec2","title":"EC2","text":""},{"location":"mlops/AWS/#ami","title":"AMI","text":"<pre><code>- image\n</code></pre>"},{"location":"mlops/AWS/#volumes","title":"Volumes","text":"<pre><code>- storage\n</code></pre>"},{"location":"mlops/AWS/#snapshots","title":"Snapshots","text":"<pre><code>- Backups\n</code></pre>"},{"location":"mlops/AWS/#key-pairs","title":"Key-Pairs","text":""},{"location":"mlops/AWS/#load-balancer","title":"Load Balancer","text":""},{"location":"mlops/AWS/#s3","title":"S3","text":""},{"location":"mlops/AWS/#bucket","title":"Bucket","text":""},{"location":"mlops/AWS/#iam","title":"IAM","text":""},{"location":"mlops/Async/","title":"AsyncIO","text":""},{"location":"mlops/Async/#asyncio","title":"AsyncIO","text":"<p>We can run CPU bound operation while we are waiting for IO bound response.</p>"},{"location":"mlops/Async/#concurrency-vs-paralleism","title":"Concurrency vs Paralleism","text":"<ul> <li>Concurrency is two lines of customers ordering from a single cashier (lines take turns ordering). This works really well in python.</li> <li>Parallelism is two lines of customers ordering from two cashiers (each line gets its own cashier). In python parallelism faces Global Interpreter Lock (GIL) on interpretor. Python code are single threaded even we start multiple thread.</li> </ul> <p><code>async</code>: Writing in front of function helps to run the mehod/function concurrently</p> <p><code>await</code>: Gives the control of order on how the things are being executed. Example. Getting data from database, getting confirmation from user</p> <p>async func/method requires await whenever they are being called.</p> <p><code>gather</code>: helps to run the function in parallel</p> <pre><code>import asyncio\nres1, res2 = await asyncio.gather(func1(args2), func2(args2))\n</code></pre>"},{"location":"mlops/CI-CD/","title":"CI/CD","text":""},{"location":"mlops/CI-CD/#cicd","title":"CI/CD","text":""},{"location":"mlops/CI-CD/#continuous-deployment","title":"Continuous Deployment","text":"<ul> <li>GitHub repo</li> <li>Dev env</li> <li>QA env</li> <li>UAT env</li> <li>Production</li> </ul>"},{"location":"mlops/CI-CD/#developer-workflow","title":"Developer Workflow","text":"<p>Key Stages</p> <ul> <li>Coding</li> <li>Version Control</li> <li>Code Review</li> <li>Testing</li> <li>Continuous Integration</li> <li>CD</li> <li>Monitoring</li> </ul> <p>Example</p> <ol> <li>Feature Development</li> <li>Push and Pull Request (PR)</li> <li>Automated CI Pipeline         - Once PR is opened, build the application and runs all the test cases.         - If pipeline pases then code will be approved then it will be merged to main</li> <li>Continuous Deployment         - After merging the PR a CD pipeline is triggered         - Application is deployed to staging environment</li> </ol>"},{"location":"mlops/DEBUG/","title":"DEBUG","text":"<p>For debugger <code>launch.json</code></p> <pre><code>{\n    // VS CODE LAUNCH.JSON CONFIGURATION NOTES\n    // This file configures debugging sessions for VS Code\n    // Location: .vscode/launch.json in your project root\n\n    \"version\": \"0.2.0\",  // Schema version - always use \"0.2.0\" for current VS Code versions\n\n    \"configurations\": [  // Array of debug configurations - you can have multiple setups\n        {\n            // BASIC PYTHON DEBUGGER CONFIGURATION\n            \"name\": \"Debug Main Script\",           // Display name in VS Code debug dropdown\n            \"type\": \"debugpy\",                     // CRITICAL: Use \"debugpy\" for Python debugging\n            \"request\": \"launch\",                   // \"launch\" starts new process, \"attach\" connects to existing\n            \"program\": \"${workspaceFolder}/main.py\",  // IMPORTANT: Path to script to debug\n            \"console\": \"integratedTerminal\",       // Where output appears: \"integratedTerminal\", \"internalConsole\", \"externalTerminal\"\n            \"args\": [],                           // Command line arguments as array of strings\n            \"envFile\": \"${workspaceFolder}/.env\", // Load environment variables from .env file\n            \"python\": \"/path/to/your/venv/bin/python\",  // ESSENTIAL: Specify Python interpreter path\n\n            // OPTIONAL PROPERTIES (commonly used)\n            \"cwd\": \"${workspaceFolder}\",          // Working directory - defaults to workspace folder\n            \"stopOnEntry\": false,                 // Pause on first line of code (useful for debugging startup)\n            \"justMyCode\": true,                   // Only debug your code, not library code\n            \"env\": {                              // Additional environment variables\n                \"PYTHONPATH\": \"${workspaceFolder}\",\n                \"DEBUG\": \"true\"\n            }\n        },\n\n        {\n            // TRAINING SCRIPT WITH ARGUMENTS CONFIGURATION\n            \"name\": \"Train Model\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\", \n            \"program\": \"${workspaceFolder}/train.py\",\n            \"console\": \"integratedTerminal\",\n\n            // COMMAND LINE ARGUMENTS\n            // Each argument and its value should be separate array elements\n            \"args\": [\n                \"--instance_data_dir\", \"examples/creature\",  // Training data directory\n                \"--num_of_assets\", \"3\",                      // Number of assets to train on\n                \"--initializer_tokens\", \"creature\", \"bowl\", \"stone\",  // Token names\n                \"--class_data_dir\", \"inputs/data_dir\",       // Class data directory\n                \"--phase1_train_steps\", \"400\",               // Training steps for phase 1\n                \"--phase2_train_steps\", \"400\",               // Training steps for phase 2\n                \"--output_dir\", \"outputs/creature\"           // Where to save trained model\n            ],\n\n            \"envFile\": \"${workspaceFolder}/.env\",\n            \"python\": \"/path/to/your/venv/bin/python\",\n\n            // TRAINING-SPECIFIC OPTIONS\n            \"justMyCode\": false,    // May want to debug into libraries during training\n            \"subProcess\": true      // Debug subprocess if training spawns child processes\n        },\n\n        {\n            // ADVANCED CONFIGURATION EXAMPLE\n            \"name\": \"Advanced Debug Setup\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"program\": \"${workspaceFolder}/advanced_script.py\",\n            \"console\": \"integratedTerminal\",\n\n            // PRE/POST LAUNCH TASKS\n            \"preLaunchTask\": \"install-dependencies\",  // Run task before debugging\n            \"postDebugTask\": \"cleanup\",               // Run task after debugging\n\n            // DEBUGGING BEHAVIOR\n            \"stopOnEntry\": true,        // Pause immediately when debugging starts\n            \"showReturnValue\": true,    // Show function return values in debug console\n            \"redirectOutput\": true,     // Capture print statements in debug console\n\n            // PYTHON-SPECIFIC OPTIONS\n            \"python\": \"${command:python.interpreterPath}\",  // Use VS Code's selected Python interpreter\n            \"justMyCode\": false,        // Debug into library code too\n            \"django\": false,           // Set to true for Django projects\n            \"flask\": {                 // Flask-specific debugging options\n                \"app\": \"app.py\",\n                \"args\": [\"run\", \"--debug\"]\n            },\n\n            // ENVIRONMENT AND PATHS\n            \"cwd\": \"${workspaceFolder}/src\",  // Different working directory\n            \"env\": {\n                \"PYTHONPATH\": \"${workspaceFolder}/src:${workspaceFolder}/lib\",\n                \"LOG_LEVEL\": \"DEBUG\",\n                \"CUDA_VISIBLE_DEVICES\": \"0\"\n            },\n\n            // DEBUGGING SPECIFIC FEATURES\n            \"gevent\": false,           // Set true if using gevent\n            \"pyramid\": false           // Set true if using Pyramid framework\n        }\n    ],\n\n    // COMMON VARIABLE SUBSTITUTIONS (read-only reference)\n    // ${workspaceFolder} - Root folder of your project\n    // ${workspaceFolderBasename} - Name of the workspace folder\n    // ${file} - Currently opened file\n    // ${fileBasename} - Basename of currently opened file\n    // ${fileDirname} - Directory of currently opened file\n    // ${fileExtname} - Extension of currently opened file\n    // ${cwd} - Current working directory\n    // ${lineNumber} - Current line number in active file\n    // ${selectedText} - Currently selected text in active file\n    // ${command:python.interpreterPath} - Path to selected Python interpreter\n\n    // IMPORTANT TROUBLESHOOTING NOTES:\n    // 1. Always use forward slashes (/) in paths, even on Windows\n    // 2. Virtual environment path usually ends with /bin/python (Linux/Mac) or /Scripts/python.exe (Windows)\n    // 3. If debugging doesn't work, check Python interpreter path first\n    // 4. Use \"justMyCode\": false to debug into third-party libraries\n    // 5. Check VS Code Python extension is installed and updated\n    // 6. Ensure debugpy is installed in your Python environment: pip install debugpy\n}\n</code></pre>"},{"location":"mlops/Docker/","title":"Docker","text":""},{"location":"mlops/Docker/#docker","title":"Docker","text":"<ul> <li>Docker contains many image, many containers</li> <li>Platform independent</li> </ul> <pre><code>docker login -u &lt;username&gt;\n\ndocker login -u rokmr\n</code></pre> <p>Running iteractive linux</p> <pre><code>docker run -it ubuntu\n</code></pre>"},{"location":"mlops/Docker/#images","title":"Images","text":"<ul> <li>Executable files to build the docker container</li> <li>Image is like class and container is like object</li> <li>Its a static snapshot</li> <li>Basis of docker containers and represents full applications</li> </ul> <p>For the firsttime initlization </p><pre><code>docker build -t &lt;imagename&gt; &lt;location&gt;\n\n# Example\ndocker build -t fastapi macros/\n</code></pre> <p>Docker runnning </p><pre><code>docker run &lt;imagename&gt;\n</code></pre> detach mode <pre><code>docker run -d  &lt;imagename&gt;\n</code></pre> <p>container name &amp; detach mode </p><pre><code>docker run --name &lt;Container_name&gt; -d  &lt;imagename&gt;\n</code></pre> <p></p><pre><code>docker images -a\n</code></pre> Some requre <pre><code>docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag\n\n# example\ndocker run --name mysql -e MYSQL_ROOT_PASSWORD=sqlpassword -d mysql\n</code></pre>"},{"location":"mlops/Docker/#container","title":"Container","text":"<ul> <li>package application with dependencies</li> <li>Its a running instance</li> <li>Created using images for running applications List all running container <pre><code>docker ps\ndocker ps -a\n</code></pre> <pre><code>docker stop &lt;container_name&gt;\n</code></pre></li> </ul> <pre><code>docker rm &lt;container_name&gt;\n</code></pre> <p>Properties</p> <ul> <li>Protable</li> <li>Light Weight</li> </ul>"},{"location":"mlops/Docker/#docker-engine","title":"Docker Engine","text":""},{"location":"mlops/Docker/#docker-hub","title":"Docker Hub","text":""},{"location":"mlops/Docker/#docker-basic-command","title":"Docker Basic Command","text":"<pre><code>docker \n</code></pre>"},{"location":"mlops/Docker/#network-bridging","title":"Network Bridging","text":"<p>container has different compared to system host system port request is connected to docker container post and this called port binding </p><pre><code>docker run  -p &lt;systemport&gt;:&lt;dockerport&gt; &lt;imagename&gt;\n\ndocker run  -p 80:5000 fastapi\n</code></pre> <pre><code>docker run --name mysql -e MYSQL_ROOT_PASSWORD=sqlpassword -p8800:3306 -d mysql\n</code></pre> <p>diffrernt doc should have different host machine port</p>"},{"location":"mlops/Docker/#changing-tag-for-pushing-it-to-dockerhub","title":"Changing tag for pushing it to dockerhub","text":"<pre><code>docker tag &lt;imagename&gt; &lt;username/imagename:version&gt;\n\n# Example\ndocker tag fastapi rokmr/fastapi:latest\n</code></pre> Push <pre><code>docker push &lt;username&gt;/&lt;imagename&gt;\ndocker push rokmr/fastapi\n</code></pre> Remove <pre><code>docker rmi &lt;imagename&gt;\ndocker rmi -f &lt;imagename&gt;\n</code></pre> <p>Pull </p><pre><code>docker pull &lt;username&gt;/&lt;imagename&gt;\ndocker pull rokmr/fastapi\n</code></pre>"},{"location":"mlops/Docker/#trouble-shoot-command","title":"Trouble Shoot Command","text":"<pre><code># For logs\ndocker logs CONT_ID\n\n# Allows additional command on already comand\ndocker exec -it CONT_ID /bin/bash \ndocker exec -it CONT_ID /bin/sh\n</code></pre>"},{"location":"mlops/Docker/#vm-and-docker","title":"VM and Docker","text":"<p>Hardware -&gt; Host OS Kernel -&gt; Appliction Layer</p> <p>VM - Virtualizes Host OS Kernel &amp; Appliction Layer - VM are compatible with all operating OS</p> <p>Docker</p> <ul> <li>Virtualizes Appliction Layer -&gt; Light weight -&gt; Faster</li> <li>Initially built for the Linux, For macOS/Windows Docker Desktop adds a hyperwizer layer of linux distribution</li> </ul>"},{"location":"mlops/Docker/#docker-network","title":"Docker Network","text":"<p>Interaction between two port</p> <p></p><pre><code>docker network ls\n\ndocker network create &lt;network-name&gt;\n\ndocker network rm &lt;network-name&gt;\n</code></pre> Driver <ul> <li>bridge </li> <li>host</li> <li>null</li> </ul>"},{"location":"mlops/Docker/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is tool for defining and running multi container applicaiton.</p> <p>create <code>.yaml</code> file composing all the command</p> <pre><code>services:\n    docker1:\n        image:\n        port:\n        environment:\n\n    docker2:\n</code></pre> <pre><code>docker compose -f filename.yaml up -d\n# oR\ndocker-container up\n\ndocker-container up --app=5\n# only create\ndocker-container create up\n\n\ndocker compose -f filename.yaml down\n</code></pre>"},{"location":"mlops/Docker/#image","title":"Image","text":"<pre><code>FROM base image\nWORKDIR\nCOPY host to image\nRUN runs intrusction (can have multiple run command)\nCMD entry point of the Appliction (can have single CMD command)\n</code></pre>"},{"location":"mlops/Docker/#docker-volumes","title":"Docker Volumes","text":"<p>Volumes are persistent data stores for container.</p> <pre><code>docker volume ls\ndocker volume create &lt;vol_name&gt;\ndocker volume rm &lt;vol_name&gt;\ndocker run -it -v &lt;data_location_host&gt;:&lt;data_location_container&gt;  &lt;image_name&gt;\n\n# Named Volume\ndocker run -v &lt;vol_name&gt;:&lt;container_dir&gt;\n\n# Anonymous Volume\ndocker run -v &lt;mount_path&gt;\n\n# Bind mount\ndocker run -v &lt;host-dir&gt;:&lt;cont_dir&gt;\n\ndocker volume prune (deletes the anonymous volume)\n</code></pre>"},{"location":"mlops/FastAPI/","title":"FastAPI","text":""},{"location":"mlops/FastAPI/#fastapi","title":"FastAPI","text":"<ul> <li>High-performance Python web framework</li> <li>Based on standard type hints</li> <li>Automatic interactive API docs</li> <li>Uses <code>uvicorn</code> as ASGI server</li> <li>Data validation with <code>pydantic</code></li> </ul>"},{"location":"mlops/FastAPI/#sample-in-memory-data","title":"Sample In-Memory Data","text":"<pre><code>services = [\n    {\"id\": 1, \"name\": \"image-gen\", \"description\": \"Generates images from prompts\"},\n    {\"id\": 2, \"name\": \"text-gen\", \"description\": \"Generates text from input\"},\n    {\"id\": 3, \"name\": \"en-nep-translate\", \"description\": \"Translates English to Nepali\"},\n]\n</code></pre>"},{"location":"mlops/FastAPI/#example","title":"Example","text":"<pre><code>from typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\") # Inline Parameter\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n</code></pre>"},{"location":"mlops/FastAPI/#get","title":"GET","text":"<p><code>GET /services</code></p> <p>Returns all AI services.</p> <p></p><pre><code>@app.get(\"/services\")\ndef get_services():\n    return services\n</code></pre> Example Call: <pre><code>GET http://127.0.0.1:8000/services\n</code></pre> <pre><code>[\n    {\n        \"id\": 1,\n        \"name\": \"image-gen\",\n        \"description\": \"Generates images from prompts\"\n    },\n    {\n        \"id\": 2,\n        \"name\": \"text-gen\",\n        \"description\": \"Generates text from input\"\n    },\n    {\n        \"id\": 3,\n        \"name\": \"en-nep-translate\",\n        \"description\": \"Translates English to Nepali\"\n    }\n]\n</code></pre> <p><code>GET /service_info/{id}?verbose=true</code></p> <p>Query by ID with optional query param:</p> <pre><code>@app.get(\"/service_info/{service_id}\") # service_id is non-optional but verbose is optional\ndef get_service_info(service_id: int, verbose: Union[bool, None] = False):\n    for service in services:\n        if service[\"id\"] == service_id:\n            if verbose:\n                return {\"id\": service_id, \"info\": service}\n            return {\"name\": service[\"name\"]}\n    return {\"error\": \"Service not found\"}\n</code></pre> <pre><code>GET http://127.0.0.1:8001/service_info/1\n</code></pre> <pre><code>{\"name\": \"image-gen\"}\n</code></pre> <pre><code>GET http://127.0.0.1:8001/service_info/1?verbose=True\n</code></pre> <pre><code>{\n    \"id\": 1,\n    \"info\": {\n        \"id\": 1,\n        \"name\": \"image-gen\",\n        \"description\": \"Generates images from prompts\"\n    }\n}\n</code></pre> <p>We pass <code>inline</code>parameters</p>"},{"location":"mlops/FastAPI/#post","title":"POST","text":"<p><code>POST /services</code></p> <p>Adds a new AI service.</p> <p></p><pre><code>@app.post(\"/add_services\")\nasync def add_service(request: Request): # resquest is data payload here which is passed seperately.\n    data = await request.json()\n    new_id = max(s[\"id\"] for s in services) + 1\n    new_service = {\"id\": new_id, \"name\": data[\"name\"], \"description\": data[\"description\"]}\n    services.append(new_service)\n    return {\"message\": \"Service added\", \"service\": new_service}\n</code></pre> Request URL: <pre><code>POST http://127.0.0.1:8000/add_services\n</code></pre> <p>Body (JSON):</p> <pre><code>{\n  \"name\": \"video-gen\",\n  \"description\": \"Generates short videos from text\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\n    \"message\": \"Service added\",\n    \"service\": {\n        \"id\": 4,\n        \"name\": \"video-gen\",\n        \"description\": \"Generates short videos from text\"\n    }\n}\n</code></pre>"},{"location":"mlops/FastAPI/#put","title":"PUT","text":"<p><code>PUT /services/{service_name}</code></p> <p>Updates a service by name.</p> <pre><code>@app.put(\"/services/{service_name}\")\nasync def update_service(service_name: str, request: Request):\n    data = await request.json()\n    for service in services:\n        if service[\"name\"] == service_name:\n            service[\"name\"] = data.get(\"name\", service[\"name\"])\n            service[\"description\"] = data.get(\"description\", service[\"description\"])\n            return {\"message\": \"Service updated\", \"service\": service}\n    return {\"error\": \"Service not found\"}\n</code></pre>"},{"location":"mlops/FastAPI/#delete","title":"DELETE","text":"<p><code>DELETE /services/{service_name}</code></p> <p>Deletes a service by name.</p> <pre><code>@app.delete(\"/services/{service_name}\")\ndef delete_service(service_name: str):\n    global services\n    services = [s for s in services if s[\"name\"] != service_name]\n    return {\"message\": f\"Service '{service_name}' deleted\"}\n</code></pre>"},{"location":"mlops/FastAPI/#load-testing","title":"Load Testing","text":""},{"location":"mlops/FastAPI/#using-postman","title":"Using Postman","text":"<ol> <li>Create a collection of APIs</li> <li>Set iteration count (e.g., N = 100)</li> <li>Note Total Execution Time (T)</li> <li>Compute Latency:    [ \\text{Latency} = \\frac{T}{N} ]</li> </ol>"},{"location":"mlops/FastAPI/#summary-table","title":"Summary Table","text":"Endpoint Method Description <code>/services</code> GET Fetch all AI services <code>/services</code> POST Add a new AI service <code>/services/{service_name}</code> PUT Update a specific service <code>/services/{service_name}</code> DELETE Delete a specific service <code>/service_info/{id}</code> GET Get info with optional verbose param"},{"location":"mlops/FastAPI/#code","title":"Code","text":"<pre><code>from fastapi import FastAPI, Request\nfrom pydantic import BaseModel\nfrom typing import Optional, List, Union\nimport uvicorn\n\napp = FastAPI()\n\nservices = [\n    {\"id\": 1, \"name\": \"image-gen\", \"description\": \"Generates images from prompts\"},\n    {\"id\": 2, \"name\": \"text-gen\", \"description\": \"Generates text from input\"},\n    {\"id\": 3, \"name\": \"en-nep-translate\", \"description\": \"Translates English to Nepali\"},\n]\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Welcome to the FastAPI\"}\n\n\n@app.get(\"/services\")\ndef get_services():\n    return services\n\n@app.get(\"/service_info/{service_id}\")\ndef get_service_info(service_id: int, verbose: Union[bool, None] = False):\n    for service in services:\n        if service[\"id\"] == service_id:\n            if verbose:\n                return {\"id\": service_id, \"info\": service}\n            return {\"name\": service[\"name\"]}\n    return {\"error\": \"Service not found\"}\n\n@app.post(\"/add_service\")\nasync def add_service(request: Request):\n    data = await request.json()\n    new_id = max(s[\"id\"] for s in services) + 1\n    new_service = {\"id\": new_id, \"name\": data[\"name\"], \"description\": data[\"description\"]}\n    services.append(new_service)\n    return {\"message\": \"Service added\", \"service\": new_service}\n</code></pre>"},{"location":"mlops/GitHubActions/","title":"GitHub Actions","text":""},{"location":"mlops/GitHubActions/#github-action","title":"GitHub Action","text":""},{"location":"mlops/GitHubActions/#overview","title":"Overview","text":"<p>GitHub Actions automates Python development workflows, including testing, model training,  and deployment of deep learning applications.</p>"},{"location":"mlops/GitHubActions/#attributes","title":"Attributes","text":"<pre><code>name: name of workflow \non: the GitHub event that triggers the workflow\njobs: defines the block that list the jobs for the workflow\nruns-on: type of machine to run the job\nsteps: to list action and command\nEach steps has access to file system in the virtual environment but runs on its distinct and seperate process\nuses: loction of the action(tell job to use specific action to use, actions are bundles of codes). Action are docker images.\nrun: runs command in the virtual environment shell\n</code></pre>"},{"location":"mlops/GitHubActions/#workflows","title":"Workflows","text":"<ul> <li>It define the event that triggers the GitHub Actions</li> <li>It also define which action will be run based on the events</li> <li>One repository can contain several workflow each calling different action based on specific events</li> <li>workflows are located at <code>.github/workflows</code></li> </ul> <p>Example</p> <pre><code>name: first\n\non: push\n\njobs:\n    job1:  # we can name as per our convenience\n        name: First job\n        runs-on: ubuntu-latest\n        steps: \n        - name: Step one\n          uses: action/checkout@v2\n        - name: Step two\n          run: env | sort\n    job2:\n        name: Second job\n        runs-on: windows-latest\n        steps:\n        - name: Step one\n          uses: action/checkout@v2\n        - name: Step two\n          run: \"Get-ChildItem Env: | Sort-Object Name\"\n</code></pre> <pre><code># Adding an action\nuses: Execute an action in the operating system\n\n# public repo\nuses: {}/{}@{}\nuses: octocat/super-cool-action@v1\n\n# same repository as the workflow\nuses: ./path to the action\nuses: ./github/actions/my-local-location\n\n# docker image registry\nuses: docker://{image}:{tag}\nuses: docker://ello-world:latest\n\n# Can be added from market place also\n</code></pre> <pre><code># Adding command\n\nrun: execute command in the operating system shell\nbash: default shell for Ubuntu, macOS\n\n# Single line Command\nrun: {command}{parameters}{arguments}\nrun: mv ./output ./archive\n\n# Multiline Command\nrun: |\n    Command 1\n    Command 2\n</code></pre> <pre><code># Adding dependencies (job need sequential rather parallel)\n# add parameter need to that job\n# needs: Identifies one or more job that must complete successfully before a job will run.\n\njobs:\n    job1:\n    job2:\n    job3:\n        needs: [job1, job2]\n</code></pre> <pre><code># Adding condition to a workflow\n\non:\n    push:\n        branches:\n            - develop\n    pull_request:\n        branches:\n            - master\n</code></pre> <pre><code># Passing Argument to an Action\nuses: {github account}/{action name}\nwith: \n    key: val\n    key: val\n\n# Example\nsteps:\n    -   name: checkout the code\n        uses: actions/checkout@v2\n        with:\n            repositoty: apache/tomcat\n            ref: master # reference we need to chckout\n            path: ./tomcat\n</code></pre>"},{"location":"mlops/GitHubActions/#environment-variable","title":"Environment variable","text":""},{"location":"mlops/GitHubActions/#default-environment-variable","title":"default environment variable","text":""},{"location":"mlops/GitHubActions/#custom-environment-variable","title":"custom environment variable","text":"<p>variable can be defined at: - workflow - jobs - steps</p> <p>Access - env.variable_name - variable is read from workflow - variable can be used in workflow configuration</p>"},{"location":"mlops/GitHubActions/#secrets","title":"Secrets","text":"<ul> <li>secrets.variable_name</li> </ul>"},{"location":"mlops/GitHubActions/#artifacts","title":"Artifacts","text":"<ul> <li>Data preserved from a workflow when it gets completed</li> <li>files</li> <li>Binary, archives, and images produced by build steps</li> <li>Exist beyond the life of build steps</li> <li>Stored and tracked in registeries</li> <li>registeries requires authentication to upload new version of the artifacts</li> </ul>"},{"location":"mlops/GitHubActions/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>CI</p> <ul> <li>Find and resolve problem early</li> <li>Work locally and then commit to repo</li> </ul> <p>CD</p> <ul> <li>compiled into artifacts</li> <li>stored</li> <li>Additional tests</li> <li>Ready for development</li> </ul>"},{"location":"mlops/GitHubActions/#linting-unit-tests","title":"Linting &amp; Unit tests","text":"<p>Linting</p> <ul> <li>Enforcing coding standards</li> <li>Improve code quality</li> <li>Catch errors early in the design cycle</li> </ul> <p>Unit Test</p> <ul> <li>First tests run</li> <li>Check code at component level</li> <li>Expose problems closer to the code</li> <li>Fast running</li> </ul>"},{"location":"mlops/GitHubActions/#build","title":"Build","text":"<ul> <li>Compile code into a binary package</li> <li>Machine-readable format</li> </ul> <p>Example:</p> <ul> <li>GNU Compiler \\(\\rightarrow\\) C/C++ \\(\\rightarrow\\) Executable</li> <li>zip, tar, rpm \\(\\rightarrow\\) files \\(\\rightarrow\\) archive</li> <li>Docker \\(\\rightarrow\\) Layer \\(\\rightarrow\\) Container Image</li> </ul>"},{"location":"mlops/GitHubActions/#test","title":"Test","text":"<ul> <li>Automated testing improves the deplyment speed</li> <li>Check for errors continuously</li> </ul>"},{"location":"mlops/GitHubActions/#status-badge","title":"Status Badge","text":"<pre><code>https://github.com/&lt;OWNER&gt;/&lt;REPOSITORY&gt;/workflows/&lt;WORKFLOW_NAME&gt;/badge.svg\n</code></pre>"},{"location":"mlops/Hydra/","title":"A2A","text":""},{"location":"mlops/NGINX/","title":"NGINX","text":""},{"location":"mlops/Postgress/","title":"A2A","text":""},{"location":"mlops/PyTorchLightening/","title":"A2A","text":""},{"location":"mlops/Pydantic/","title":"Pydantic","text":""},{"location":"mlops/Pydantic/#pydantic","title":"Pydantic","text":"<p>It helps to get structured output from the LLM.</p> <pre><code>from pydantic import BaseModel, EmailStr\nfrom typing import Literal, List\n\nclass CustomerQuery(BaseModel):\n    name: str\n    email: EmailStr\n    query: str\n    category: Literal['refund_request', 'information_request', 'other']\n    is_complaint: bool\n    tags: List[str]\n</code></pre> <pre><code>valid_data = CustomerQuery.model_validate_json(json_output)\n</code></pre> <p>The above can have the following errors, which are handled by Pydantic:</p> <ol> <li>JSON ValidationError: If the LLM's response contains extra text or unexpected formatting, or if the JSON itself is not properly formatted.</li> <li>Data ValidationError: JSON is correct, but the data contained in the JSON does not match the model.</li> </ol> <p>More info at notebook</p>"},{"location":"mlops/UV/","title":"UV","text":""},{"location":"nlp/","title":"Natural Language Procesing","text":""},{"location":"notebooks/","title":"Notebooks","text":""},{"location":"notebooks/langgraph/","title":"Basics","text":"In\u00a0[142]: Copied! <pre>%%capture --no-stderr\n%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python langgraph-checkpoint-sqlite wikipedia\n</pre> %%capture --no-stderr %pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python langgraph-checkpoint-sqlite wikipedia In\u00a0[5]: Copied! <pre>from dotenv import load_dotenv\nload_dotenv()\n</pre> from dotenv import load_dotenv load_dotenv() Out[5]: <pre>True</pre> In\u00a0[3]: Copied! <pre>from langchain_openai import ChatOpenAI\ngpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\ngpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n</pre> from langchain_openai import ChatOpenAI gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0) gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0) <pre>/Users/rohitkumar/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n</pre> In\u00a0[4]: Copied! <pre>from langchain_core.messages import HumanMessage\n\n# Create a message\nmsg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n\n# Message list\nmessages = [msg]\n\n# Invoke the model with a list of messages \ngpt4o_chat.invoke(messages)\n</pre> from langchain_core.messages import HumanMessage  # Create a message msg = HumanMessage(content=\"Hello world\", name=\"Lance\")  # Message list messages = [msg]  # Invoke the model with a list of messages  gpt4o_chat.invoke(messages) Out[4]: <pre>AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAmLXOBDy95huCIXW95JHOKVMJM1b', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--48ec4ecf-d907-4044-b5f1-78c98ef263a0-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})</pre> In\u00a0[6]: Copied! <pre>from langchain_community.tools.tavily_search import TavilySearchResults\ntavily_search = TavilySearchResults(max_results=3)\nsearch_docs = tavily_search.invoke(\"What is LangGraph?\")\n</pre> from langchain_community.tools.tavily_search import TavilySearchResults tavily_search = TavilySearchResults(max_results=3) search_docs = tavily_search.invoke(\"What is LangGraph?\") <pre>/var/folders/xm/hv0x78kx5mvbzd2tsn4q98km0000gn/T/ipykernel_70189/3227139208.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n  tavily_search = TavilySearchResults(max_results=3)\n</pre> In\u00a0[7]: Copied! <pre>search_docs\n</pre> search_docs Out[7]: <pre>[{'title': 'What is LangGraph? - IBM',\n  'url': 'https://www.ibm.com/think/topics/langgraph',\n  'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\n\\nLLM applications: By using LangGraph\u2019s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.',\n  'score': 0.94608605},\n {'title': 'What is LangGraph? - GeeksforGeeks',\n  'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n  'content': '# What is LangGraph?\\n\\nLast Updated : \\n25 Aug, 2025\\n\\nSuggest changes\\n\\n1 Like\\n\\nLangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. At its core, LangGraph combines large language models (LLMs) with graph-based architectures allowing developers to map, organize and optimize how AI agents interact and make decisions.',\n  'score': 0.9412289},\n {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n  'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n  'content': \"Skip to main content\\n\\n# LangGraph Tutorial: What Is LangGraph and How to Use It?\\n\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\n\\nJun 26, 2024  \u00b7 12 min read [...] Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. [...] For applications requiring autonomous decision-making, LangGraph enables the creation of agents that can perform tasks independently based on user inputs and predefined logic.\\n\\nThese agents can execute complex workflows, interact with other systems, and adapt to new information dynamically. LangGraph's structured framework ensures that each agent operates efficiently and effectively, making it suitable for tasks like automated customer support, data processing, and system monitoring.\",\n  'score': 0.9374919}]</pre> In\u00a0[8]: Copied! <pre>from typing_extensions import TypedDict\n\nclass State(TypedDict):\n    graph_state: str\n</pre> from typing_extensions import TypedDict  class State(TypedDict):     graph_state: str In\u00a0[9]: Copied! <pre>def node_1(state):\n    print(\"---Node 1---\")\n    return {\"graph_state\": state['graph_state'] +\" I am\"}\n\ndef node_2(state):\n    print(\"---Node 2---\")\n    return {\"graph_state\": state['graph_state'] +\" happy!\"}\n\ndef node_3(state):\n    print(\"---Node 3---\")\n    return {\"graph_state\": state['graph_state'] +\" sad!\"}\n</pre> def node_1(state):     print(\"---Node 1---\")     return {\"graph_state\": state['graph_state'] +\" I am\"}  def node_2(state):     print(\"---Node 2---\")     return {\"graph_state\": state['graph_state'] +\" happy!\"}  def node_3(state):     print(\"---Node 3---\")     return {\"graph_state\": state['graph_state'] +\" sad!\"} In\u00a0[10]: Copied! <pre>import random\nfrom typing import Literal\n\ndef decide_mood(state) -&gt; Literal[\"node_2\", \"node_3\"]:\n    \n    # Often, we will use state to decide on the next node to visit\n    user_input = state['graph_state'] \n    \n    # Here, let's just do a 50 / 50 split between nodes 2, 3\n    if random.random() &lt; 0.5:\n\n        # 50% of the time, we return Node 2\n        return \"node_2\"\n    \n    # 50% of the time, we return Node 3\n    return \"node_3\"\n</pre> import random from typing import Literal  def decide_mood(state) -&gt; Literal[\"node_2\", \"node_3\"]:          # Often, we will use state to decide on the next node to visit     user_input = state['graph_state']           # Here, let's just do a 50 / 50 split between nodes 2, 3     if random.random() &lt; 0.5:          # 50% of the time, we return Node 2         return \"node_2\"          # 50% of the time, we return Node 3     return \"node_3\" In\u00a0[11]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\"node_1\", decide_mood)\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import StateGraph, START, END  # Build graph builder = StateGraph(State) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2) builder.add_node(\"node_3\", node_3)  # Logic builder.add_edge(START, \"node_1\") builder.add_conditional_edges(\"node_1\", decide_mood) builder.add_edge(\"node_2\", END) builder.add_edge(\"node_3\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[12]: Copied! <pre>graph.invoke({\"graph_state\" : \"Hi, this is Lance.\"})\n</pre> graph.invoke({\"graph_state\" : \"Hi, this is Lance.\"}) <pre>---Node 1---\n---Node 3---\n</pre> Out[12]: <pre>{'graph_state': 'Hi, this is Lance. I am sad!'}</pre> In\u00a0[13]: Copied! <pre>from pprint import pprint\nfrom langchain_core.messages import AIMessage, HumanMessage\n\nmessages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\nmessages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\"))\nmessages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\nmessages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))\n\nfor m in messages:\n    m.pretty_print()\n</pre> from pprint import pprint from langchain_core.messages import AIMessage, HumanMessage  messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")] messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\")) messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\")) messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))  for m in messages:     m.pretty_print() <pre>================================== Ai Message ==================================\nName: Model\n\nSo you said you were researching ocean mammals?\n================================ Human Message =================================\nName: Lance\n\nYes, that's right.\n================================== Ai Message ==================================\nName: Model\n\nGreat, what would you like to learn about.\n================================ Human Message =================================\nName: Lance\n\nI want to learn about the best place to see Orcas in the US.\n</pre> In\u00a0[14]: Copied! <pre>def multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nllm_with_tools = llm.bind_tools([multiply])\n</pre> def multiply(a: int, b: int) -&gt; int:     \"\"\"Multiply a and b.      Args:         a: first int         b: second int     \"\"\"     return a * b  llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0) llm_with_tools = llm.bind_tools([multiply]) In\u00a0[15]: Copied! <pre>tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])\n</pre> tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")]) In\u00a0[16]: Copied! <pre>tool_call.tool_calls\n</pre> tool_call.tool_calls Out[16]: <pre>[{'name': 'multiply',\n  'args': {'a': 2, 'b': 3},\n  'id': 'call_mR4NSLIzhxRf082GYwX8TU5a',\n  'type': 'tool_call'}]</pre> In\u00a0[18]: Copied! <pre>from typing_extensions import TypedDict\nfrom langchain_core.messages import AnyMessage\n\nclass MessagesState(TypedDict):\n    messages: list[AnyMessage]\n</pre> from typing_extensions import TypedDict from langchain_core.messages import AnyMessage  class MessagesState(TypedDict):     messages: list[AnyMessage] In\u00a0[\u00a0]: Copied! <pre># Reducers\nfrom typing import Annotated\nfrom langgraph.graph.message import add_messages\n\nclass MessagesState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n</pre> # Reducers from typing import Annotated from langgraph.graph.message import add_messages  class MessagesState(TypedDict):     messages: Annotated[list[AnyMessage], add_messages] In\u00a0[20]: Copied! <pre>from langgraph.graph import MessagesState\n\nclass MessagesState(MessagesState):\n    # Add any keys needed beyond messages, which is pre-built \n    pass\n</pre> from langgraph.graph import MessagesState  class MessagesState(MessagesState):     # Add any keys needed beyond messages, which is pre-built      pass In\u00a0[21]: Copied! <pre># Initial state\ninitial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n                   ]\n\n# New message to add\nnew_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n\n# Test\nadd_messages(initial_messages , new_message)\n</pre> # Initial state initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),                     HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")                    ]  # New message to add new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")  # Test add_messages(initial_messages , new_message) Out[21]: <pre>[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='5b5d6119-a8cf-4fae-bf36-0ed383dffcab'),\n HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='9c9ec5de-2501-49e7-836e-6f2248ed6cf2'),\n AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='54963398-6f56-4f4f-863d-74a1767a326b')]</pre> In\u00a0[22]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n    \n# Node\ndef tool_calling_llm(state: MessagesState):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"tool_calling_llm\", tool_calling_llm)\nbuilder.add_edge(START, \"tool_calling_llm\")\nbuilder.add_edge(\"tool_calling_llm\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import StateGraph, START, END      # Node def tool_calling_llm(state: MessagesState):     return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}  # Build graph builder = StateGraph(MessagesState) builder.add_node(\"tool_calling_llm\", tool_calling_llm) builder.add_edge(START, \"tool_calling_llm\") builder.add_edge(\"tool_calling_llm\", END) graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[23]: Copied! <pre>messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\nfor m in messages['messages']:\n    m.pretty_print()\n</pre> messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")}) for m in messages['messages']:     m.pretty_print() <pre>================================ Human Message =================================\n\nHello!\n================================== Ai Message ==================================\n\nHello! How can I assist you today?\n</pre> In\u00a0[24]: Copied! <pre>messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\nfor m in messages['messages']:\n    m.pretty_print()\n</pre> messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}) for m in messages['messages']:     m.pretty_print() <pre>================================ Human Message =================================\n\nMultiply 2 and 3\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_IRJfOsgKpXJncu0JLyoGDo59)\n Call ID: call_IRJfOsgKpXJncu0JLyoGDo59\n  Args:\n    a: 2\n    b: 3\n</pre> In\u00a0[\u00a0]: Copied! <pre>from langchain_openai import ChatOpenAI\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\nllm = ChatOpenAI(model=\"gpt-4o\")\nllm_with_tools = llm.bind_tools([multiply])\n</pre> from langchain_openai import ChatOpenAI  def multiply(a: int, b: int) -&gt; int:     \"\"\"Multiply a and b.      Args:         a: first int         b: second int     \"\"\"     return a * b  llm = ChatOpenAI(model=\"gpt-4o\") llm_with_tools = llm.bind_tools([multiply]) In\u00a0[25]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph import MessagesState\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.prebuilt import tools_condition\n\n# Node\ndef tool_calling_llm(state: MessagesState):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"tool_calling_llm\", tool_calling_llm)\nbuilder.add_node(\"tools\", ToolNode([multiply]))\nbuilder.add_edge(START, \"tool_calling_llm\")\nbuilder.add_conditional_edges(\n    \"tool_calling_llm\",\n    # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools\n    # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END\n    tools_condition,\n)\nbuilder.add_edge(\"tools\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import StateGraph, START, END from langgraph.graph import MessagesState from langgraph.prebuilt import ToolNode from langgraph.prebuilt import tools_condition  # Node def tool_calling_llm(state: MessagesState):     return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}  # Build graph builder = StateGraph(MessagesState) builder.add_node(\"tool_calling_llm\", tool_calling_llm) builder.add_node(\"tools\", ToolNode([multiply])) builder.add_edge(START, \"tool_calling_llm\") builder.add_conditional_edges(     \"tool_calling_llm\",     # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools     # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END     tools_condition, ) builder.add_edge(\"tools\", END) graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[26]: Copied! <pre>from langchain_core.messages import HumanMessage\nmessages = [HumanMessage(content=\"Hello, what is 2 multiplied by 2?\")]\nmessages = graph.invoke({\"messages\": messages})\nfor m in messages['messages']:\n    m.pretty_print()\n</pre> from langchain_core.messages import HumanMessage messages = [HumanMessage(content=\"Hello, what is 2 multiplied by 2?\")] messages = graph.invoke({\"messages\": messages}) for m in messages['messages']:     m.pretty_print() <pre>================================ Human Message =================================\n\nHello, what is 2 multiplied by 2?\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_xjF9epHHGfS19XfhPYhLGl51)\n Call ID: call_xjF9epHHGfS19XfhPYhLGl51\n  Args:\n    a: 2\n    b: 2\n================================= Tool Message =================================\nName: multiply\n\n4\n</pre> In\u00a0[27]: Copied! <pre>os.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\"\n</pre> os.environ[\"LANGSMITH_TRACING\"] = \"true\" os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\" In\u00a0[28]: Copied! <pre>from langchain_openai import ChatOpenAI\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\n# This will be a tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Adds a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a + b\n\ndef divide(a: int, b: int) -&gt; float:\n    \"\"\"Divide a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a / b\n\ntools = [add, multiply, divide]\nllm = ChatOpenAI(model=\"gpt-4o\")\n\n# For this ipynb we set parallel tool calling to false as math generally is done sequentially, and this time we have 3 tools that can do math\n# the OpenAI model specifically defaults to parallel tool calling for efficiency, see https://python.langchain.com/docs/how_to/tool_calling_parallel/\n# play around with it and see how the model behaves with math equations!\nllm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)\n</pre> from langchain_openai import ChatOpenAI  def multiply(a: int, b: int) -&gt; int:     \"\"\"Multiply a and b.      Args:         a: first int         b: second int     \"\"\"     return a * b  # This will be a tool def add(a: int, b: int) -&gt; int:     \"\"\"Adds a and b.      Args:         a: first int         b: second int     \"\"\"     return a + b  def divide(a: int, b: int) -&gt; float:     \"\"\"Divide a and b.      Args:         a: first int         b: second int     \"\"\"     return a / b  tools = [add, multiply, divide] llm = ChatOpenAI(model=\"gpt-4o\")  # For this ipynb we set parallel tool calling to false as math generally is done sequentially, and this time we have 3 tools that can do math # the OpenAI model specifically defaults to parallel tool calling for efficiency, see https://python.langchain.com/docs/how_to/tool_calling_parallel/ # play around with it and see how the model behaves with math equations! llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False) In\u00a0[29]: Copied! <pre>from langgraph.graph import MessagesState\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n# System message\nsys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n\n# Node\ndef assistant(state: MessagesState):\n   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n</pre> from langgraph.graph import MessagesState from langchain_core.messages import HumanMessage, SystemMessage  # System message sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")  # Node def assistant(state: MessagesState):    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]} In\u00a0[30]: Copied! <pre>from langgraph.graph import START, StateGraph\nfrom langgraph.prebuilt import tools_condition\nfrom langgraph.prebuilt import ToolNode\nfrom IPython.display import Image, display\n\n# Graph\nbuilder = StateGraph(MessagesState)\n\n# Define nodes: these do the work\nbuilder.add_node(\"assistant\", assistant)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\n# Define edges: these determine how the control flow moves\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_conditional_edges(\n    \"assistant\",\n    # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools\n    # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END\n    tools_condition,\n)\nbuilder.add_edge(\"tools\", \"assistant\")\nreact_graph = builder.compile()\n\n# Show\ndisplay(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))\n</pre> from langgraph.graph import START, StateGraph from langgraph.prebuilt import tools_condition from langgraph.prebuilt import ToolNode from IPython.display import Image, display  # Graph builder = StateGraph(MessagesState)  # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools))  # Define edges: these determine how the control flow moves builder.add_edge(START, \"assistant\") builder.add_conditional_edges(     \"assistant\",     # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools     # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END     tools_condition, ) builder.add_edge(\"tools\", \"assistant\") react_graph = builder.compile()  # Show display(Image(react_graph.get_graph(xray=True).draw_mermaid_png())) In\u00a0[31]: Copied! <pre>messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2. Divide the output by 5\")]\nmessages = react_graph.invoke({\"messages\": messages})\n</pre> messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2. Divide the output by 5\")] messages = react_graph.invoke({\"messages\": messages}) In\u00a0[32]: Copied! <pre>for m in messages['messages']:\n    m.pretty_print()\n</pre> for m in messages['messages']:     m.pretty_print() <pre>================================ Human Message =================================\n\nAdd 3 and 4. Multiply the output by 2. Divide the output by 5\n================================== Ai Message ==================================\nTool Calls:\n  add (call_lF2Yf66xR4N9DFCO66taETel)\n Call ID: call_lF2Yf66xR4N9DFCO66taETel\n  Args:\n    a: 3\n    b: 4\n================================= Tool Message =================================\nName: add\n\n7\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_haYAb4khVvbvjkE97k4FMqGQ)\n Call ID: call_haYAb4khVvbvjkE97k4FMqGQ\n  Args:\n    a: 7\n    b: 2\n================================= Tool Message =================================\nName: multiply\n\n14\n================================== Ai Message ==================================\nTool Calls:\n  divide (call_escBXd3lBMUhGaMEfyJyAkHl)\n Call ID: call_escBXd3lBMUhGaMEfyJyAkHl\n  Args:\n    a: 14\n    b: 5\n================================= Tool Message =================================\nName: divide\n\n2.8\n================================== Ai Message ==================================\n\nThe result of adding 3 and 4, multiplying the output by 2, and then dividing by 5 is 2.8.\n</pre> In\u00a0[33]: Copied! <pre>from langgraph.checkpoint.memory import MemorySaver\nmemory = MemorySaver()\nreact_graph_memory = builder.compile(checkpointer=memory)\n</pre> from langgraph.checkpoint.memory import MemorySaver memory = MemorySaver() react_graph_memory = builder.compile(checkpointer=memory) In\u00a0[34]: Copied! <pre># Specify a thread\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Specify an input\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\n\n# Run\nmessages = react_graph_memory.invoke({\"messages\": messages},config)\nfor m in messages['messages']:\n    m.pretty_print()\n</pre> # Specify a thread config = {\"configurable\": {\"thread_id\": \"1\"}}  # Specify an input messages = [HumanMessage(content=\"Add 3 and 4.\")]  # Run messages = react_graph_memory.invoke({\"messages\": messages},config) for m in messages['messages']:     m.pretty_print() <pre>================================ Human Message =================================\n\nAdd 3 and 4.\n================================== Ai Message ==================================\nTool Calls:\n  add (call_RKEWPMnCOGah82RkC2iV6TUU)\n Call ID: call_RKEWPMnCOGah82RkC2iV6TUU\n  Args:\n    a: 3\n    b: 4\n================================= Tool Message =================================\nName: add\n\n7\n================================== Ai Message ==================================\n\nThe sum of 3 and 4 is 7.\n</pre> In\u00a0[35]: Copied! <pre>messages = [HumanMessage(content=\"Multiply that by 2.\")]\nmessages = react_graph_memory.invoke({\"messages\": messages}, config)\nfor m in messages['messages']:\n    m.pretty_print()\n</pre> messages = [HumanMessage(content=\"Multiply that by 2.\")] messages = react_graph_memory.invoke({\"messages\": messages}, config) for m in messages['messages']:     m.pretty_print() <pre>================================ Human Message =================================\n\nAdd 3 and 4.\n================================== Ai Message ==================================\nTool Calls:\n  add (call_RKEWPMnCOGah82RkC2iV6TUU)\n Call ID: call_RKEWPMnCOGah82RkC2iV6TUU\n  Args:\n    a: 3\n    b: 4\n================================= Tool Message =================================\nName: add\n\n7\n================================== Ai Message ==================================\n\nThe sum of 3 and 4 is 7.\n================================ Human Message =================================\n\nMultiply that by 2.\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_Jy5b8ZTYaKbCFJL7mY4Zb8AL)\n Call ID: call_Jy5b8ZTYaKbCFJL7mY4Zb8AL\n  Args:\n    a: 7\n    b: 2\n================================= Tool Message =================================\nName: multiply\n\n14\n================================== Ai Message ==================================\n\nThe result of multiplying 7 by 2 is 14.\n</pre> In\u00a0[1]: Copied! <pre>from typing_extensions import TypedDict\n\nclass TypedDictState(TypedDict):\n    foo: str\n    bar: str\n</pre> from typing_extensions import TypedDict  class TypedDictState(TypedDict):     foo: str     bar: str In\u00a0[2]: Copied! <pre>from typing import Literal\n\nclass TypedDictState(TypedDict):\n    name: str\n    mood: Literal[\"happy\",\"sad\"]\n</pre> from typing import Literal  class TypedDictState(TypedDict):     name: str     mood: Literal[\"happy\",\"sad\"] In\u00a0[3]: Copied! <pre>import random\nfrom IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\ndef node_1(state):\n    print(\"---Node 1---\")\n    return {\"name\": state['name'] + \" is ... \"}\n\ndef node_2(state):\n    print(\"---Node 2---\")\n    return {\"mood\": \"happy\"}\n\ndef node_3(state):\n    print(\"---Node 3---\")\n    return {\"mood\": \"sad\"}\n\ndef decide_mood(state) -&gt; Literal[\"node_2\", \"node_3\"]:\n        \n    # Here, let's just do a 50 / 50 split between nodes 2, 3\n    if random.random() &lt; 0.5:\n\n        # 50% of the time, we return Node 2\n        return \"node_2\"\n    \n    # 50% of the time, we return Node 3\n    return \"node_3\"\n\n# Build graph\nbuilder = StateGraph(TypedDictState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\"node_1\", decide_mood)\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> import random from IPython.display import Image, display from langgraph.graph import StateGraph, START, END  def node_1(state):     print(\"---Node 1---\")     return {\"name\": state['name'] + \" is ... \"}  def node_2(state):     print(\"---Node 2---\")     return {\"mood\": \"happy\"}  def node_3(state):     print(\"---Node 3---\")     return {\"mood\": \"sad\"}  def decide_mood(state) -&gt; Literal[\"node_2\", \"node_3\"]:              # Here, let's just do a 50 / 50 split between nodes 2, 3     if random.random() &lt; 0.5:          # 50% of the time, we return Node 2         return \"node_2\"          # 50% of the time, we return Node 3     return \"node_3\"  # Build graph builder = StateGraph(TypedDictState) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2) builder.add_node(\"node_3\", node_3)  # Logic builder.add_edge(START, \"node_1\") builder.add_conditional_edges(\"node_1\", decide_mood) builder.add_edge(\"node_2\", END) builder.add_edge(\"node_3\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) <pre>/Users/rohitkumar/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n</pre> In\u00a0[4]: Copied! <pre>graph.invoke({\"name\":\"Lance\"})\n</pre> graph.invoke({\"name\":\"Lance\"}) <pre>---Node 1---\n---Node 2---\n</pre> Out[4]: <pre>{'name': 'Lance is ... ', 'mood': 'happy'}</pre> In\u00a0[5]: Copied! <pre>from dataclasses import dataclass\n\n@dataclass\nclass DataclassState:\n    name: str\n    mood: Literal[\"happy\",\"sad\"]\n</pre> from dataclasses import dataclass  @dataclass class DataclassState:     name: str     mood: Literal[\"happy\",\"sad\"] In\u00a0[6]: Copied! <pre>def node_1(state):\n    print(\"---Node 1---\")\n    return {\"name\": state.name + \" is ... \"}\n\n# Build graph\nbuilder = StateGraph(DataclassState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\"node_1\", decide_mood)\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> def node_1(state):     print(\"---Node 1---\")     return {\"name\": state.name + \" is ... \"}  # Build graph builder = StateGraph(DataclassState) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2) builder.add_node(\"node_3\", node_3)  # Logic builder.add_edge(START, \"node_1\") builder.add_conditional_edges(\"node_1\", decide_mood) builder.add_edge(\"node_2\", END) builder.add_edge(\"node_3\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[7]: Copied! <pre>graph.invoke(DataclassState(name=\"Lance\",mood=\"sad\"))\n</pre> graph.invoke(DataclassState(name=\"Lance\",mood=\"sad\")) <pre>---Node 1---\n---Node 2---\n</pre> Out[7]: <pre>{'name': 'Lance is ... ', 'mood': 'happy'}</pre> In\u00a0[8]: Copied! <pre>dataclass_instance = DataclassState(name=\"Lance\", mood=\"mad\")\n</pre> dataclass_instance = DataclassState(name=\"Lance\", mood=\"mad\") In\u00a0[9]: Copied! <pre>from pydantic import BaseModel, field_validator, ValidationError\n\nclass PydanticState(BaseModel):\n    name: str\n    mood: str # \"happy\" or \"sad\" \n\n    @field_validator('mood')\n    @classmethod\n    def validate_mood(cls, value):\n        # Ensure the mood is either \"happy\" or \"sad\"\n        if value not in [\"happy\", \"sad\"]:\n            raise ValueError(\"Each mood must be either 'happy' or 'sad'\")\n        return value\n\ntry:\n    state = PydanticState(name=\"John Doe\", mood=\"mad\")\nexcept ValidationError as e:\n    print(\"Validation Error:\", e)\n</pre> from pydantic import BaseModel, field_validator, ValidationError  class PydanticState(BaseModel):     name: str     mood: str # \"happy\" or \"sad\"       @field_validator('mood')     @classmethod     def validate_mood(cls, value):         # Ensure the mood is either \"happy\" or \"sad\"         if value not in [\"happy\", \"sad\"]:             raise ValueError(\"Each mood must be either 'happy' or 'sad'\")         return value  try:     state = PydanticState(name=\"John Doe\", mood=\"mad\") except ValidationError as e:     print(\"Validation Error:\", e) <pre>Validation Error: 1 validation error for PydanticState\nmood\n  Value error, Each mood must be either 'happy' or 'sad' [type=value_error, input_value='mad', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n</pre> In\u00a0[10]: Copied! <pre># Build graph\nbuilder = StateGraph(PydanticState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\"node_1\", decide_mood)\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> # Build graph builder = StateGraph(PydanticState) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2) builder.add_node(\"node_3\", node_3)  # Logic builder.add_edge(START, \"node_1\") builder.add_conditional_edges(\"node_1\", decide_mood) builder.add_edge(\"node_2\", END) builder.add_edge(\"node_3\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[11]: Copied! <pre>graph.invoke(PydanticState(name=\"Lance\",mood=\"sad\"))\n</pre> graph.invoke(PydanticState(name=\"Lance\",mood=\"sad\")) <pre>---Node 1---\n---Node 2---\n</pre> Out[11]: <pre>{'name': 'Lance is ... ', 'mood': 'happy'}</pre> In\u00a0[12]: Copied! <pre>from typing_extensions import TypedDict\nfrom IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    foo: int\n\ndef node_1(state):\n    print(\"---Node 1---\")\n    return {\"foo\": state['foo'] + 1}\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from typing_extensions import TypedDict from IPython.display import Image, display from langgraph.graph import StateGraph, START, END  class State(TypedDict):     foo: int  def node_1(state):     print(\"---Node 1---\")     return {\"foo\": state['foo'] + 1}  # Build graph builder = StateGraph(State) builder.add_node(\"node_1\", node_1)  # Logic builder.add_edge(START, \"node_1\") builder.add_edge(\"node_1\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[13]: Copied! <pre>graph.invoke({\"foo\" : 1})\n</pre> graph.invoke({\"foo\" : 1}) <pre>---Node 1---\n</pre> Out[13]: <pre>{'foo': 2}</pre> In\u00a0[14]: Copied! <pre>class State(TypedDict):\n    foo: int\n\ndef node_1(state):\n    print(\"---Node 1---\")\n    return {\"foo\": state['foo'] + 1}\n\ndef node_2(state):\n    print(\"---Node 2---\")\n    return {\"foo\": state['foo'] + 1}\n\ndef node_3(state):\n    print(\"---Node 3---\")\n    return {\"foo\": state['foo'] + 1}\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_1\", \"node_3\")\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> class State(TypedDict):     foo: int  def node_1(state):     print(\"---Node 1---\")     return {\"foo\": state['foo'] + 1}  def node_2(state):     print(\"---Node 2---\")     return {\"foo\": state['foo'] + 1}  def node_3(state):     print(\"---Node 3---\")     return {\"foo\": state['foo'] + 1}  # Build graph builder = StateGraph(State) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2) builder.add_node(\"node_3\", node_3)  # Logic builder.add_edge(START, \"node_1\") builder.add_edge(\"node_1\", \"node_2\") builder.add_edge(\"node_1\", \"node_3\") builder.add_edge(\"node_2\", END) builder.add_edge(\"node_3\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[15]: Copied! <pre>from langgraph.errors import InvalidUpdateError\ntry:\n    graph.invoke({\"foo\" : 1})\nexcept InvalidUpdateError as e:\n    print(f\"InvalidUpdateError occurred: {e}\")\n</pre> from langgraph.errors import InvalidUpdateError try:     graph.invoke({\"foo\" : 1}) except InvalidUpdateError as e:     print(f\"InvalidUpdateError occurred: {e}\")  <pre>---Node 1---\n---Node 3---\n---Node 2---\nInvalidUpdateError occurred: At key 'foo': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE\n</pre> In\u00a0[16]: Copied! <pre>from operator import add\nfrom typing import Annotated\n\nclass State(TypedDict):\n    foo: Annotated[list[int], add]\n\ndef node_1(state):\n    print(\"---Node 1---\")\n    return {\"foo\": [state['foo'][0] + 1]}\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from operator import add from typing import Annotated  class State(TypedDict):     foo: Annotated[list[int], add]  def node_1(state):     print(\"---Node 1---\")     return {\"foo\": [state['foo'][0] + 1]}  # Build graph builder = StateGraph(State) builder.add_node(\"node_1\", node_1)  # Logic builder.add_edge(START, \"node_1\") builder.add_edge(\"node_1\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[17]: Copied! <pre>graph.invoke({\"foo\" : [1]})\n</pre> graph.invoke({\"foo\" : [1]}) <pre>---Node 1---\n</pre> Out[17]: <pre>{'foo': [1, 2]}</pre> In\u00a0[18]: Copied! <pre>def node_1(state):\n    print(\"---Node 1---\")\n    return {\"foo\": [state['foo'][-1] + 1]}\n\ndef node_2(state):\n    print(\"---Node 2---\")\n    return {\"foo\": [state['foo'][-1] + 1]}\n\ndef node_3(state):\n    print(\"---Node 3---\")\n    return {\"foo\": [state['foo'][-1] + 1]}\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_1\", \"node_3\")\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> def node_1(state):     print(\"---Node 1---\")     return {\"foo\": [state['foo'][-1] + 1]}  def node_2(state):     print(\"---Node 2---\")     return {\"foo\": [state['foo'][-1] + 1]}  def node_3(state):     print(\"---Node 3---\")     return {\"foo\": [state['foo'][-1] + 1]}  # Build graph builder = StateGraph(State) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2) builder.add_node(\"node_3\", node_3)  # Logic builder.add_edge(START, \"node_1\") builder.add_edge(\"node_1\", \"node_2\") builder.add_edge(\"node_1\", \"node_3\") builder.add_edge(\"node_2\", END) builder.add_edge(\"node_3\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[19]: Copied! <pre>graph.invoke({\"foo\" : [1]})\n</pre> graph.invoke({\"foo\" : [1]}) <pre>---Node 1---\n---Node 2---\n---Node 3---\n</pre> Out[19]: <pre>{'foo': [1, 2, 3, 3]}</pre> In\u00a0[20]: Copied! <pre>try:\n    graph.invoke({\"foo\" : None})\nexcept TypeError as e:\n    print(f\"TypeError occurred: {e}\")\n</pre> try:     graph.invoke({\"foo\" : None}) except TypeError as e:     print(f\"TypeError occurred: {e}\") <pre>TypeError occurred: can only concatenate list (not \"NoneType\") to list\n</pre> In\u00a0[23]: Copied! <pre>from typing import Union\ndef reduce_list(left: Union[list, None], right: Union[list, None]) -&gt; list:\n    \"\"\"Safely combine two lists, handling cases where either or both inputs might be None.\n\n    Args:\n        left (list | None): The first list to combine, or None.\n        right (list | None): The second list to combine, or None.\n\n    Returns:\n        list: A new list containing all elements from both input lists.\n               If an input is None, it's treated as an empty list.\n    \"\"\"\n    if not left:\n        left = []\n    if not right:\n        right = []\n    return left + right\n\nclass DefaultState(TypedDict):\n    foo: Annotated[list[int], add]\n\nclass CustomReducerState(TypedDict):\n    foo: Annotated[list[int], reduce_list]\n</pre> from typing import Union def reduce_list(left: Union[list, None], right: Union[list, None]) -&gt; list:     \"\"\"Safely combine two lists, handling cases where either or both inputs might be None.      Args:         left (list | None): The first list to combine, or None.         right (list | None): The second list to combine, or None.      Returns:         list: A new list containing all elements from both input lists.                If an input is None, it's treated as an empty list.     \"\"\"     if not left:         left = []     if not right:         right = []     return left + right  class DefaultState(TypedDict):     foo: Annotated[list[int], add]  class CustomReducerState(TypedDict):     foo: Annotated[list[int], reduce_list] In\u00a0[24]: Copied! <pre>def node_1(state):\n    print(\"---Node 1---\")\n    return {\"foo\": [2]}\n\n# Build graph\nbuilder = StateGraph(DefaultState)\nbuilder.add_node(\"node_1\", node_1)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\ntry:\n    print(graph.invoke({\"foo\" : None}))\nexcept TypeError as e:\n    print(f\"TypeError occurred: {e}\")\n</pre> def node_1(state):     print(\"---Node 1---\")     return {\"foo\": [2]}  # Build graph builder = StateGraph(DefaultState) builder.add_node(\"node_1\", node_1)  # Logic builder.add_edge(START, \"node_1\") builder.add_edge(\"node_1\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png()))  try:     print(graph.invoke({\"foo\" : None})) except TypeError as e:     print(f\"TypeError occurred: {e}\") <pre>TypeError occurred: can only concatenate list (not \"NoneType\") to list\n</pre> In\u00a0[25]: Copied! <pre># Build graph\nbuilder = StateGraph(CustomReducerState)\nbuilder.add_node(\"node_1\", node_1)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\ntry:\n    print(graph.invoke({\"foo\" : None}))\nexcept TypeError as e:\n    print(f\"TypeError occurred: {e}\")\n</pre> # Build graph builder = StateGraph(CustomReducerState) builder.add_node(\"node_1\", node_1)  # Logic builder.add_edge(START, \"node_1\") builder.add_edge(\"node_1\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png()))  try:     print(graph.invoke({\"foo\" : None})) except TypeError as e:     print(f\"TypeError occurred: {e}\") <pre>---Node 1---\n{'foo': [2]}\n</pre> In\u00a0[26]: Copied! <pre>from typing import Annotated\nfrom langgraph.graph import MessagesState\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\n\n# Define a custom TypedDict that includes a list of messages with add_messages reducer\nclass CustomMessagesState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    added_key_1: str\n    added_key_2: str\n    # etc\n\n# Use MessagesState, which includes the messages key with add_messages reducer\nclass ExtendedMessagesState(MessagesState):\n    # Add any keys needed beyond messages, which is pre-built \n    added_key_1: str\n    added_key_2: str\n    # etc\n</pre> from typing import Annotated from langgraph.graph import MessagesState from langchain_core.messages import AnyMessage from langgraph.graph.message import add_messages  # Define a custom TypedDict that includes a list of messages with add_messages reducer class CustomMessagesState(TypedDict):     messages: Annotated[list[AnyMessage], add_messages]     added_key_1: str     added_key_2: str     # etc  # Use MessagesState, which includes the messages key with add_messages reducer class ExtendedMessagesState(MessagesState):     # Add any keys needed beyond messages, which is pre-built      added_key_1: str     added_key_2: str     # etc In\u00a0[27]: Copied! <pre>from langgraph.graph.message import add_messages\nfrom langchain_core.messages import AIMessage, HumanMessage\n\n# Initial state\ninitial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n                   ]\n\n# New message to add\nnew_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n\n# Test\nadd_messages(initial_messages , new_message)\n</pre> from langgraph.graph.message import add_messages from langchain_core.messages import AIMessage, HumanMessage  # Initial state initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),                     HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")                    ]  # New message to add new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")  # Test add_messages(initial_messages , new_message) Out[27]: <pre>[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='45277698-45a9-4a69-beec-5dbdbefa0712'),\n HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='80e7a23b-2e0e-4454-ab43-46faff7609bc'),\n AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='d2d22d93-2f81-4832-9694-293cf59f4b29')]</pre> In\u00a0[28]: Copied! <pre># Initial state\ninitial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\", id=\"1\"),\n                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\", id=\"2\")\n                   ]\n\n# New message to add\nnew_message = HumanMessage(content=\"I'm looking for information on whales, specifically\", name=\"Lance\", id=\"2\")\n\n# Test\nadd_messages(initial_messages , new_message)\n</pre> # Initial state initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\", id=\"1\"),                     HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\", id=\"2\")                    ]  # New message to add new_message = HumanMessage(content=\"I'm looking for information on whales, specifically\", name=\"Lance\", id=\"2\")  # Test add_messages(initial_messages , new_message) Out[28]: <pre>[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='1'),\n HumanMessage(content=\"I'm looking for information on whales, specifically\", additional_kwargs={}, response_metadata={}, name='Lance', id='2')]</pre> In\u00a0[29]: Copied! <pre>from langchain_core.messages import RemoveMessage\n\n# Message list\nmessages = [AIMessage(\"Hi.\", name=\"Bot\", id=\"1\")]\nmessages.append(HumanMessage(\"Hi.\", name=\"Lance\", id=\"2\"))\nmessages.append(AIMessage(\"So you said you were researching ocean mammals?\", name=\"Bot\", id=\"3\"))\nmessages.append(HumanMessage(\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\", id=\"4\"))\n\n# Isolate messages to delete\ndelete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]]\nprint(delete_messages)\n</pre> from langchain_core.messages import RemoveMessage  # Message list messages = [AIMessage(\"Hi.\", name=\"Bot\", id=\"1\")] messages.append(HumanMessage(\"Hi.\", name=\"Lance\", id=\"2\")) messages.append(AIMessage(\"So you said you were researching ocean mammals?\", name=\"Bot\", id=\"3\")) messages.append(HumanMessage(\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\", id=\"4\"))  # Isolate messages to delete delete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]] print(delete_messages) <pre>[RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='1'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='2')]\n</pre> In\u00a0[30]: Copied! <pre>add_messages(messages , delete_messages)\n</pre> add_messages(messages , delete_messages) Out[30]: <pre>[AIMessage(content='So you said you were researching ocean mammals?', additional_kwargs={}, response_metadata={}, name='Bot', id='3'),\n HumanMessage(content='Yes, I know about whales. But what others should I learn about?', additional_kwargs={}, response_metadata={}, name='Lance', id='4')]</pre> In\u00a0[32]: Copied! <pre>from typing_extensions import TypedDict\nfrom IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\nclass OverallState(TypedDict):\n    foo: int\n\nclass PrivateState(TypedDict):\n    baz: int\n\ndef node_1(state: OverallState) -&gt; PrivateState:\n    print(\"---Node 1---\")\n    return {\"baz\": state['foo'] + 1}\n\ndef node_2(state: PrivateState) -&gt; OverallState:\n    print(\"---Node 2---\")\n    return {\"foo\": state['baz'] + 1}\n\n# Build graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", END)\n\n# Add\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from typing_extensions import TypedDict from IPython.display import Image, display from langgraph.graph import StateGraph, START, END  class OverallState(TypedDict):     foo: int  class PrivateState(TypedDict):     baz: int  def node_1(state: OverallState) -&gt; PrivateState:     print(\"---Node 1---\")     return {\"baz\": state['foo'] + 1}  def node_2(state: PrivateState) -&gt; OverallState:     print(\"---Node 2---\")     return {\"foo\": state['baz'] + 1}  # Build graph builder = StateGraph(OverallState) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2)  # Logic builder.add_edge(START, \"node_1\") builder.add_edge(\"node_1\", \"node_2\") builder.add_edge(\"node_2\", END)  # Add graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[33]: Copied! <pre>graph.invoke({\"foo\" : 1})\n</pre> graph.invoke({\"foo\" : 1}) <pre>---Node 1---\n---Node 2---\n</pre> Out[33]: <pre>{'foo': 3}</pre> In\u00a0[34]: Copied! <pre>class OverallState(TypedDict):\n    question: str\n    answer: str\n    notes: str\n\ndef thinking_node(state: OverallState):\n    return {\"answer\": \"bye\", \"notes\": \"... his name is Lance\"}\n\ndef answer_node(state: OverallState):\n    return {\"answer\": \"bye Lance\"}\n\ngraph = StateGraph(OverallState)\ngraph.add_node(\"answer_node\", answer_node)\ngraph.add_node(\"thinking_node\", thinking_node)\ngraph.add_edge(START, \"thinking_node\")\ngraph.add_edge(\"thinking_node\", \"answer_node\")\ngraph.add_edge(\"answer_node\", END)\n\ngraph = graph.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> class OverallState(TypedDict):     question: str     answer: str     notes: str  def thinking_node(state: OverallState):     return {\"answer\": \"bye\", \"notes\": \"... his name is Lance\"}  def answer_node(state: OverallState):     return {\"answer\": \"bye Lance\"}  graph = StateGraph(OverallState) graph.add_node(\"answer_node\", answer_node) graph.add_node(\"thinking_node\", thinking_node) graph.add_edge(START, \"thinking_node\") graph.add_edge(\"thinking_node\", \"answer_node\") graph.add_edge(\"answer_node\", END)  graph = graph.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[35]: Copied! <pre>graph.invoke({\"question\":\"hi\"})\n</pre> graph.invoke({\"question\":\"hi\"}) Out[35]: <pre>{'question': 'hi', 'answer': 'bye Lance', 'notes': '... his name is Lance'}</pre> In\u00a0[36]: Copied! <pre>class InputState(TypedDict):\n    question: str\n\nclass OutputState(TypedDict):\n    answer: str\n\nclass OverallState(TypedDict):\n    question: str\n    answer: str\n    notes: str\n\ndef thinking_node(state: InputState):\n    return {\"answer\": \"bye\", \"notes\": \"... his is name is Lance\"}\n\ndef answer_node(state: OverallState) -&gt; OutputState:\n    return {\"answer\": \"bye Lance\"}\n\ngraph = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\ngraph.add_node(\"answer_node\", answer_node)\ngraph.add_node(\"thinking_node\", thinking_node)\ngraph.add_edge(START, \"thinking_node\")\ngraph.add_edge(\"thinking_node\", \"answer_node\")\ngraph.add_edge(\"answer_node\", END)\n\ngraph = graph.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\ngraph.invoke({\"question\":\"hi\"})\n</pre> class InputState(TypedDict):     question: str  class OutputState(TypedDict):     answer: str  class OverallState(TypedDict):     question: str     answer: str     notes: str  def thinking_node(state: InputState):     return {\"answer\": \"bye\", \"notes\": \"... his is name is Lance\"}  def answer_node(state: OverallState) -&gt; OutputState:     return {\"answer\": \"bye Lance\"}  graph = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState) graph.add_node(\"answer_node\", answer_node) graph.add_node(\"thinking_node\", thinking_node) graph.add_edge(START, \"thinking_node\") graph.add_edge(\"thinking_node\", \"answer_node\") graph.add_edge(\"answer_node\", END)  graph = graph.compile()  # View display(Image(graph.get_graph().draw_mermaid_png()))  graph.invoke({\"question\":\"hi\"}) Out[36]: <pre>{'answer': 'bye Lance'}</pre> In\u00a0[38]: Copied! <pre>from pprint import pprint\nfrom langchain_core.messages import AIMessage, HumanMessage\nmessages = [AIMessage(f\"So you said you were researching ocean mammals?\", name=\"Bot\")]\nmessages.append(HumanMessage(f\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\"))\n\nfor m in messages:\n    m.pretty_print()\n</pre> from pprint import pprint from langchain_core.messages import AIMessage, HumanMessage messages = [AIMessage(f\"So you said you were researching ocean mammals?\", name=\"Bot\")] messages.append(HumanMessage(f\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\"))  for m in messages:     m.pretty_print() <pre>================================== Ai Message ==================================\nName: Bot\n\nSo you said you were researching ocean mammals?\n================================ Human Message =================================\nName: Lance\n\nYes, I know about whales. But what others should I learn about?\n</pre> In\u00a0[39]: Copied! <pre>from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o\")\nllm.invoke(messages)\n</pre> from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=\"gpt-4o\") llm.invoke(messages) Out[39]: <pre>AIMessage(content='In addition to whales, several other fascinating ocean mammals are worth learning about:\\n\\n1. **Dolphins**: Known for their intelligence and playful behavior, dolphins are part of the cetacean family, which also includes whales and porpoises. There are about 40 species of dolphins, including the well-known bottlenose dolphin.\\n\\n2. **Porpoises**: Often confused with dolphins, porpoises are smaller and have different physical features, such as shorter snouts and spade-shaped teeth. There are seven species of porpoises.\\n\\n3. **Seals**: These pinnipeds include a variety of species such as harbor seals, gray seals, and elephant seals. They are adapted to live both in water and on land, often seen lounging on beaches or ice.\\n\\n4. **Sea Lions**: Related to seals, sea lions are also pinnipeds but are typically larger and known for their loud barking. They have external ear flaps, unlike true seals.\\n\\n5. **Walruses**: Recognizable by their long tusks and whiskers, walruses are large pinnipeds that inhabit Arctic regions. They are social animals and often found in large groups.\\n\\n6. **Manatees and Dugongs**: These gentle, slow-moving creatures are part of the Sirenia order, often referred to as sea cows. They graze on seagrass and are found in warm coastal waters.\\n\\n7. **Otters**: Sea otters are marine mammals known for their tool-using behavior and thick fur, which is the densest of any animal. They play a crucial role in maintaining the health of kelp forests.\\n\\n8. **Polar Bears**: While primarily land-based, polar bears are considered marine mammals due to their reliance on the ocean for hunting seals. They are excellent swimmers and are well-adapted to the Arctic environment.\\n\\nEach of these ocean mammals plays a unique role in their ecosystem and has distinct characteristics that make them interesting to study.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 404, 'prompt_tokens': 39, 'total_tokens': 443, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CAnmHbbbw2bRzdOebFsxgyj5Xn6J3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f6978c45-5d33-482f-832c-42885499190d-0', usage_metadata={'input_tokens': 39, 'output_tokens': 404, 'total_tokens': 443, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})</pre> In\u00a0[40]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import MessagesState\nfrom langgraph.graph import StateGraph, START, END\n\n# Node\ndef chat_model_node(state: MessagesState):\n    return {\"messages\": llm.invoke(state[\"messages\"])}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"chat_model\", chat_model_node)\nbuilder.add_edge(START, \"chat_model\")\nbuilder.add_edge(\"chat_model\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import MessagesState from langgraph.graph import StateGraph, START, END  # Node def chat_model_node(state: MessagesState):     return {\"messages\": llm.invoke(state[\"messages\"])}  # Build graph builder = StateGraph(MessagesState) builder.add_node(\"chat_model\", chat_model_node) builder.add_edge(START, \"chat_model\") builder.add_edge(\"chat_model\", END) graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[41]: Copied! <pre>output = graph.invoke({'messages': messages})\nfor m in output['messages']:\n    m.pretty_print()\n</pre> output = graph.invoke({'messages': messages}) for m in output['messages']:     m.pretty_print() <pre>================================== Ai Message ==================================\nName: Bot\n\nSo you said you were researching ocean mammals?\n================================ Human Message =================================\nName: Lance\n\nYes, I know about whales. But what others should I learn about?\n================================== Ai Message ==================================\n\nIn addition to whales, there are several fascinating ocean mammals that you might want to learn about:\n\n1. **Dolphins**: Intelligent and social creatures, dolphins are known for their playful behavior and complex communication. They belong to the same family as whales, called cetaceans.\n\n2. **Porpoises**: Similar to dolphins but generally smaller, porpoises also belong to the cetacean family. They have shorter snouts and are typically more shy and reclusive.\n\n3. **Seals**: These semi-aquatic marine mammals are part of the pinniped family. They are adapted to both land and sea and are known for their thick fur and blubber to keep them warm.\n\n4. **Sea Lions**: Often confused with seals, sea lions are also pinnipeds but are distinguished by their external ear flaps and their ability to \"walk\" on land using their flippers.\n\n5. **Walruses**: Recognizable by their long tusks and whiskers, walruses are large pinnipeds that inhabit Arctic regions and are adapted to both land and icy waters.\n\n6. **Manatees**: Also called sea cows, manatees are gentle, slow-moving herbivores found in warm coastal waters and rivers.\n\n7. **Dugongs**: Similar to manatees, dugongs are found in the warm coastal waters of the Indian and Pacific Oceans. They are also herbivores but have a more streamlined body.\n\n8. **Sea Otters**: These small marine mammals are part of the weasel family and are known for their use of tools to open shellfish while floating on their backs.\n\nEach of these ocean mammals has unique characteristics and behaviors that make them interesting subjects for study and conservation.\n</pre> In\u00a0[42]: Copied! <pre>from langchain_core.messages import RemoveMessage\n\n# Nodes\ndef filter_messages(state: MessagesState):\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"messages\": delete_messages}\n\ndef chat_model_node(state: MessagesState):    \n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"filter\", filter_messages)\nbuilder.add_node(\"chat_model\", chat_model_node)\nbuilder.add_edge(START, \"filter\")\nbuilder.add_edge(\"filter\", \"chat_model\")\nbuilder.add_edge(\"chat_model\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from langchain_core.messages import RemoveMessage  # Nodes def filter_messages(state: MessagesState):     # Delete all but the 2 most recent messages     delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]     return {\"messages\": delete_messages}  def chat_model_node(state: MessagesState):         return {\"messages\": [llm.invoke(state[\"messages\"])]}  # Build graph builder = StateGraph(MessagesState) builder.add_node(\"filter\", filter_messages) builder.add_node(\"chat_model\", chat_model_node) builder.add_edge(START, \"filter\") builder.add_edge(\"filter\", \"chat_model\") builder.add_edge(\"chat_model\", END) graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[43]: Copied! <pre># Message list with a preamble\nmessages = [AIMessage(\"Hi.\", name=\"Bot\", id=\"1\")]\nmessages.append(HumanMessage(\"Hi.\", name=\"Lance\", id=\"2\"))\nmessages.append(AIMessage(\"So you said you were researching ocean mammals?\", name=\"Bot\", id=\"3\"))\nmessages.append(HumanMessage(\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\", id=\"4\"))\n\n# Invoke\noutput = graph.invoke({'messages': messages})\nfor m in output['messages']:\n    m.pretty_print()\n</pre> # Message list with a preamble messages = [AIMessage(\"Hi.\", name=\"Bot\", id=\"1\")] messages.append(HumanMessage(\"Hi.\", name=\"Lance\", id=\"2\")) messages.append(AIMessage(\"So you said you were researching ocean mammals?\", name=\"Bot\", id=\"3\")) messages.append(HumanMessage(\"Yes, I know about whales. But what others should I learn about?\", name=\"Lance\", id=\"4\"))  # Invoke output = graph.invoke({'messages': messages}) for m in output['messages']:     m.pretty_print() <pre>================================== Ai Message ==================================\nName: Bot\n\nSo you said you were researching ocean mammals?\n================================ Human Message =================================\nName: Lance\n\nYes, I know about whales. But what others should I learn about?\n================================== Ai Message ==================================\n\nAside from whales, there are a variety of fascinating ocean mammals you might explore:\n\n1. **Dolphins**: Known for their intelligence and playful behavior, dolphins are found in oceans worldwide and are closely related to whales.\n\n2. **Porpoises**: They are similar to dolphins but generally smaller and less social, with different facial structures and teeth.\n\n3. **Seals**: These pinnipeds have adapted to life in the water with streamlined bodies and flippers while maintaining the ability to come onto land.\n\n4. **Sea Lions**: Often confused with seals, sea lions are more vocal and have external ear flaps.\n\n5. **Walruses**: Known for their long tusks and whiskers, walruses are large pinnipeds that are well adapted to cold Arctic environments.\n\n6. **Manatees**: Also known as sea cows, these slow-moving herbivores are found in warm coastal waters and rivers.\n\n7. **Dugongs**: Similar to manatees, dugongs are marine mammals that graze on underwater grasses in the warm waters of the Indian and Pacific Oceans.\n\n8. **Sea Otters**: Found along the coasts of the northern and eastern North Pacific Ocean, sea otters are known for their use of tools and dense fur.\n\n9. **Polar Bears**: Though not fully aquatic, polar bears spend a significant amount of time in the water and rely heavily on marine ecosystems for hunting and survival.\n\nExploring these ocean mammals will give you a broader understanding of the diversity of life in marine environments and the unique adaptations that enable these animals to thrive.\n</pre> In\u00a0[44]: Copied! <pre># Node\ndef chat_model_node(state: MessagesState):\n    return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"chat_model\", chat_model_node)\nbuilder.add_edge(START, \"chat_model\")\nbuilder.add_edge(\"chat_model\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> # Node def chat_model_node(state: MessagesState):     return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}  # Build graph builder = StateGraph(MessagesState) builder.add_node(\"chat_model\", chat_model_node) builder.add_edge(START, \"chat_model\") builder.add_edge(\"chat_model\", END) graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[45]: Copied! <pre>messages.append(output['messages'][-1])\nmessages.append(HumanMessage(f\"Tell me more about Narwhals!\", name=\"Lance\"))\n</pre> messages.append(output['messages'][-1]) messages.append(HumanMessage(f\"Tell me more about Narwhals!\", name=\"Lance\")) In\u00a0[46]: Copied! <pre>for m in messages:\n    m.pretty_print()\n</pre> for m in messages:     m.pretty_print() <pre>================================== Ai Message ==================================\nName: Bot\n\nHi.\n================================ Human Message =================================\nName: Lance\n\nHi.\n================================== Ai Message ==================================\nName: Bot\n\nSo you said you were researching ocean mammals?\n================================ Human Message =================================\nName: Lance\n\nYes, I know about whales. But what others should I learn about?\n================================== Ai Message ==================================\n\nAside from whales, there are a variety of fascinating ocean mammals you might explore:\n\n1. **Dolphins**: Known for their intelligence and playful behavior, dolphins are found in oceans worldwide and are closely related to whales.\n\n2. **Porpoises**: They are similar to dolphins but generally smaller and less social, with different facial structures and teeth.\n\n3. **Seals**: These pinnipeds have adapted to life in the water with streamlined bodies and flippers while maintaining the ability to come onto land.\n\n4. **Sea Lions**: Often confused with seals, sea lions are more vocal and have external ear flaps.\n\n5. **Walruses**: Known for their long tusks and whiskers, walruses are large pinnipeds that are well adapted to cold Arctic environments.\n\n6. **Manatees**: Also known as sea cows, these slow-moving herbivores are found in warm coastal waters and rivers.\n\n7. **Dugongs**: Similar to manatees, dugongs are marine mammals that graze on underwater grasses in the warm waters of the Indian and Pacific Oceans.\n\n8. **Sea Otters**: Found along the coasts of the northern and eastern North Pacific Ocean, sea otters are known for their use of tools and dense fur.\n\n9. **Polar Bears**: Though not fully aquatic, polar bears spend a significant amount of time in the water and rely heavily on marine ecosystems for hunting and survival.\n\nExploring these ocean mammals will give you a broader understanding of the diversity of life in marine environments and the unique adaptations that enable these animals to thrive.\n================================ Human Message =================================\nName: Lance\n\nTell me more about Narwhals!\n</pre> In\u00a0[47]: Copied! <pre># Invoke, using message filtering\noutput = graph.invoke({'messages': messages})\nfor m in output['messages']:\n    m.pretty_print()\n</pre> # Invoke, using message filtering output = graph.invoke({'messages': messages}) for m in output['messages']:     m.pretty_print() <pre>================================== Ai Message ==================================\nName: Bot\n\nHi.\n================================ Human Message =================================\nName: Lance\n\nHi.\n================================== Ai Message ==================================\nName: Bot\n\nSo you said you were researching ocean mammals?\n================================ Human Message =================================\nName: Lance\n\nYes, I know about whales. But what others should I learn about?\n================================== Ai Message ==================================\n\nAside from whales, there are a variety of fascinating ocean mammals you might explore:\n\n1. **Dolphins**: Known for their intelligence and playful behavior, dolphins are found in oceans worldwide and are closely related to whales.\n\n2. **Porpoises**: They are similar to dolphins but generally smaller and less social, with different facial structures and teeth.\n\n3. **Seals**: These pinnipeds have adapted to life in the water with streamlined bodies and flippers while maintaining the ability to come onto land.\n\n4. **Sea Lions**: Often confused with seals, sea lions are more vocal and have external ear flaps.\n\n5. **Walruses**: Known for their long tusks and whiskers, walruses are large pinnipeds that are well adapted to cold Arctic environments.\n\n6. **Manatees**: Also known as sea cows, these slow-moving herbivores are found in warm coastal waters and rivers.\n\n7. **Dugongs**: Similar to manatees, dugongs are marine mammals that graze on underwater grasses in the warm waters of the Indian and Pacific Oceans.\n\n8. **Sea Otters**: Found along the coasts of the northern and eastern North Pacific Ocean, sea otters are known for their use of tools and dense fur.\n\n9. **Polar Bears**: Though not fully aquatic, polar bears spend a significant amount of time in the water and rely heavily on marine ecosystems for hunting and survival.\n\nExploring these ocean mammals will give you a broader understanding of the diversity of life in marine environments and the unique adaptations that enable these animals to thrive.\n================================ Human Message =================================\nName: Lance\n\nTell me more about Narwhals!\n================================== Ai Message ==================================\n\nCertainly! Narwhals are fascinating marine mammals known for their unique and distinctive long, spiral tusks that resemble horns, earning them the nickname \"unicorns of the sea.\" Here are some key points about narwhals:\n\n1. **Scientific Classification**: \n   - Scientific name: Monodon monoceros.\n   - Family: Monodontidae, which also includes belugas.\n\n2. **Physical Characteristics**:\n   - They are medium-sized whales with a mottled black and white skin pattern.\n   - Males typically range from 4 to 5 meters (13 to 16 feet) in length, not including the tusk.\n   - Females are generally smaller and rarely have tusks.\n\n3. **The Tusk**:\n   - Typically, it is the male narwhals that exhibit a prominent tusk, an elongated upper left canine that can grow up to 3 meters (about 10 feet) long.\n   - It is not uncommon for a male to have two tusks if both canine teeth grow out, although it's rare.\n   - The exact purpose of the tusk is still debated among scientists; it may play a role in mating rituals, dominance displays, or sensory functions.\n\n4. **Habitat and Distribution**:\n   - Narwhals are found predominantly in Arctic waters around Canada, Greenland, Norway, and Russia.\n   - They prefer deep, cold waters and are often found in areas with heavy ice cover.\n\n5. **Diet**:\n   - Their diet mainly consists of fish, squid, and shrimp. They are known to dive to great depths in search of food, sometimes reaching 1,500 meters (about 4,900 feet).\n\n6. **Behavior and Social Structure**:\n   - Narwhals are social animals, often found in groups called pods that range from a few individuals to several dozen.\n   - They are known for their deep-diving abilities and are relatively elusive compared to other whale species.\n\n7. **Conservation Status**:\n   - Currently, narwhals are listed as Near Threatened due to climate change, increased human activities in the Arctic, and hunting pressures.\n   - The melting of Arctic ice poses a significant threat as it alters their habitat and exposes them to more predation and human interaction.\n\n8. **Cultural Significance**:\n   - Indigenous communities in the Arctic regions have traditionally hunted narwhals for their meat, skin, and tusks, and they hold cultural significance.\n\nNarwhals are a subject of much scientific study because of their unique adaptations to the harsh Arctic environment and their mysterious tusks. They continue to capture the imagination of people around the world due to their elusive nature and striking appearance.\n</pre> In\u00a0[48]: Copied! <pre>from langchain_core.messages import trim_messages\n\n# Node\ndef chat_model_node(state: MessagesState):\n    messages = trim_messages(\n            state[\"messages\"],\n            max_tokens=100,\n            strategy=\"last\",\n            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n            allow_partial=False,\n        )\n    return {\"messages\": [llm.invoke(messages)]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"chat_model\", chat_model_node)\nbuilder.add_edge(START, \"chat_model\")\nbuilder.add_edge(\"chat_model\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from langchain_core.messages import trim_messages  # Node def chat_model_node(state: MessagesState):     messages = trim_messages(             state[\"messages\"],             max_tokens=100,             strategy=\"last\",             token_counter=ChatOpenAI(model=\"gpt-4o\"),             allow_partial=False,         )     return {\"messages\": [llm.invoke(messages)]}  # Build graph builder = StateGraph(MessagesState) builder.add_node(\"chat_model\", chat_model_node) builder.add_edge(START, \"chat_model\") builder.add_edge(\"chat_model\", END) graph = builder.compile()  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[49]: Copied! <pre>messages.append(output['messages'][-1])\nmessages.append(HumanMessage(f\"Tell me where Orcas live!\", name=\"Lance\"))\n</pre> messages.append(output['messages'][-1]) messages.append(HumanMessage(f\"Tell me where Orcas live!\", name=\"Lance\")) In\u00a0[50]: Copied! <pre># Example of trimming messages\ntrim_messages(\n            messages,\n            max_tokens=100,\n            strategy=\"last\",\n            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n            allow_partial=False\n        )\n</pre> # Example of trimming messages trim_messages(             messages,             max_tokens=100,             strategy=\"last\",             token_counter=ChatOpenAI(model=\"gpt-4o\"),             allow_partial=False         ) Out[50]: <pre>[HumanMessage(content='Tell me where Orcas live!', additional_kwargs={}, response_metadata={}, name='Lance')]</pre> In\u00a0[51]: Copied! <pre># Invoke, using message trimming in the chat_model_node \nmessages_out_trim = graph.invoke({'messages': messages})\n</pre> # Invoke, using message trimming in the chat_model_node  messages_out_trim = graph.invoke({'messages': messages}) In\u00a0[52]: Copied! <pre>messages_out_trim\n</pre> messages_out_trim Out[52]: <pre>{'messages': [AIMessage(content='Hi.', additional_kwargs={}, response_metadata={}, name='Bot', id='1'),\n  HumanMessage(content='Hi.', additional_kwargs={}, response_metadata={}, name='Lance', id='2'),\n  AIMessage(content='So you said you were researching ocean mammals?', additional_kwargs={}, response_metadata={}, name='Bot', id='3'),\n  HumanMessage(content='Yes, I know about whales. But what others should I learn about?', additional_kwargs={}, response_metadata={}, name='Lance', id='4'),\n  AIMessage(content='Aside from whales, there are a variety of fascinating ocean mammals you might explore:\\n\\n1. **Dolphins**: Known for their intelligence and playful behavior, dolphins are found in oceans worldwide and are closely related to whales.\\n\\n2. **Porpoises**: They are similar to dolphins but generally smaller and less social, with different facial structures and teeth.\\n\\n3. **Seals**: These pinnipeds have adapted to life in the water with streamlined bodies and flippers while maintaining the ability to come onto land.\\n\\n4. **Sea Lions**: Often confused with seals, sea lions are more vocal and have external ear flaps.\\n\\n5. **Walruses**: Known for their long tusks and whiskers, walruses are large pinnipeds that are well adapted to cold Arctic environments.\\n\\n6. **Manatees**: Also known as sea cows, these slow-moving herbivores are found in warm coastal waters and rivers.\\n\\n7. **Dugongs**: Similar to manatees, dugongs are marine mammals that graze on underwater grasses in the warm waters of the Indian and Pacific Oceans.\\n\\n8. **Sea Otters**: Found along the coasts of the northern and eastern North Pacific Ocean, sea otters are known for their use of tools and dense fur.\\n\\n9. **Polar Bears**: Though not fully aquatic, polar bears spend a significant amount of time in the water and rely heavily on marine ecosystems for hunting and survival.\\n\\nExploring these ocean mammals will give you a broader understanding of the diversity of life in marine environments and the unique adaptations that enable these animals to thrive.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 329, 'prompt_tokens': 39, 'total_tokens': 368, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAnnF8IsHQunjUwbpgeCYrGU1BR6p', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9fa9b690-f316-42c1-8644-94045d675fec-0', usage_metadata={'input_tokens': 39, 'output_tokens': 329, 'total_tokens': 368, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n  HumanMessage(content='Tell me more about Narwhals!', additional_kwargs={}, response_metadata={}, name='Lance', id='9e2a03f2-bb22-4a8c-bf8f-7af968ef30e8'),\n  AIMessage(content='Certainly! Narwhals are fascinating marine mammals known for their unique and distinctive long, spiral tusks that resemble horns, earning them the nickname \"unicorns of the sea.\" Here are some key points about narwhals:\\n\\n1. **Scientific Classification**: \\n   - Scientific name: Monodon monoceros.\\n   - Family: Monodontidae, which also includes belugas.\\n\\n2. **Physical Characteristics**:\\n   - They are medium-sized whales with a mottled black and white skin pattern.\\n   - Males typically range from 4 to 5 meters (13 to 16 feet) in length, not including the tusk.\\n   - Females are generally smaller and rarely have tusks.\\n\\n3. **The Tusk**:\\n   - Typically, it is the male narwhals that exhibit a prominent tusk, an elongated upper left canine that can grow up to 3 meters (about 10 feet) long.\\n   - It is not uncommon for a male to have two tusks if both canine teeth grow out, although it\\'s rare.\\n   - The exact purpose of the tusk is still debated among scientists; it may play a role in mating rituals, dominance displays, or sensory functions.\\n\\n4. **Habitat and Distribution**:\\n   - Narwhals are found predominantly in Arctic waters around Canada, Greenland, Norway, and Russia.\\n   - They prefer deep, cold waters and are often found in areas with heavy ice cover.\\n\\n5. **Diet**:\\n   - Their diet mainly consists of fish, squid, and shrimp. They are known to dive to great depths in search of food, sometimes reaching 1,500 meters (about 4,900 feet).\\n\\n6. **Behavior and Social Structure**:\\n   - Narwhals are social animals, often found in groups called pods that range from a few individuals to several dozen.\\n   - They are known for their deep-diving abilities and are relatively elusive compared to other whale species.\\n\\n7. **Conservation Status**:\\n   - Currently, narwhals are listed as Near Threatened due to climate change, increased human activities in the Arctic, and hunting pressures.\\n   - The melting of Arctic ice poses a significant threat as it alters their habitat and exposes them to more predation and human interaction.\\n\\n8. **Cultural Significance**:\\n   - Indigenous communities in the Arctic regions have traditionally hunted narwhals for their meat, skin, and tusks, and they hold cultural significance.\\n\\nNarwhals are a subject of much scientific study because of their unique adaptations to the harsh Arctic environment and their mysterious tusks. They continue to capture the imagination of people around the world due to their elusive nature and striking appearance.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 548, 'prompt_tokens': 17, 'total_tokens': 565, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAnolIywAIBYzE8qJnBUmclAAN5cG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--767df82f-bad0-4bdd-b285-774c44472706-0', usage_metadata={'input_tokens': 17, 'output_tokens': 548, 'total_tokens': 565, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n  HumanMessage(content='Tell me where Orcas live!', additional_kwargs={}, response_metadata={}, name='Lance', id='7e5f72be-4ce6-4868-aeb1-14e86098acf3'),\n  AIMessage(content=\"Orcas, also known as killer whales, are found in all of the world's oceans, making them one of the most widespread marine mammals. They inhabit a variety of marine environments, including:\\n\\n1. **Polar Regions**: Orcas are often spotted in the icy waters of the Arctic and Antarctic, where they hunt seals, fish, and other marine mammals.\\n\\n2. **Temperate Regions**: These whales are common in the temperate waters of the North Pacific, North Atlantic, and parts of the Southern Hemisphere. Notable populations can be found off the coasts of North America, such as in the Pacific Northwest and along the coast of Norway.\\n\\n3. **Tropical Regions**: While less common, orcas can also be found in warmer tropical waters. They tend to prefer areas with abundant prey.\\n\\n4. **Coastal Waters**: Orcas are frequently sighted along coastlines and are known to enter bays, estuaries, and river mouths while hunting for food.\\n\\n5. **Open Ocean**: Some populations of orcas are pelagic, meaning they inhabit the open ocean, traveling vast distances in search of food.\\n\\nOrcas are highly adaptable and have diverse diets, which allows them to thrive in these varied environments. Their presence often depends on the availability of prey, ranging from fish and squid to seals and other marine mammals.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 272, 'prompt_tokens': 16, 'total_tokens': 288, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAnwVS9YVzt3P4FWVUHMQYtmQdYMy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dd6e3677-f1e2-4267-87c4-7c7047f78b63-0', usage_metadata={'input_tokens': 16, 'output_tokens': 272, 'total_tokens': 288, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}</pre> In\u00a0[53]: Copied! <pre>from langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\",temperature=0)\n</pre> from langchain_openai import ChatOpenAI model = ChatOpenAI(model=\"gpt-4o\",temperature=0) In\u00a0[54]: Copied! <pre>from langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str\n</pre> from langgraph.graph import MessagesState class State(MessagesState):     summary: str In\u00a0[55]: Copied! <pre>from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n\n# Define the logic to call the model\ndef call_model(state: State):\n    \n    # Get summary if it exists\n    summary = state.get(\"summary\", \"\")\n\n    # If there is summary, then we add it\n    if summary:\n        \n        # Add summary to system message\n        system_message = f\"Summary of conversation earlier: {summary}\"\n\n        # Append summary to any newer messages\n        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n    \n    else:\n        messages = state[\"messages\"]\n    \n    response = model.invoke(messages)\n    return {\"messages\": response}\n</pre> from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage  # Define the logic to call the model def call_model(state: State):          # Get summary if it exists     summary = state.get(\"summary\", \"\")      # If there is summary, then we add it     if summary:                  # Add summary to system message         system_message = f\"Summary of conversation earlier: {summary}\"          # Append summary to any newer messages         messages = [SystemMessage(content=system_message)] + state[\"messages\"]          else:         messages = state[\"messages\"]          response = model.invoke(messages)     return {\"messages\": response} In\u00a0[56]: Copied! <pre>def summarize_conversation(state: State):\n    \n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt \n    if summary:\n        \n        # A summary already exists\n        summary_message = (\n            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n        \n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n    \n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n</pre> def summarize_conversation(state: State):          # First, we get any existing summary     summary = state.get(\"summary\", \"\")      # Create our summarization prompt      if summary:                  # A summary already exists         summary_message = (             f\"This is summary of the conversation to date: {summary}\\n\\n\"             \"Extend the summary by taking into account the new messages above:\"         )              else:         summary_message = \"Create a summary of the conversation above:\"      # Add prompt to our history     messages = state[\"messages\"] + [HumanMessage(content=summary_message)]     response = model.invoke(messages)          # Delete all but the 2 most recent messages     delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]     return {\"summary\": response.content, \"messages\": delete_messages} In\u00a0[57]: Copied! <pre>from langgraph.graph import END\nfrom typing_extensions import Literal\n# Determine whether to end or summarize the conversation\ndef should_continue(state: State) -&gt; Literal [\"summarize_conversation\",END]:\n    \n    \"\"\"Return the next node to execute.\"\"\"\n    \n    messages = state[\"messages\"]\n    \n    # If there are more than six messages, then we summarize the conversation\n    if len(messages) &gt; 6:\n        return \"summarize_conversation\"\n    \n    # Otherwise we can just end\n    return END\n</pre> from langgraph.graph import END from typing_extensions import Literal # Determine whether to end or summarize the conversation def should_continue(state: State) -&gt; Literal [\"summarize_conversation\",END]:          \"\"\"Return the next node to execute.\"\"\"          messages = state[\"messages\"]          # If there are more than six messages, then we summarize the conversation     if len(messages) &gt; 6:         return \"summarize_conversation\"          # Otherwise we can just end     return END In\u00a0[58]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START\n\n# Define a new graph\nworkflow = StateGraph(State)\nworkflow.add_node(\"conversation\", call_model)\nworkflow.add_node(summarize_conversation)\n\n# Set the entrypoint as conversation\nworkflow.add_edge(START, \"conversation\")\nworkflow.add_conditional_edges(\"conversation\", should_continue)\nworkflow.add_edge(\"summarize_conversation\", END)\n\n# Compile\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START  # Define a new graph workflow = StateGraph(State) workflow.add_node(\"conversation\", call_model) workflow.add_node(summarize_conversation)  # Set the entrypoint as conversation workflow.add_edge(START, \"conversation\") workflow.add_conditional_edges(\"conversation\", should_continue) workflow.add_edge(\"summarize_conversation\", END)  # Compile memory = MemorySaver() graph = workflow.compile(checkpointer=memory) display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[59]: Copied! <pre># Create a thread\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Start conversation\ninput_message = HumanMessage(content=\"hi! I'm Lance\")\noutput = graph.invoke({\"messages\": [input_message]}, config) \nfor m in output['messages'][-1:]:\n    m.pretty_print()\n\ninput_message = HumanMessage(content=\"what's my name?\")\noutput = graph.invoke({\"messages\": [input_message]}, config) \nfor m in output['messages'][-1:]:\n    m.pretty_print()\n\ninput_message = HumanMessage(content=\"i like the 49ers!\")\noutput = graph.invoke({\"messages\": [input_message]}, config) \nfor m in output['messages'][-1:]:\n    m.pretty_print()\n</pre> # Create a thread config = {\"configurable\": {\"thread_id\": \"1\"}}  # Start conversation input_message = HumanMessage(content=\"hi! I'm Lance\") output = graph.invoke({\"messages\": [input_message]}, config)  for m in output['messages'][-1:]:     m.pretty_print()  input_message = HumanMessage(content=\"what's my name?\") output = graph.invoke({\"messages\": [input_message]}, config)  for m in output['messages'][-1:]:     m.pretty_print()  input_message = HumanMessage(content=\"i like the 49ers!\") output = graph.invoke({\"messages\": [input_message]}, config)  for m in output['messages'][-1:]:     m.pretty_print() <pre>================================== Ai Message ==================================\n\nHello Lance! How can I assist you today?\n================================== Ai Message ==================================\n\nYou mentioned that your name is Lance. How can I help you today?\n================================== Ai Message ==================================\n\nThat's great! The San Francisco 49ers have a rich history and a passionate fan base. Do you have a favorite player or a memorable game that you particularly enjoyed?\n</pre> In\u00a0[60]: Copied! <pre>graph.get_state(config).values.get(\"summary\",\"\")\n</pre> graph.get_state(config).values.get(\"summary\",\"\") Out[60]: <pre>''</pre> In\u00a0[61]: Copied! <pre>input_message = HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\")\noutput = graph.invoke({\"messages\": [input_message]}, config) \nfor m in output['messages'][-1:]:\n    m.pretty_print()\n</pre> input_message = HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\") output = graph.invoke({\"messages\": [input_message]}, config)  for m in output['messages'][-1:]:     m.pretty_print() <pre>================================== Ai Message ==================================\n\nYes, as of September 2023, Nick Bosa became the highest-paid defensive player in NFL history. He signed a five-year contract extension with the San Francisco 49ers worth $170 million, with $122.5 million guaranteed. Bosa is known for his exceptional skills as a defensive end and has been a key player for the 49ers.\n</pre> In\u00a0[62]: Copied! <pre>graph.get_state(config).values.get(\"summary\",\"\")\n</pre> graph.get_state(config).values.get(\"summary\",\"\") Out[62]: <pre>'Lance introduced himself and expressed his support for the San Francisco 49ers, mentioning a particular liking for Nick Bosa. The conversation highlighted that Nick Bosa became the highest-paid defensive player in NFL history as of September 2023, with a significant contract extension with the 49ers.'</pre> In\u00a0[63]: Copied! <pre>import sqlite3\n# In memory\nconn = sqlite3.connect(\":memory:\", check_same_thread = False)\n</pre> import sqlite3 # In memory conn = sqlite3.connect(\":memory:\", check_same_thread = False) In\u00a0[64]: Copied! <pre># pull file if it doesn't exist and connect to local db\n!mkdir -p state_db &amp;&amp; [ ! -f state_db/example.db ] &amp;&amp; wget -P state_db https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\n\ndb_path = \"state_db/example.db\"\nconn = sqlite3.connect(db_path, check_same_thread=False)\n</pre> # pull file if it doesn't exist and connect to local db !mkdir -p state_db &amp;&amp; [ ! -f state_db/example.db ] &amp;&amp; wget -P state_db https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db  db_path = \"state_db/example.db\" conn = sqlite3.connect(db_path, check_same_thread=False) <pre>--2025-09-01 07:22:36--  https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db\nResolving github.com (github.com)... 64:ff9b::14cf:4952, 20.207.73.82\nConnecting to github.com (github.com)|64:ff9b::14cf:4952|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/langchain-ai/langchain-academy/main/module-2/state_db/example.db [following]\n--2025-09-01 07:22:37--  https://raw.githubusercontent.com/langchain-ai/langchain-academy/main/module-2/state_db/example.db\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8002::154, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 110592 (108K) [application/octet-stream]\nSaving to: \u2018state_db/example.db\u2019\n\nexample.db          100%[===================&gt;] 108.00K  --.-KB/s    in 0.1s    \n\n2025-09-01 07:22:38 (864 KB/s) - \u2018state_db/example.db\u2019 saved [110592/110592]\n\n</pre> In\u00a0[67]: Copied! <pre># Here is our checkpointer \nfrom langgraph.checkpoint.sqlite import SqliteSaver\nmemory = SqliteSaver(conn)\n</pre> # Here is our checkpointer  from langgraph.checkpoint.sqlite import SqliteSaver memory = SqliteSaver(conn) In\u00a0[68]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n\nfrom langgraph.graph import END\nfrom langgraph.graph import MessagesState\n\nmodel = ChatOpenAI(model=\"gpt-4o\",temperature=0)\n\nclass State(MessagesState):\n    summary: str\n\n# Define the logic to call the model\ndef call_model(state: State):\n    \n    # Get summary if it exists\n    summary = state.get(\"summary\", \"\")\n\n    # If there is summary, then we add it\n    if summary:\n        \n        # Add summary to system message\n        system_message = f\"Summary of conversation earlier: {summary}\"\n\n        # Append summary to any newer messages\n        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n    \n    else:\n        messages = state[\"messages\"]\n    \n    response = model.invoke(messages)\n    return {\"messages\": response}\n\ndef summarize_conversation(state: State):\n    \n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt \n    if summary:\n        \n        # A summary already exists\n        summary_message = (\n            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n        \n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n    \n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n\n# Determine whether to end or summarize the conversation\ndef should_continue(state: State):\n    \n    \"\"\"Return the next node to execute.\"\"\"\n    \n    messages = state[\"messages\"]\n    \n    # If there are more than six messages, then we summarize the conversation\n    if len(messages) &gt; 6:\n        return \"summarize_conversation\"\n    \n    # Otherwise we can just end\n    return END\n</pre> from langchain_openai import ChatOpenAI from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage  from langgraph.graph import END from langgraph.graph import MessagesState  model = ChatOpenAI(model=\"gpt-4o\",temperature=0)  class State(MessagesState):     summary: str  # Define the logic to call the model def call_model(state: State):          # Get summary if it exists     summary = state.get(\"summary\", \"\")      # If there is summary, then we add it     if summary:                  # Add summary to system message         system_message = f\"Summary of conversation earlier: {summary}\"          # Append summary to any newer messages         messages = [SystemMessage(content=system_message)] + state[\"messages\"]          else:         messages = state[\"messages\"]          response = model.invoke(messages)     return {\"messages\": response}  def summarize_conversation(state: State):          # First, we get any existing summary     summary = state.get(\"summary\", \"\")      # Create our summarization prompt      if summary:                  # A summary already exists         summary_message = (             f\"This is summary of the conversation to date: {summary}\\n\\n\"             \"Extend the summary by taking into account the new messages above:\"         )              else:         summary_message = \"Create a summary of the conversation above:\"      # Add prompt to our history     messages = state[\"messages\"] + [HumanMessage(content=summary_message)]     response = model.invoke(messages)          # Delete all but the 2 most recent messages     delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]     return {\"summary\": response.content, \"messages\": delete_messages}  # Determine whether to end or summarize the conversation def should_continue(state: State):          \"\"\"Return the next node to execute.\"\"\"          messages = state[\"messages\"]          # If there are more than six messages, then we summarize the conversation     if len(messages) &gt; 6:         return \"summarize_conversation\"          # Otherwise we can just end     return END In\u00a0[69]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START\n\n# Define a new graph\nworkflow = StateGraph(State)\nworkflow.add_node(\"conversation\", call_model)\nworkflow.add_node(summarize_conversation)\n\n# Set the entrypoint as conversation\nworkflow.add_edge(START, \"conversation\")\nworkflow.add_conditional_edges(\"conversation\", should_continue)\nworkflow.add_edge(\"summarize_conversation\", END)\n\n# Compile\ngraph = workflow.compile(checkpointer=memory)\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import StateGraph, START  # Define a new graph workflow = StateGraph(State) workflow.add_node(\"conversation\", call_model) workflow.add_node(summarize_conversation)  # Set the entrypoint as conversation workflow.add_edge(START, \"conversation\") workflow.add_conditional_edges(\"conversation\", should_continue) workflow.add_edge(\"summarize_conversation\", END)  # Compile graph = workflow.compile(checkpointer=memory) display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[70]: Copied! <pre># Create a thread\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Start conversation\ninput_message = HumanMessage(content=\"hi! I'm Lance\")\noutput = graph.invoke({\"messages\": [input_message]}, config) \nfor m in output['messages'][-1:]:\n    m.pretty_print()\n\ninput_message = HumanMessage(content=\"what's my name?\")\noutput = graph.invoke({\"messages\": [input_message]}, config) \nfor m in output['messages'][-1:]:\n    m.pretty_print()\n\ninput_message = HumanMessage(content=\"i like the 49ers!\")\noutput = graph.invoke({\"messages\": [input_message]}, config) \nfor m in output['messages'][-1:]:\n    m.pretty_print()\n</pre> # Create a thread config = {\"configurable\": {\"thread_id\": \"1\"}}  # Start conversation input_message = HumanMessage(content=\"hi! I'm Lance\") output = graph.invoke({\"messages\": [input_message]}, config)  for m in output['messages'][-1:]:     m.pretty_print()  input_message = HumanMessage(content=\"what's my name?\") output = graph.invoke({\"messages\": [input_message]}, config)  for m in output['messages'][-1:]:     m.pretty_print()  input_message = HumanMessage(content=\"i like the 49ers!\") output = graph.invoke({\"messages\": [input_message]}, config)  for m in output['messages'][-1:]:     m.pretty_print() <pre>================================== Ai Message ==================================\n\nHi Lance! It's great to hear from you again. If there's anything specific you'd like to discuss or any questions you have, feel free to let me know!\n================================== Ai Message ==================================\n\nYour name is Lance! If there's anything else you'd like to talk about or any questions you have, just let me know.\n================================== Ai Message ==================================\n\nThat's awesome! The San Francisco 49ers have a rich history and a passionate fan base. Is there anything specific about the 49ers you'd like to discuss, like their current season, players, or memorable games?\n</pre> In\u00a0[71]: Copied! <pre>config = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph_state = graph.get_state(config)\ngraph_state\n</pre> config = {\"configurable\": {\"thread_id\": \"1\"}} graph_state = graph.get_state(config) graph_state Out[71]: <pre>StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", additional_kwargs={}, response_metadata={}, id='db243d93-60c4-4da7-a3c0-fd759a32047e'), AIMessage(content=\"Hi Lance! It's great to hear from you again. If there's anything specific you'd like to discuss or any questions you have, feel free to let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 337, 'total_tokens': 369, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAo2M1zjripfSTJRkE2fjIq9PX6Vj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c495c6c8-82ea-4a6d-9017-f03ddff00969-0', usage_metadata={'input_tokens': 337, 'output_tokens': 32, 'total_tokens': 369, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='04518f7c-4068-4465-a3ba-0ab368eb9710'), AIMessage(content=\"Your name is Lance! If there's anything else you'd like to talk about or any questions you have, just let me know.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 203, 'total_tokens': 228, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CAo2Q8Q1O6r0kWl8lmSPPnqux7koR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e8f9c650-3f6e-4a98-8cfa-76a6502ff45d-0', usage_metadata={'input_tokens': 203, 'output_tokens': 25, 'total_tokens': 228, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='i like the 49ers!', additional_kwargs={}, response_metadata={}, id='b47b1594-ce54-406e-9241-d9ac485b464a'), AIMessage(content=\"That's awesome! The San Francisco 49ers have a rich history and a passionate fan base. Is there anything specific about the 49ers you'd like to discuss, like their current season, players, or memorable games?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 243, 'total_tokens': 287, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CAo2RJJNb9aeyUr4FvHQc5Vpx0Cy5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac23a103-0c9a-4274-8170-c66317d44786-0', usage_metadata={'input_tokens': 243, 'output_tokens': 44, 'total_tokens': 287, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'summary': \"Lance introduced himself multiple times throughout the conversation, consistently expressing his fondness for the San Francisco 49ers football team. The AI assistant acknowledged Lance's name each time and demonstrated a willingness to engage in a discussion about the 49ers, offering to explore various aspects of the team, such as their history, current roster, memorable games, and more. Despite the AI's attempts to delve deeper into the topic, the conversation remained brief and somewhat repetitive, with Lance reintroducing himself again without directly engaging with the AI's questions or prompts about the 49ers. The interaction was friendly, with the AI maintaining a welcoming tone and readiness to discuss any topic Lance might be interested in.\"}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d69-5a1c-669e-801b-da6372b56c1e'}}, metadata={'source': 'loop', 'step': 27, 'parents': {}}, created_at='2025-09-01T01:54:24.001591+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d69-4e0b-6aa8-801a-3d4ea27a3b4a'}}, tasks=(), interrupts=())</pre> In\u00a0[72]: Copied! <pre># Create a thread\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph_state = graph.get_state(config)\ngraph_state\n</pre> # Create a thread config = {\"configurable\": {\"thread_id\": \"1\"}} graph_state = graph.get_state(config) graph_state Out[72]: <pre>StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Lance\", additional_kwargs={}, response_metadata={}, id='db243d93-60c4-4da7-a3c0-fd759a32047e'), AIMessage(content=\"Hi Lance! It's great to hear from you again. If there's anything specific you'd like to discuss or any questions you have, feel free to let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 337, 'total_tokens': 369, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAo2M1zjripfSTJRkE2fjIq9PX6Vj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c495c6c8-82ea-4a6d-9017-f03ddff00969-0', usage_metadata={'input_tokens': 337, 'output_tokens': 32, 'total_tokens': 369, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='04518f7c-4068-4465-a3ba-0ab368eb9710'), AIMessage(content=\"Your name is Lance! If there's anything else you'd like to talk about or any questions you have, just let me know.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 203, 'total_tokens': 228, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CAo2Q8Q1O6r0kWl8lmSPPnqux7koR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e8f9c650-3f6e-4a98-8cfa-76a6502ff45d-0', usage_metadata={'input_tokens': 203, 'output_tokens': 25, 'total_tokens': 228, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='i like the 49ers!', additional_kwargs={}, response_metadata={}, id='b47b1594-ce54-406e-9241-d9ac485b464a'), AIMessage(content=\"That's awesome! The San Francisco 49ers have a rich history and a passionate fan base. Is there anything specific about the 49ers you'd like to discuss, like their current season, players, or memorable games?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 243, 'total_tokens': 287, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f33640a400', 'id': 'chatcmpl-CAo2RJJNb9aeyUr4FvHQc5Vpx0Cy5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac23a103-0c9a-4274-8170-c66317d44786-0', usage_metadata={'input_tokens': 243, 'output_tokens': 44, 'total_tokens': 287, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'summary': \"Lance introduced himself multiple times throughout the conversation, consistently expressing his fondness for the San Francisco 49ers football team. The AI assistant acknowledged Lance's name each time and demonstrated a willingness to engage in a discussion about the 49ers, offering to explore various aspects of the team, such as their history, current roster, memorable games, and more. Despite the AI's attempts to delve deeper into the topic, the conversation remained brief and somewhat repetitive, with Lance reintroducing himself again without directly engaging with the AI's questions or prompts about the 49ers. The interaction was friendly, with the AI maintaining a welcoming tone and readiness to discuss any topic Lance might be interested in.\"}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d69-5a1c-669e-801b-da6372b56c1e'}}, metadata={'source': 'loop', 'step': 27, 'parents': {}}, created_at='2025-09-01T01:54:24.001591+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d69-4e0b-6aa8-801a-3d4ea27a3b4a'}}, tasks=(), interrupts=())</pre> In\u00a0[73]: Copied! <pre>from IPython.display import Image, display\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\nfrom langchain_core.runnables import RunnableConfig\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph import MessagesState\n\n# LLM\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n\n# State \nclass State(MessagesState):\n    summary: str\n\n# Define the logic to call the model\ndef call_model(state: State, config: RunnableConfig):\n    \n    # Get summary if it exists\n    summary = state.get(\"summary\", \"\")\n\n    # If there is summary, then we add it\n    if summary:\n        \n        # Add summary to system message\n        system_message = f\"Summary of conversation earlier: {summary}\"\n\n        # Append summary to any newer messages\n        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n    \n    else:\n        messages = state[\"messages\"]\n    \n    response = model.invoke(messages, config)\n    return {\"messages\": response}\n\ndef summarize_conversation(state: State):\n    \n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt \n    if summary:\n        \n        # A summary already exists\n        summary_message = (\n            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n        \n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n    \n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n\n# Determine whether to end or summarize the conversation\ndef should_continue(state: State):\n    \n    \"\"\"Return the next node to execute.\"\"\"\n    \n    messages = state[\"messages\"]\n    \n    # If there are more than six messages, then we summarize the conversation\n    if len(messages) &gt; 6:\n        return \"summarize_conversation\"\n    \n    # Otherwise we can just end\n    return END\n\n# Define a new graph\nworkflow = StateGraph(State)\nworkflow.add_node(\"conversation\", call_model)\nworkflow.add_node(summarize_conversation)\n\n# Set the entrypoint as conversation\nworkflow.add_edge(START, \"conversation\")\nworkflow.add_conditional_edges(\"conversation\", should_continue)\nworkflow.add_edge(\"summarize_conversation\", END)\n\n# Compile\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display  from langchain_openai import ChatOpenAI from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage from langchain_core.runnables import RunnableConfig  from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.graph import MessagesState  # LLM model = ChatOpenAI(model=\"gpt-4o\", temperature=0)   # State  class State(MessagesState):     summary: str  # Define the logic to call the model def call_model(state: State, config: RunnableConfig):          # Get summary if it exists     summary = state.get(\"summary\", \"\")      # If there is summary, then we add it     if summary:                  # Add summary to system message         system_message = f\"Summary of conversation earlier: {summary}\"          # Append summary to any newer messages         messages = [SystemMessage(content=system_message)] + state[\"messages\"]          else:         messages = state[\"messages\"]          response = model.invoke(messages, config)     return {\"messages\": response}  def summarize_conversation(state: State):          # First, we get any existing summary     summary = state.get(\"summary\", \"\")      # Create our summarization prompt      if summary:                  # A summary already exists         summary_message = (             f\"This is summary of the conversation to date: {summary}\\n\\n\"             \"Extend the summary by taking into account the new messages above:\"         )              else:         summary_message = \"Create a summary of the conversation above:\"      # Add prompt to our history     messages = state[\"messages\"] + [HumanMessage(content=summary_message)]     response = model.invoke(messages)          # Delete all but the 2 most recent messages     delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]     return {\"summary\": response.content, \"messages\": delete_messages}  # Determine whether to end or summarize the conversation def should_continue(state: State):          \"\"\"Return the next node to execute.\"\"\"          messages = state[\"messages\"]          # If there are more than six messages, then we summarize the conversation     if len(messages) &gt; 6:         return \"summarize_conversation\"          # Otherwise we can just end     return END  # Define a new graph workflow = StateGraph(State) workflow.add_node(\"conversation\", call_model) workflow.add_node(summarize_conversation)  # Set the entrypoint as conversation workflow.add_edge(START, \"conversation\") workflow.add_conditional_edges(\"conversation\", should_continue) workflow.add_edge(\"summarize_conversation\", END)  # Compile memory = MemorySaver() graph = workflow.compile(checkpointer=memory) display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[74]: Copied! <pre># Create a thread\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Start conversation\nfor chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n    print(chunk)\n</pre> # Create a thread config = {\"configurable\": {\"thread_id\": \"1\"}}  # Start conversation for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):     print(chunk) <pre>{'conversation': {'messages': AIMessage(content='Hello Lance! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ff25b2783a', 'id': 'chatcmpl-CAoDisfUx7ZQc1VkCpC7UcvYX4y4g', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9be2e403-bc9b-4cf2-a080-1e515ccdf2af-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n</pre> In\u00a0[75]: Copied! <pre># Start conversation\nfor chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n    chunk['conversation'][\"messages\"].pretty_print()\n</pre> # Start conversation for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):     chunk['conversation'][\"messages\"].pretty_print() <pre>================================== Ai Message ==================================\n\nHi Lance! How's it going? What can I do for you today?\n</pre> In\u00a0[76]: Copied! <pre># Start conversation, again\nconfig = {\"configurable\": {\"thread_id\": \"2\"}}\n\n# Start conversation\ninput_message = HumanMessage(content=\"hi! I'm Lance\")\nfor event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    for m in event['messages']:\n        m.pretty_print()\n    print(\"---\"*25)\n</pre> # Start conversation, again config = {\"configurable\": {\"thread_id\": \"2\"}}  # Start conversation input_message = HumanMessage(content=\"hi! I'm Lance\") for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):     for m in event['messages']:         m.pretty_print()     print(\"---\"*25) <pre>================================ Human Message =================================\n\nhi! I'm Lance\n---------------------------------------------------------------------------\n================================ Human Message =================================\n\nhi! I'm Lance\n================================== Ai Message ==================================\n\nHello, Lance! How can I assist you today?\n---------------------------------------------------------------------------\n</pre> In\u00a0[77]: Copied! <pre>config = {\"configurable\": {\"thread_id\": \"3\"}}\ninput_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\nasync for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")\n</pre> config = {\"configurable\": {\"thread_id\": \"3\"}} input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\") async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):     print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\") <pre>Node: . Type: on_chain_start. Name: LangGraph\nNode: conversation. Type: on_chain_start. Name: conversation\nNode: conversation. Type: on_chat_model_start. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\nNode: conversation. Type: on_chat_model_end. Name: ChatOpenAI\nNode: conversation. Type: on_chain_start. Name: should_continue\nNode: conversation. Type: on_chain_end. Name: should_continue\nNode: conversation. Type: on_chain_stream. Name: conversation\nNode: conversation. Type: on_chain_end. Name: conversation\nNode: . Type: on_chain_stream. Name: LangGraph\nNode: . Type: on_chain_end. Name: LangGraph\n</pre> In\u00a0[78]: Copied! <pre>node_to_stream = 'conversation'\nconfig = {\"configurable\": {\"thread_id\": \"4\"}}\ninput_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\nasync for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n    # Get chat model tokens from a particular node \n    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n        print(event[\"data\"])\n</pre> node_to_stream = 'conversation' config = {\"configurable\": {\"thread_id\": \"4\"}} input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\") async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):     # Get chat model tokens from a particular node      if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:         print(event[\"data\"]) <pre>{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' professional', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' American', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' based', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Area', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' compete', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' League', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='NFL', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' league', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='N', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' was', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' founded', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='6', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' charter', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' All', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='-Amer', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ica', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='AA', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' joined', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' when', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' leagues', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' merged', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' rich', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' one', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' successful', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' teams', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' five', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' championships', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' victories', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' XVI', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' XIX', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='III', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='IV', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='IX', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' also', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' numerous', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' titles', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' made', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' playoff', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' appearances', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' iconic', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' coaches', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Hall', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Fam', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' like', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Joe', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Jerry', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Rice', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Steve', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Young', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Ronnie', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' L', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ott', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' coach', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Bill', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Walsh', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' who', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' credited', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' popular', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='izing', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Coast', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' offense', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' colors', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' red', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' gold', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' play', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' games', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Levi', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Stadium', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Santa', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Clara', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' California', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' which', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' moved', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='4', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' after', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' previously', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' playing', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Cand', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='lestick', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Park', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' passionate', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' fan', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' base', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' stor', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ied', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' rivalry', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' teams', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' like', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Dallas', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Cowboys', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Green', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Packers', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Seattle', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Seahawks', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' particularly', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' under', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' leadership', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Bill', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Walsh', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' George', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' Se', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='if', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='ert', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' helped', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' establish', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' them', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' one', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' premier', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' franchises', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'service_tier': 'default'}, id='run--db6b9ca6-ca5e-4a50-b9bf-32498a54114b')}\n</pre> In\u00a0[79]: Copied! <pre>config = {\"configurable\": {\"thread_id\": \"5\"}}\ninput_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\nasync for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n    # Get chat model tokens from a particular node \n    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n        data = event[\"data\"]\n        print(data[\"chunk\"].content, end=\"|\")\n</pre> config = {\"configurable\": {\"thread_id\": \"5\"}} input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\") async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):     # Get chat model tokens from a particular node      if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:         data = event[\"data\"]         print(data[\"chunk\"].content, end=\"|\") <pre>|The| San| Francisco| |49|ers| are| a| professional| American| football| team| based| in| the| San| Francisco| Bay| Area|.| They| compete| in| the| National| Football| League| (|NFL|)| as| a| member| of| the| league|'s| National| Football| Conference| (|N|FC|)| West| division|.| The| team| was| founded| in| |194|6| as| a| charter| member| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.\n\n|###| Key| Points|:\n\n|-| **|St|adium|**|:| The| |49|ers| play| their| home| games| at| Levi|'s| Stadium| in| Santa| Clara|,| California|,| which| they| moved| to| in| |201|4|.| Before| that|,| they| played| at| Cand|lestick| Park| in| San| Francisco|.\n\n|-| **|Team| Colors|**|:| The| team's| colors| are| red|,| gold|,| and| white|.\n\n|-| **|Masc|ot|**|:| The| |49|ers|'| mascot| is| S|ourd|ough| Sam|.\n\n|-| **|Champ|ionship|s|**|:| The| |49|ers| have| won| five| Super| Bowl| titles| (|X|VI|,| XIX|,| XX|III|,| XX|IV|,| and| XX|IX|),| with| their| most| successful| period| being| the| |198|0|s| and| early| |199|0|s|.| They| have| also| won| numerous| division| titles| and| conference| championships|.\n\n|-| **|Not|able| Players|**|:| The| team| has| had| several| Hall| of| Fame| players|,| including| Joe| Montana|,| Jerry| Rice|,| Steve| Young|,| Ronnie| L|ott|,| and| Charles| Haley|,| among| others|.\n\n|-| **|Co|aching| and| Management|**|:| The| team| has| been| led| by| several| notable| coaches|,| including| Bill| Walsh|,| who| is| credited| with| popular|izing| the| West| Coast| offense|.| As| of| the| |202|3| season|,| the| head| coach| is| Kyle| Shan|ahan|.\n\n|-| **|R|ival|ries|**|:| The| |49|ers| have| notable| rival|ries| with| the| Seattle| Seahawks|,| Los| Angeles| Rams|,| and| historically| with| the| Dallas| Cowboys| and| Green| Bay| Packers|.\n\n|-| **|Ownership|**|:| The| team| is| owned| by| the| York| family|,| with| Jed| York| serving| as| the| CEO|.\n\n|The| |49|ers| are| known| for| their| rich| history| and| have| a| strong| fan| base|.| They| are| one| of| the| most| successful| teams| in| NFL| history|,| both| in| terms| of| championships| and| overall| win|-loss| record|.||</pre> In\u00a0[80]: Copied! <pre>from langgraph_sdk import get_client\n\n# This is the URL of the local development server\nURL = \"http://127.0.0.1:2024\"\nclient = get_client(url=URL)\n\n# Search all hosted graphs\nassistants = await client.assistants.search()\n</pre> from langgraph_sdk import get_client  # This is the URL of the local development server URL = \"http://127.0.0.1:2024\" client = get_client(url=URL)  # Search all hosted graphs assistants = await client.assistants.search() In\u00a0[81]: Copied! <pre># Create a new thread\nthread = await client.threads.create()\n# Input message\ninput_message = HumanMessage(content=\"Multiply 2 and 3\")\nasync for event in client.runs.stream(thread[\"thread_id\"], \n                                      assistant_id=\"agent\", \n                                      input={\"messages\": [input_message]}, \n                                      stream_mode=\"values\"):\n    print(event)\n</pre> # Create a new thread thread = await client.threads.create() # Input message input_message = HumanMessage(content=\"Multiply 2 and 3\") async for event in client.runs.stream(thread[\"thread_id\"],                                        assistant_id=\"agent\",                                        input={\"messages\": [input_message]},                                        stream_mode=\"values\"):     print(event) <pre>StreamPart(event='metadata', data={'run_id': '01990308-2e14-70e6-a7e9-e56a61548376', 'attempt': 1})\nStreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '77737bc9-7c62-4a5a-970e-2284fbbe0a14', 'example': False}]})\nStreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '77737bc9-7c62-4a5a-970e-2284fbbe0a14', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoFq2a5Dyuqb59AYKGVQsQ7VjpSH', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run--4ae097ae-492a-47e2-ba73-910a5b725275-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\nStreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '77737bc9-7c62-4a5a-970e-2284fbbe0a14', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoFq2a5Dyuqb59AYKGVQsQ7VjpSH', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run--4ae097ae-492a-47e2-ba73-910a5b725275-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '0a575135-7594-418b-87b1-74ab306571ea', 'tool_call_id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'artifact': None, 'status': 'success'}]})\nStreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '77737bc9-7c62-4a5a-970e-2284fbbe0a14', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoFq2a5Dyuqb59AYKGVQsQ7VjpSH', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run--4ae097ae-492a-47e2-ba73-910a5b725275-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '0a575135-7594-418b-87b1-74ab306571ea', 'tool_call_id': 'call_djkQi1mUAvibtMaEUrRmHcWh', 'artifact': None, 'status': 'success'}, {'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 14, 'prompt_tokens': 159, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoFqbDSM9Yda9WsG7oNSGaNDCRh0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run--790334a2-5ec1-43a0-8a3f-d7a5a8a2c928-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 159, 'output_tokens': 14, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\n</pre> In\u00a0[82]: Copied! <pre>from langchain_core.messages import convert_to_messages\nthread = await client.threads.create()\ninput_message = HumanMessage(content=\"Multiply 2 and 3\")\nasync for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n    messages = event.data.get('messages',None)\n    if messages:\n        print(convert_to_messages(messages)[-1])\n    print('='*25)\n</pre> from langchain_core.messages import convert_to_messages thread = await client.threads.create() input_message = HumanMessage(content=\"Multiply 2 and 3\") async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):     messages = event.data.get('messages',None)     if messages:         print(convert_to_messages(messages)[-1])     print('='*25) <pre>=========================\ncontent='Multiply 2 and 3' additional_kwargs={} response_metadata={} id='9d28b745-5d71-49f0-8731-cf07f41ae60d'\n=========================\ncontent='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'tool_calls': [{'id': 'call_SFAs4GOFjbHxjQrF3JShCKgW', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoGbVGJlwNvLrwdIh78NBi0qehnK', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--fb146cfe-30a1-4ee8-9c52-7220c346ac4e-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_SFAs4GOFjbHxjQrF3JShCKgW', 'type': 'tool_call'}]\n=========================\ncontent='6' name='multiply' id='541135b7-a59f-4438-8932-012f52adaffa' tool_call_id='call_SFAs4GOFjbHxjQrF3JShCKgW'\n=========================\ncontent='The result of multiplying 2 and 3 is 6.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 159, 'output_tokens': 14, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 159, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoGcFK0kEW4WjGhFqpK6X5AwRMv2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--cdb83463-c2c6-4a25-b2e6-19f37c0474ff-0'\n=========================\n</pre> In\u00a0[83]: Copied! <pre>thread = await client.threads.create()\ninput_message = HumanMessage(content=\"Multiply 2 and 3\")\nasync for event in client.runs.stream(thread[\"thread_id\"], \n                                      assistant_id=\"agent\", \n                                      input={\"messages\": [input_message]}, \n                                      stream_mode=\"messages\"):\n    print(event.event)\n</pre> thread = await client.threads.create() input_message = HumanMessage(content=\"Multiply 2 and 3\") async for event in client.runs.stream(thread[\"thread_id\"],                                        assistant_id=\"agent\",                                        input={\"messages\": [input_message]},                                        stream_mode=\"messages\"):     print(event.event) <pre>metadata\nmessages/metadata\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/metadata\nmessages/complete\nmessages/metadata\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\nmessages/partial\n</pre> In\u00a0[84]: Copied! <pre>thread = await client.threads.create()\ninput_message = HumanMessage(content=\"Multiply 2 and 3\")\n\ndef format_tool_calls(tool_calls):\n    \"\"\"\n    Format a list of tool calls into a readable string.\n\n    Args:\n        tool_calls (list): A list of dictionaries, each representing a tool call.\n            Each dictionary should have 'id', 'name', and 'args' keys.\n\n    Returns:\n        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n\n    \"\"\"\n\n    if tool_calls:\n        formatted_calls = []\n        for call in tool_calls:\n            formatted_calls.append(\n                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n            )\n        return \"\\n\".join(formatted_calls)\n    return \"No tool calls\"\n\nasync for event in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id=\"agent\",\n    input={\"messages\": [input_message]},\n    stream_mode=\"messages\",):\n    \n    # Handle metadata events\n    if event.event == \"metadata\":\n        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n        print(\"-\" * 50)\n    \n    # Handle partial message events\n    elif event.event == \"messages/partial\":\n        for data_item in event.data:\n            # Process user messages\n            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n                print(f\"Human: {data_item['content']}\")\n            else:\n                # Extract relevant data from the event\n                tool_calls = data_item.get(\"tool_calls\", [])\n                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n                content = data_item.get(\"content\", \"\")\n                response_metadata = data_item.get(\"response_metadata\", {})\n\n                if content:\n                    print(f\"AI: {content}\")\n\n                if tool_calls:\n                    print(\"Tool Calls:\")\n                    print(format_tool_calls(tool_calls))\n\n                if invalid_tool_calls:\n                    print(\"Invalid Tool Calls:\")\n                    print(format_tool_calls(invalid_tool_calls))\n\n                if response_metadata:\n                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n                    \n        print(\"-\" * 50)\n</pre> thread = await client.threads.create() input_message = HumanMessage(content=\"Multiply 2 and 3\")  def format_tool_calls(tool_calls):     \"\"\"     Format a list of tool calls into a readable string.      Args:         tool_calls (list): A list of dictionaries, each representing a tool call.             Each dictionary should have 'id', 'name', and 'args' keys.      Returns:         str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.      \"\"\"      if tool_calls:         formatted_calls = []         for call in tool_calls:             formatted_calls.append(                 f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"             )         return \"\\n\".join(formatted_calls)     return \"No tool calls\"  async for event in client.runs.stream(     thread[\"thread_id\"],     assistant_id=\"agent\",     input={\"messages\": [input_message]},     stream_mode=\"messages\",):          # Handle metadata events     if event.event == \"metadata\":         print(f\"Metadata: Run ID - {event.data['run_id']}\")         print(\"-\" * 50)          # Handle partial message events     elif event.event == \"messages/partial\":         for data_item in event.data:             # Process user messages             if \"role\" in data_item and data_item[\"role\"] == \"user\":                 print(f\"Human: {data_item['content']}\")             else:                 # Extract relevant data from the event                 tool_calls = data_item.get(\"tool_calls\", [])                 invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])                 content = data_item.get(\"content\", \"\")                 response_metadata = data_item.get(\"response_metadata\", {})                  if content:                     print(f\"AI: {content}\")                  if tool_calls:                     print(\"Tool Calls:\")                     print(format_tool_calls(tool_calls))                  if invalid_tool_calls:                     print(\"Invalid Tool Calls:\")                     print(format_tool_calls(invalid_tool_calls))                  if response_metadata:                     finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")                     print(f\"Response Metadata: Finish Reason - {finish_reason}\")                              print(\"-\" * 50) <pre>Metadata: Run ID - 01990309-3974-7742-bb59-a211a86c68a3\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {'a': 2}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {'a': 2}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {'a': 2}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {'a': 2}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {'a': 2, 'b': 3}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {'a': 2, 'b': 3}\n--------------------------------------------------\nTool Calls:\nTool Call ID: call_yHRk4cNezx8uufLZMwW4nTMa, Function: multiply, Arguments: {'a': 2, 'b': 3}\nResponse Metadata: Finish Reason - tool_calls\n--------------------------------------------------\n--------------------------------------------------\nAI: The\n--------------------------------------------------\nAI: The result\n--------------------------------------------------\nAI: The result of\n--------------------------------------------------\nAI: The result of multiplying\n--------------------------------------------------\nAI: The result of multiplying \n--------------------------------------------------\nAI: The result of multiplying 2\n--------------------------------------------------\nAI: The result of multiplying 2 and\n--------------------------------------------------\nAI: The result of multiplying 2 and \n--------------------------------------------------\nAI: The result of multiplying 2 and 3\n--------------------------------------------------\nAI: The result of multiplying 2 and 3 is\n--------------------------------------------------\nAI: The result of multiplying 2 and 3 is \n--------------------------------------------------\nAI: The result of multiplying 2 and 3 is 6\n--------------------------------------------------\nAI: The result of multiplying 2 and 3 is 6.\n--------------------------------------------------\nAI: The result of multiplying 2 and 3 is 6.\nResponse Metadata: Finish Reason - stop\n--------------------------------------------------\n</pre> In\u00a0[85]: Copied! <pre>from langchain_openai import ChatOpenAI\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\n# This will be a tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Adds a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a + b\n\ndef divide(a: int, b: int) -&gt; float:\n    \"\"\"Divide a by b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a / b\n\ntools = [add, multiply, divide]\nllm = ChatOpenAI(model=\"gpt-4o\")\nllm_with_tools = llm.bind_tools(tools)\n</pre> from langchain_openai import ChatOpenAI  def multiply(a: int, b: int) -&gt; int:     \"\"\"Multiply a and b.      Args:         a: first int         b: second int     \"\"\"     return a * b  # This will be a tool def add(a: int, b: int) -&gt; int:     \"\"\"Adds a and b.      Args:         a: first int         b: second int     \"\"\"     return a + b  def divide(a: int, b: int) -&gt; float:     \"\"\"Divide a by b.      Args:         a: first int         b: second int     \"\"\"     return a / b  tools = [add, multiply, divide] llm = ChatOpenAI(model=\"gpt-4o\") llm_with_tools = llm.bind_tools(tools) In\u00a0[86]: Copied! <pre>from IPython.display import Image, display\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import MessagesState\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.prebuilt import tools_condition, ToolNode\n\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\n# System message\nsys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n\n# Node\ndef assistant(state: MessagesState):\n   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n\n# Graph\nbuilder = StateGraph(MessagesState)\n\n# Define nodes: these do the work\nbuilder.add_node(\"assistant\", assistant)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\n# Define edges: these determine the control flow\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_conditional_edges(\n    \"assistant\",\n    # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools\n    # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END\n    tools_condition,\n)\nbuilder.add_edge(\"tools\", \"assistant\")\n\nmemory = MemorySaver()\ngraph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)\n\n# Show\ndisplay(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n</pre> from IPython.display import Image, display  from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import MessagesState from langgraph.graph import START, StateGraph from langgraph.prebuilt import tools_condition, ToolNode  from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  # System message sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")  # Node def assistant(state: MessagesState):    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}  # Graph builder = StateGraph(MessagesState)  # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools))  # Define edges: these determine the control flow builder.add_edge(START, \"assistant\") builder.add_conditional_edges(     \"assistant\",     # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools     # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END     tools_condition, ) builder.add_edge(\"tools\", \"assistant\")  memory = MemorySaver() graph = builder.compile(interrupt_before=[\"tools\"], checkpointer=memory)  # Show display(Image(graph.get_graph(xray=True).draw_mermaid_png())) In\u00a0[87]: Copied! <pre># Input\ninitial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> # Input initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}  # Thread thread = {\"configurable\": {\"thread_id\": \"1\"}}  # Run the graph until the first interruption for event in graph.stream(initial_input, thread, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================ Human Message =================================\n\nMultiply 2 and 3\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_sb33zlSvrgBQhnLkXtrB5iKk)\n Call ID: call_sb33zlSvrgBQhnLkXtrB5iKk\n  Args:\n    a: 2\n    b: 3\n</pre> In\u00a0[88]: Copied! <pre>state = graph.get_state(thread)\nstate.next\n</pre> state = graph.get_state(thread) state.next Out[88]: <pre>('tools',)</pre> In\u00a0[89]: Copied! <pre>for event in graph.stream(None, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> for event in graph.stream(None, thread, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================== Ai Message ==================================\nTool Calls:\n  multiply (call_sb33zlSvrgBQhnLkXtrB5iKk)\n Call ID: call_sb33zlSvrgBQhnLkXtrB5iKk\n  Args:\n    a: 2\n    b: 3\n================================= Tool Message =================================\nName: multiply\n\n6\n================================== Ai Message ==================================\n\nThe result of multiplying 2 and 3 is 6.\n</pre> In\u00a0[90]: Copied! <pre># Input\ninitial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"2\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n\n# Get user feedback\nuser_approval = input(\"Do you want to call the tool? (yes/no): \")\n\n# Check approval\nif user_approval.lower() == \"yes\":\n    \n    # If approved, continue the graph execution\n    for event in graph.stream(None, thread, stream_mode=\"values\"):\n        event['messages'][-1].pretty_print()\n        \nelse:\n    print(\"Operation cancelled by user.\")\n</pre> # Input initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}  # Thread thread = {\"configurable\": {\"thread_id\": \"2\"}}  # Run the graph until the first interruption for event in graph.stream(initial_input, thread, stream_mode=\"values\"):     event['messages'][-1].pretty_print()  # Get user feedback user_approval = input(\"Do you want to call the tool? (yes/no): \")  # Check approval if user_approval.lower() == \"yes\":          # If approved, continue the graph execution     for event in graph.stream(None, thread, stream_mode=\"values\"):         event['messages'][-1].pretty_print()          else:     print(\"Operation cancelled by user.\") <pre>================================ Human Message =================================\n\nMultiply 2 and 3\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_8K0G6vbdNikuAiMF1NNDqib0)\n Call ID: call_8K0G6vbdNikuAiMF1NNDqib0\n  Args:\n    a: 2\n    b: 3\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_8K0G6vbdNikuAiMF1NNDqib0)\n Call ID: call_8K0G6vbdNikuAiMF1NNDqib0\n  Args:\n    a: 2\n    b: 3\n================================= Tool Message =================================\nName: multiply\n\n6\n================================== Ai Message ==================================\n\nThe result of multiplying 2 and 3 is 6.\n</pre> In\u00a0[91]: Copied! <pre>from langchain_openai import ChatOpenAI\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\n# This will be a tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Adds a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a + b\n\ndef divide(a: int, b: int) -&gt; float:\n    \"\"\"Divide a by b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a / b\n\ntools = [add, multiply, divide]\nllm = ChatOpenAI(model=\"gpt-4o\")\nllm_with_tools = llm.bind_tools(tools)\n</pre> from langchain_openai import ChatOpenAI  def multiply(a: int, b: int) -&gt; int:     \"\"\"Multiply a and b.      Args:         a: first int         b: second int     \"\"\"     return a * b  # This will be a tool def add(a: int, b: int) -&gt; int:     \"\"\"Adds a and b.      Args:         a: first int         b: second int     \"\"\"     return a + b  def divide(a: int, b: int) -&gt; float:     \"\"\"Divide a by b.      Args:         a: first int         b: second int     \"\"\"     return a / b  tools = [add, multiply, divide] llm = ChatOpenAI(model=\"gpt-4o\") llm_with_tools = llm.bind_tools(tools)  In\u00a0[92]: Copied! <pre>from IPython.display import Image, display\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import MessagesState\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.prebuilt import tools_condition, ToolNode\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n# System message\nsys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n\n# Node\ndef assistant(state: MessagesState):\n   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n\n# Graph\nbuilder = StateGraph(MessagesState)\n\n# Define nodes: these do the work\nbuilder.add_node(\"assistant\", assistant)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\n# Define edges: these determine the control flow\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_conditional_edges(\n    \"assistant\",\n    # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools\n    # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END\n    tools_condition,\n)\nbuilder.add_edge(\"tools\", \"assistant\")\n\nmemory = MemorySaver()\ngraph = builder.compile(interrupt_before=[\"assistant\"], checkpointer=memory)\n\n# Show\ndisplay(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n</pre> from IPython.display import Image, display  from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import MessagesState from langgraph.graph import START, StateGraph from langgraph.prebuilt import tools_condition, ToolNode  from langchain_core.messages import HumanMessage, SystemMessage  # System message sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")  # Node def assistant(state: MessagesState):    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}  # Graph builder = StateGraph(MessagesState)  # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools))  # Define edges: these determine the control flow builder.add_edge(START, \"assistant\") builder.add_conditional_edges(     \"assistant\",     # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools     # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END     tools_condition, ) builder.add_edge(\"tools\", \"assistant\")  memory = MemorySaver() graph = builder.compile(interrupt_before=[\"assistant\"], checkpointer=memory)  # Show display(Image(graph.get_graph(xray=True).draw_mermaid_png())) In\u00a0[93]: Copied! <pre># Input\ninitial_input = {\"messages\": \"Multiply 2 and 3\"}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> # Input initial_input = {\"messages\": \"Multiply 2 and 3\"}  # Thread thread = {\"configurable\": {\"thread_id\": \"1\"}}  # Run the graph until the first interruption for event in graph.stream(initial_input, thread, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================ Human Message =================================\n\nMultiply 2 and 3\n</pre> In\u00a0[94]: Copied! <pre>state = graph.get_state(thread)\nstate\n</pre> state = graph.get_state(thread) state Out[94]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='ac527f42-9f48-47cf-b1d6-5137bc7dfc0a')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d92-8218-678c-8000-a5f67af11eab'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-09-01T02:12:48.779642+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d92-8212-6292-bfff-9ebe48b1689f'}}, tasks=(PregelTask(id='086143ff-e69c-faff-0a1c-94a73f43a7fd', name='assistant', path=('__pregel_pull', 'assistant'), error=None, interrupts=(), state=None, result=None),), interrupts=())</pre> In\u00a0[95]: Copied! <pre>graph.update_state(\n    thread,\n    {\"messages\": [HumanMessage(content=\"No, actually multiply 3 and 3!\")]},\n)\n</pre> graph.update_state(     thread,     {\"messages\": [HumanMessage(content=\"No, actually multiply 3 and 3!\")]}, ) Out[95]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1f086d92-fe19-616a-8001-05344dc6f81c'}}</pre> In\u00a0[96]: Copied! <pre>new_state = graph.get_state(thread).values\nfor m in new_state['messages']:\n    m.pretty_print()\n</pre> new_state = graph.get_state(thread).values for m in new_state['messages']:     m.pretty_print() <pre>================================ Human Message =================================\n\nMultiply 2 and 3\n================================ Human Message =================================\n\nNo, actually multiply 3 and 3!\n</pre> In\u00a0[97]: Copied! <pre>for event in graph.stream(None, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> for event in graph.stream(None, thread, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================ Human Message =================================\n\nNo, actually multiply 3 and 3!\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_H0QsVwu0smlsp68z10SPzecZ)\n Call ID: call_H0QsVwu0smlsp68z10SPzecZ\n  Args:\n    a: 3\n    b: 3\n================================= Tool Message =================================\nName: multiply\n\n9\n</pre> In\u00a0[98]: Copied! <pre>for event in graph.stream(None, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> for event in graph.stream(None, thread, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================= Tool Message =================================\nName: multiply\n\n9\n================================== Ai Message ==================================\n\nThe product of 3 and 3 is 9.\n</pre> In\u00a0[99]: Copied! <pre>from IPython.display import Image, display\n\nfrom typing_extensions import TypedDict\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.errors import NodeInterrupt\nfrom langgraph.graph import START, END, StateGraph\n\nclass State(TypedDict):\n    input: str\n\ndef step_1(state: State) -&gt; State:\n    print(\"---Step 1---\")\n    return state\n\ndef step_2(state: State) -&gt; State:\n    # Let's optionally raise a NodeInterrupt if the length of the input is longer than 5 characters\n    if len(state['input']) &gt; 5:\n        raise NodeInterrupt(f\"Received input that is longer than 5 characters: {state['input']}\")\n    \n    print(\"---Step 2---\")\n    return state\n\ndef step_3(state: State) -&gt; State:\n    print(\"---Step 3---\")\n    return state\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"step_1\", step_1)\nbuilder.add_node(\"step_2\", step_2)\nbuilder.add_node(\"step_3\", step_3)\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"step_2\")\nbuilder.add_edge(\"step_2\", \"step_3\")\nbuilder.add_edge(\"step_3\", END)\n\n# Set up memory\nmemory = MemorySaver()\n\n# Compile the graph with memory\ngraph = builder.compile(checkpointer=memory)\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display  from typing_extensions import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.errors import NodeInterrupt from langgraph.graph import START, END, StateGraph  class State(TypedDict):     input: str  def step_1(state: State) -&gt; State:     print(\"---Step 1---\")     return state  def step_2(state: State) -&gt; State:     # Let's optionally raise a NodeInterrupt if the length of the input is longer than 5 characters     if len(state['input']) &gt; 5:         raise NodeInterrupt(f\"Received input that is longer than 5 characters: {state['input']}\")          print(\"---Step 2---\")     return state  def step_3(state: State) -&gt; State:     print(\"---Step 3---\")     return state  builder = StateGraph(State) builder.add_node(\"step_1\", step_1) builder.add_node(\"step_2\", step_2) builder.add_node(\"step_3\", step_3) builder.add_edge(START, \"step_1\") builder.add_edge(\"step_1\", \"step_2\") builder.add_edge(\"step_2\", \"step_3\") builder.add_edge(\"step_3\", END)  # Set up memory memory = MemorySaver()  # Compile the graph with memory graph = builder.compile(checkpointer=memory)  # View display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[100]: Copied! <pre>initial_input = {\"input\": \"hello world\"}\nthread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream(initial_input, thread_config, stream_mode=\"values\"):\n    print(event)\n</pre> initial_input = {\"input\": \"hello world\"} thread_config = {\"configurable\": {\"thread_id\": \"1\"}}  # Run the graph until the first interruption for event in graph.stream(initial_input, thread_config, stream_mode=\"values\"):     print(event) <pre>{'input': 'hello world'}\n---Step 1---\n{'input': 'hello world'}\n</pre> <pre>/var/folders/xm/hv0x78kx5mvbzd2tsn4q98km0000gn/T/ipykernel_96760/4174644561.py:18: LangGraphDeprecatedSinceV10: NodeInterrupt is deprecated. Please use `langgraph.types.interrupt` instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n  raise NodeInterrupt(f\"Received input that is longer than 5 characters: {state['input']}\")\n</pre> In\u00a0[101]: Copied! <pre>state = graph.get_state(thread_config)\nprint(state.next)\n</pre> state = graph.get_state(thread_config) print(state.next) <pre>('step_2',)\n</pre> In\u00a0[102]: Copied! <pre>print(state.tasks)\n</pre> print(state.tasks) <pre>(PregelTask(id='12d21560-565f-34c5-78c2-c03ff3302d7d', name='step_2', path=('__pregel_pull', 'step_2'), error=None, interrupts=(Interrupt(value='Received input that is longer than 5 characters: hello world', id='placeholder-id'),), state=None, result=None),)\n</pre> In\u00a0[103]: Copied! <pre>for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n    print(event)\n</pre> for event in graph.stream(None, thread_config, stream_mode=\"values\"):     print(event) <pre>{'input': 'hello world'}\n</pre> <pre>/var/folders/xm/hv0x78kx5mvbzd2tsn4q98km0000gn/T/ipykernel_96760/4174644561.py:18: LangGraphDeprecatedSinceV10: NodeInterrupt is deprecated. Please use `langgraph.types.interrupt` instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n  raise NodeInterrupt(f\"Received input that is longer than 5 characters: {state['input']}\")\n</pre> In\u00a0[104]: Copied! <pre>state = graph.get_state(thread_config)\nprint(state.next)\n</pre> state = graph.get_state(thread_config) print(state.next) <pre>('step_2',)\n</pre> In\u00a0[105]: Copied! <pre>graph.update_state(\n    thread_config,\n    {\"input\": \"hi\"},\n)\n</pre> graph.update_state(     thread_config,     {\"input\": \"hi\"}, ) Out[105]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1f086d97-98a4-6faa-8002-b88fe18c2867'}}</pre> In\u00a0[106]: Copied! <pre>for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n    print(event)\n</pre> for event in graph.stream(None, thread_config, stream_mode=\"values\"):     print(event) <pre>{'input': 'hi'}\n---Step 2---\n{'input': 'hi'}\n---Step 3---\n{'input': 'hi'}\n</pre> In\u00a0[107]: Copied! <pre>from langchain_openai import ChatOpenAI\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\n# This will be a tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Adds a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a + b\n\ndef divide(a: int, b: int) -&gt; float:\n    \"\"\"Divide a by b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a / b\n\ntools = [add, multiply, divide]\nllm = ChatOpenAI(model=\"gpt-4o\")\nllm_with_tools = llm.bind_tools(tools)\n</pre> from langchain_openai import ChatOpenAI  def multiply(a: int, b: int) -&gt; int:     \"\"\"Multiply a and b.      Args:         a: first int         b: second int     \"\"\"     return a * b  # This will be a tool def add(a: int, b: int) -&gt; int:     \"\"\"Adds a and b.      Args:         a: first int         b: second int     \"\"\"     return a + b  def divide(a: int, b: int) -&gt; float:     \"\"\"Divide a by b.      Args:         a: first int         b: second int     \"\"\"     return a / b  tools = [add, multiply, divide] llm = ChatOpenAI(model=\"gpt-4o\") llm_with_tools = llm.bind_tools(tools) In\u00a0[108]: Copied! <pre>from IPython.display import Image, display\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import MessagesState\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.prebuilt import tools_condition, ToolNode\n\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n\n# System message\nsys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n\n# Node\ndef assistant(state: MessagesState):\n   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n\n# Graph\nbuilder = StateGraph(MessagesState)\n\n# Define nodes: these do the work\nbuilder.add_node(\"assistant\", assistant)\nbuilder.add_node(\"tools\", ToolNode(tools))\n\n# Define edges: these determine the control flow\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_conditional_edges(\n    \"assistant\",\n    # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools\n    # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END\n    tools_condition,\n)\nbuilder.add_edge(\"tools\", \"assistant\")\n\nmemory = MemorySaver()\ngraph = builder.compile(checkpointer=MemorySaver())\n\n# Show\ndisplay(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n</pre> from IPython.display import Image, display  from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import MessagesState from langgraph.graph import START, END, StateGraph from langgraph.prebuilt import tools_condition, ToolNode  from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  # System message sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")  # Node def assistant(state: MessagesState):    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}  # Graph builder = StateGraph(MessagesState)  # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools))  # Define edges: these determine the control flow builder.add_edge(START, \"assistant\") builder.add_conditional_edges(     \"assistant\",     # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools     # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END     tools_condition, ) builder.add_edge(\"tools\", \"assistant\")  memory = MemorySaver() graph = builder.compile(checkpointer=MemorySaver())  # Show display(Image(graph.get_graph(xray=True).draw_mermaid_png())) In\u00a0[109]: Copied! <pre># Input\ninitial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> # Input initial_input = {\"messages\": HumanMessage(content=\"Multiply 2 and 3\")}  # Thread thread = {\"configurable\": {\"thread_id\": \"1\"}}  # Run the graph until the first interruption for event in graph.stream(initial_input, thread, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================ Human Message =================================\n\nMultiply 2 and 3\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_LCEWDmXyzieRkHLo1wWJG5do)\n Call ID: call_LCEWDmXyzieRkHLo1wWJG5do\n  Args:\n    a: 2\n    b: 3\n================================= Tool Message =================================\nName: multiply\n\n6\n================================== Ai Message ==================================\n\nThe result of multiplying 2 and 3 is 6.\n</pre> In\u00a0[110]: Copied! <pre>graph.get_state({'configurable': {'thread_id': '1'}})\n</pre> graph.get_state({'configurable': {'thread_id': '1'}}) Out[110]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_LCEWDmXyzieRkHLo1wWJG5do', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 131, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoNVOK3Vl2GJt2wY1xJfSnna4lk9', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--285879fd-e3ac-467d-93aa-9e22c3976577-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_LCEWDmXyzieRkHLo1wWJG5do', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 17, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='6', name='multiply', id='b9822158-3abb-4bd4-8492-8542ecafc1ce', tool_call_id='call_LCEWDmXyzieRkHLo1wWJG5do'), AIMessage(content='The result of multiplying 2 and 3 is 6.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 156, 'total_tokens': 170, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoNWLl68Qqni9tjDPZbSHP5ihioP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8598ecb4-4178-4fa3-96ed-ff812f2d50a2-0', usage_metadata={'input_tokens': 156, 'output_tokens': 14, 'total_tokens': 170, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d9a-0b9e-68d8-8003-28dbbe239dad'}}, metadata={'source': 'loop', 'step': 3, 'parents': {}}, created_at='2025-09-01T02:16:11.104863+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d9a-013d-6d44-8002-559fb054eb73'}}, tasks=(), interrupts=())</pre> In\u00a0[111]: Copied! <pre>all_states = [s for s in graph.get_state_history(thread)]\n</pre> all_states = [s for s in graph.get_state_history(thread)] In\u00a0[112]: Copied! <pre>len(all_states)\n</pre> len(all_states) Out[112]: <pre>5</pre> In\u00a0[113]: Copied! <pre>all_states[-2]\n</pre> all_states[-2] Out[113]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d99-f4e9-65a2-8000-e8e8d42e860a'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-09-01T02:16:08.723799+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d99-f4e7-6252-bfff-fe49bd4331bf'}}, tasks=(PregelTask(id='63a0947a-cdc4-d163-3a58-c1820d7eacd1', name='assistant', path=('__pregel_pull', 'assistant'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_LCEWDmXyzieRkHLo1wWJG5do', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 131, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoNVOK3Vl2GJt2wY1xJfSnna4lk9', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--285879fd-e3ac-467d-93aa-9e22c3976577-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_LCEWDmXyzieRkHLo1wWJG5do', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 17, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}),), interrupts=())</pre> In\u00a0[114]: Copied! <pre>to_replay = all_states[-2]\n</pre> to_replay = all_states[-2] In\u00a0[115]: Copied! <pre>to_replay\n</pre> to_replay Out[115]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d99-f4e9-65a2-8000-e8e8d42e860a'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-09-01T02:16:08.723799+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d99-f4e7-6252-bfff-fe49bd4331bf'}}, tasks=(PregelTask(id='63a0947a-cdc4-d163-3a58-c1820d7eacd1', name='assistant', path=('__pregel_pull', 'assistant'), error=None, interrupts=(), state=None, result={'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_LCEWDmXyzieRkHLo1wWJG5do', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 131, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoNVOK3Vl2GJt2wY1xJfSnna4lk9', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--285879fd-e3ac-467d-93aa-9e22c3976577-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_LCEWDmXyzieRkHLo1wWJG5do', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 17, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}),), interrupts=())</pre> In\u00a0[116]: Copied! <pre>to_replay.values\n</pre> to_replay.values Out[116]: <pre>{'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188')]}</pre> In\u00a0[117]: Copied! <pre>to_replay.next\n</pre> to_replay.next Out[117]: <pre>('assistant',)</pre> In\u00a0[118]: Copied! <pre>to_replay.config\n</pre> to_replay.config Out[118]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1f086d99-f4e9-65a2-8000-e8e8d42e860a'}}</pre> In\u00a0[119]: Copied! <pre>for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================ Human Message =================================\n\nMultiply 2 and 3\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_MS47bZhuwqExt5dd1gDO8EeR)\n Call ID: call_MS47bZhuwqExt5dd1gDO8EeR\n  Args:\n    a: 2\n    b: 3\n================================= Tool Message =================================\nName: multiply\n\n6\n================================== Ai Message ==================================\n\nThe result of multiplying 2 and 3 is 6.\n</pre> In\u00a0[120]: Copied! <pre>to_fork = all_states[-2]\nto_fork.values[\"messages\"]\n</pre> to_fork = all_states[-2] to_fork.values[\"messages\"] Out[120]: <pre>[HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188')]</pre> In\u00a0[121]: Copied! <pre>to_fork.config\n</pre> to_fork.config Out[121]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1f086d99-f4e9-65a2-8000-e8e8d42e860a'}}</pre> In\u00a0[122]: Copied! <pre>fork_config = graph.update_state(\n    to_fork.config,\n    {\"messages\": [HumanMessage(content='Multiply 5 and 3', \n                               id=to_fork.values[\"messages\"][0].id)]},\n)\n</pre> fork_config = graph.update_state(     to_fork.config,     {\"messages\": [HumanMessage(content='Multiply 5 and 3',                                 id=to_fork.values[\"messages\"][0].id)]}, ) In\u00a0[123]: Copied! <pre>fork_config\n</pre> fork_config Out[123]: <pre>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1f086dac-c23e-69d2-8001-481e415514a8'}}</pre> In\u00a0[124]: Copied! <pre>all_states = [state for state in graph.get_state_history(thread) ]\nall_states[0].values[\"messages\"]\n</pre> all_states = [state for state in graph.get_state_history(thread) ] all_states[0].values[\"messages\"] Out[124]: <pre>[HumanMessage(content='Multiply 5 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188')]</pre> In\u00a0[125]: Copied! <pre>graph.get_state({'configurable': {'thread_id': '1'}})\n</pre> graph.get_state({'configurable': {'thread_id': '1'}}) Out[125]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Multiply 5 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086dac-c23e-69d2-8001-481e415514a8'}}, metadata={'source': 'update', 'step': 1, 'parents': {}}, created_at='2025-09-01T02:24:33.438350+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086d99-f4e9-65a2-8000-e8e8d42e860a'}}, tasks=(PregelTask(id='8d488882-0fdf-dca2-36c1-eb939434a26f', name='assistant', path=('__pregel_pull', 'assistant'), error=None, interrupts=(), state=None, result=None),), interrupts=())</pre> In\u00a0[126]: Copied! <pre>for event in graph.stream(None, fork_config, stream_mode=\"values\"):\n    event['messages'][-1].pretty_print()\n</pre> for event in graph.stream(None, fork_config, stream_mode=\"values\"):     event['messages'][-1].pretty_print() <pre>================================ Human Message =================================\n\nMultiply 5 and 3\n================================== Ai Message ==================================\nTool Calls:\n  multiply (call_p9l2fI7hi0mjnWWR7M1w8yn4)\n Call ID: call_p9l2fI7hi0mjnWWR7M1w8yn4\n  Args:\n    a: 5\n    b: 3\n================================= Tool Message =================================\nName: multiply\n\n15\n================================== Ai Message ==================================\n\nThe result of multiplying 5 and 3 is 15.\n</pre> In\u00a0[127]: Copied! <pre>graph.get_state({'configurable': {'thread_id': '1'}})\n</pre> graph.get_state({'configurable': {'thread_id': '1'}}) Out[127]: <pre>StateSnapshot(values={'messages': [HumanMessage(content='Multiply 5 and 3', additional_kwargs={}, response_metadata={}, id='dee305dd-e442-4352-ad89-7a1748566188'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_p9l2fI7hi0mjnWWR7M1w8yn4', 'function': {'arguments': '{\"a\":5,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 131, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoW6vYrHmMFT1XxsDFfhKhkXKrSx', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ea528862-5624-46d5-a3a1-6c671591d93d-0', tool_calls=[{'name': 'multiply', 'args': {'a': 5, 'b': 3}, 'id': 'call_p9l2fI7hi0mjnWWR7M1w8yn4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 17, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='15', name='multiply', id='17b8fa08-3f09-42cd-a84c-92be1f50ff9c', tool_call_id='call_p9l2fI7hi0mjnWWR7M1w8yn4'), AIMessage(content='The result of multiplying 5 and 3 is 15.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 156, 'total_tokens': 170, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_46bff0e0c8', 'id': 'chatcmpl-CAoW72AwnorvbYKYzODko6yrjiLHC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d3c0c782-929d-4f7c-b314-6465e0b2494b-0', usage_metadata={'input_tokens': 156, 'output_tokens': 14, 'total_tokens': 170, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086dad-e902-62f2-8004-2490fb0dbe2e'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-09-01T02:25:04.346579+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f086dad-e06c-603e-8003-47e3ff690d44'}}, tasks=(), interrupts=())</pre> In\u00a0[128]: Copied! <pre>from IPython.display import Image, display\n\nfrom typing import Any\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    state: str\n\nclass ReturnNodeValue:\n    def __init__(self, node_secret: str):\n        self._value = node_secret\n\n    def __call__(self, state: State) -&gt; Any:\n        print(f\"Adding {self._value} to {state['state']}\")\n        return {\"state\": [self._value]}\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret \nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"b\", \"c\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display  from typing import Any from typing_extensions import TypedDict  from langgraph.graph import StateGraph, START, END  class State(TypedDict):     # The operator.add reducer fn makes this append-only     state: str  class ReturnNodeValue:     def __init__(self, node_secret: str):         self._value = node_secret      def __call__(self, state: State) -&gt; Any:         print(f\"Adding {self._value} to {state['state']}\")         return {\"state\": [self._value]}  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret  builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"b\", \"c\") builder.add_edge(\"c\", \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[129]: Copied! <pre>graph.invoke({\"state\": []})\n</pre> graph.invoke({\"state\": []}) <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm C to [\"I'm B\"]\nAdding I'm D to [\"I'm C\"]\n</pre> Out[129]: <pre>{'state': [\"I'm D\"]}</pre> In\u00a0[130]: Copied! <pre>builder = StateGraph(State)\n\n# Initialize each node with node_secret \nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> builder = StateGraph(State)  # Initialize each node with node_secret  builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"d\") builder.add_edge(\"c\", \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[131]: Copied! <pre>from langgraph.errors import InvalidUpdateError\ntry:\n    graph.invoke({\"state\": []})\nexcept InvalidUpdateError as e:\n    print(f\"An error occurred: {e}\")\n</pre> from langgraph.errors import InvalidUpdateError try:     graph.invoke({\"state\": []}) except InvalidUpdateError as e:     print(f\"An error occurred: {e}\") <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm C to [\"I'm A\"]\nAn error occurred: At key 'state': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE\n</pre> In\u00a0[132]: Copied! <pre>import operator\nfrom typing import Annotated\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    state: Annotated[list, operator.add]\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret \nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> import operator from typing import Annotated  class State(TypedDict):     # The operator.add reducer fn makes this append-only     state: Annotated[list, operator.add]  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret  builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"d\") builder.add_edge(\"c\", \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[133]: Copied! <pre>builder = StateGraph(State)\n\n# Initialize each node with node_secret \nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b2\")\nbuilder.add_edge([\"b2\", \"c\"], \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> builder = StateGraph(State)  # Initialize each node with node_secret  builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"b2\") builder.add_edge([\"b2\", \"c\"], \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[134]: Copied! <pre>graph.invoke({\"state\": []})\n</pre> graph.invoke({\"state\": []}) <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm C to [\"I'm A\"]\nAdding I'm B2 to [\"I'm A\", \"I'm B\", \"I'm C\"]\nAdding I'm D to [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm B2\"]\n</pre> Out[134]: <pre>{'state': [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm B2\", \"I'm D\"]}</pre> In\u00a0[135]: Copied! <pre>def sorting_reducer(left, right):\n    \"\"\" Combines and sorts the values in a list\"\"\"\n    if not isinstance(left, list):\n        left = [left]\n\n    if not isinstance(right, list):\n        right = [right]\n    \n    return sorted(left + right, reverse=False)\n\nclass State(TypedDict):\n    # sorting_reducer will sort the values in state\n    state: Annotated[list, sorting_reducer]\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret \nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n\n# Flow\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b2\")\nbuilder.add_edge([\"b2\", \"c\"], \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> def sorting_reducer(left, right):     \"\"\" Combines and sorts the values in a list\"\"\"     if not isinstance(left, list):         left = [left]      if not isinstance(right, list):         right = [right]          return sorted(left + right, reverse=False)  class State(TypedDict):     # sorting_reducer will sort the values in state     state: Annotated[list, sorting_reducer]  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret  builder.add_node(\"a\", ReturnNodeValue(\"I'm A\")) builder.add_node(\"b\", ReturnNodeValue(\"I'm B\")) builder.add_node(\"b2\", ReturnNodeValue(\"I'm B2\")) builder.add_node(\"c\", ReturnNodeValue(\"I'm C\")) builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))  # Flow builder.add_edge(START, \"a\") builder.add_edge(\"a\", \"b\") builder.add_edge(\"a\", \"c\") builder.add_edge(\"b\", \"b2\") builder.add_edge([\"b2\", \"c\"], \"d\") builder.add_edge(\"d\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[136]: Copied! <pre>graph.invoke({\"state\": []})\n</pre> graph.invoke({\"state\": []}) <pre>Adding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm C to [\"I'm A\"]\nAdding I'm B2 to [\"I'm A\", \"I'm B\", \"I'm C\"]\nAdding I'm D to [\"I'm A\", \"I'm B\", \"I'm B2\", \"I'm C\"]\n</pre> Out[136]: <pre>{'state': [\"I'm A\", \"I'm B\", \"I'm B2\", \"I'm C\", \"I'm D\"]}</pre> In\u00a0[137]: Copied! <pre>from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n</pre> from langchain_openai import ChatOpenAI llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)  In\u00a0[138]: Copied! <pre>class State(TypedDict):\n    question: str\n    answer: str\n    context: Annotated[list, operator.add]\n</pre> class State(TypedDict):     question: str     answer: str     context: Annotated[list, operator.add] In\u00a0[139]: Copied! <pre>import os, getpass\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n_set_env(\"TAVILY_API_KEY\")\n</pre> import os, getpass def _set_env(var: str):     if not os.environ.get(var):         os.environ[var] = getpass.getpass(f\"{var}: \") _set_env(\"TAVILY_API_KEY\") In\u00a0[143]: Copied! <pre>from langchain_core.messages import HumanMessage, SystemMessage\n\nfrom langchain_community.document_loaders import WikipediaLoader\nfrom langchain_community.tools import TavilySearchResults\n\ndef search_web(state):\n    \n    \"\"\" Retrieve docs from web search \"\"\"\n\n    # Search\n    tavily_search = TavilySearchResults(max_results=3)\n    search_docs = tavily_search.invoke(state['question'])\n\n     # Format\n    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n        [\n            f'&lt;Document href=\"{doc[\"url\"]}\"&gt;\\n{doc[\"content\"]}\\n&lt;/Document&gt;'\n            for doc in search_docs\n        ]\n    )\n\n    return {\"context\": [formatted_search_docs]} \n\ndef search_wikipedia(state):\n    \n    \"\"\" Retrieve docs from wikipedia \"\"\"\n\n    # Search\n    search_docs = WikipediaLoader(query=state['question'], \n                                  load_max_docs=2).load()\n\n     # Format\n    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n        [\n            f'&lt;Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"&gt;\\n{doc.page_content}\\n&lt;/Document&gt;'\n            for doc in search_docs\n        ]\n    )\n\n    return {\"context\": [formatted_search_docs]} \n\ndef generate_answer(state):\n    \n    \"\"\" Node to answer a question \"\"\"\n\n    # Get state\n    context = state[\"context\"]\n    question = state[\"question\"]\n\n    # Template\n    answer_template = \"\"\"Answer the question {question} using this context: {context}\"\"\"\n    answer_instructions = answer_template.format(question=question, \n                                                       context=context)    \n    \n    # Answer\n    answer = llm.invoke([SystemMessage(content=answer_instructions)]+[HumanMessage(content=f\"Answer the question.\")])\n      \n    # Append it to state\n    return {\"answer\": answer}\n\n# Add nodes\nbuilder = StateGraph(State)\n\n# Initialize each node with node_secret \nbuilder.add_node(\"search_web\",search_web)\nbuilder.add_node(\"search_wikipedia\", search_wikipedia)\nbuilder.add_node(\"generate_answer\", generate_answer)\n\n# Flow\nbuilder.add_edge(START, \"search_wikipedia\")\nbuilder.add_edge(START, \"search_web\")\nbuilder.add_edge(\"search_wikipedia\", \"generate_answer\")\nbuilder.add_edge(\"search_web\", \"generate_answer\")\nbuilder.add_edge(\"generate_answer\", END)\ngraph = builder.compile()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from langchain_core.messages import HumanMessage, SystemMessage  from langchain_community.document_loaders import WikipediaLoader from langchain_community.tools import TavilySearchResults  def search_web(state):          \"\"\" Retrieve docs from web search \"\"\"      # Search     tavily_search = TavilySearchResults(max_results=3)     search_docs = tavily_search.invoke(state['question'])       # Format     formatted_search_docs = \"\\n\\n---\\n\\n\".join(         [             f'\\n{doc[\"content\"]}\\n'             for doc in search_docs         ]     )      return {\"context\": [formatted_search_docs]}   def search_wikipedia(state):          \"\"\" Retrieve docs from wikipedia \"\"\"      # Search     search_docs = WikipediaLoader(query=state['question'],                                    load_max_docs=2).load()       # Format     formatted_search_docs = \"\\n\\n---\\n\\n\".join(         [             f'\\n{doc.page_content}\\n'             for doc in search_docs         ]     )      return {\"context\": [formatted_search_docs]}   def generate_answer(state):          \"\"\" Node to answer a question \"\"\"      # Get state     context = state[\"context\"]     question = state[\"question\"]      # Template     answer_template = \"\"\"Answer the question {question} using this context: {context}\"\"\"     answer_instructions = answer_template.format(question=question,                                                         context=context)              # Answer     answer = llm.invoke([SystemMessage(content=answer_instructions)]+[HumanMessage(content=f\"Answer the question.\")])            # Append it to state     return {\"answer\": answer}  # Add nodes builder = StateGraph(State)  # Initialize each node with node_secret  builder.add_node(\"search_web\",search_web) builder.add_node(\"search_wikipedia\", search_wikipedia) builder.add_node(\"generate_answer\", generate_answer)  # Flow builder.add_edge(START, \"search_wikipedia\") builder.add_edge(START, \"search_web\") builder.add_edge(\"search_wikipedia\", \"generate_answer\") builder.add_edge(\"search_web\", \"generate_answer\") builder.add_edge(\"generate_answer\", END) graph = builder.compile()  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[144]: Copied! <pre>result = graph.invoke({\"question\": \"How were Nvidia's Q2 2024 earnings\"})\nresult['answer'].content\n</pre> result = graph.invoke({\"question\": \"How were Nvidia's Q2 2024 earnings\"}) result['answer'].content Out[144]: <pre>\"Nvidia's Q2 2024 earnings were very strong. The company reported a record quarterly revenue of $30.0 billion, which was up 15% from the previous quarter and up 122% from a year ago. The data center revenue was also a record at $26.3 billion, up 16% from the previous quarter and up 154% from a year ago. Nvidia's earnings per share were adjusted to 68 cents, beating the estimates of 64 cents. Additionally, Nvidia returned $15.4 billion to shareholders in the form of share repurchases and cash dividends during the first half of fiscal 2025. The Board of Directors also approved an additional $50.0 billion in share repurchase authorization. Overall, Nvidia's Q2 2024 earnings were significantly above expectations and demonstrated strong growth.\"</pre> In\u00a0[154]: Copied! <pre>from operator import add\nfrom typing_extensions import TypedDict\nfrom typing import List, Optional, Annotated\n\n# The structure of the logs\nclass Log(TypedDict):\n    id: str\n    question: str\n    docs: Optional[List]\n    answer: str\n    grade: Optional[int]\n    grader: Optional[str]\n    feedback: Optional[str]\n</pre> from operator import add from typing_extensions import TypedDict from typing import List, Optional, Annotated  # The structure of the logs class Log(TypedDict):     id: str     question: str     docs: Optional[List]     answer: str     grade: Optional[int]     grader: Optional[str]     feedback: Optional[str] In\u00a0[155]: Copied! <pre>from IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\n# Failure Analysis Sub-graph\nclass FailureAnalysisState(TypedDict):\n    cleaned_logs: List[Log]\n    failures: List[Log]\n    fa_summary: str\n    processed_logs: List[str]\n\nclass FailureAnalysisOutputState(TypedDict):\n    fa_summary: str\n    processed_logs: List[str]\n\ndef get_failures(state):\n    \"\"\" Get logs that contain a failure \"\"\"\n    cleaned_logs = state[\"cleaned_logs\"]\n    failures = [log for log in cleaned_logs if \"grade\" in log]\n    return {\"failures\": failures}\n\ndef generate_summary(state):\n    \"\"\" Generate summary of failures \"\"\"\n    failures = state[\"failures\"]\n    # Add fxn: fa_summary = summarize(failures)\n    fa_summary = \"Poor quality retrieval of Chroma documentation.\"\n    return {\"fa_summary\": fa_summary, \"processed_logs\": [f\"failure-analysis-on-log-{failure['id']}\" for failure in failures]}\n\nfa_builder = StateGraph(state_schema=FailureAnalysisState,output_schema=FailureAnalysisOutputState)\nfa_builder.add_node(\"get_failures\", get_failures)\nfa_builder.add_node(\"generate_summary\", generate_summary)\nfa_builder.add_edge(START, \"get_failures\")\nfa_builder.add_edge(\"get_failures\", \"generate_summary\")\nfa_builder.add_edge(\"generate_summary\", END)\n\ngraph = fa_builder.compile()\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display from langgraph.graph import StateGraph, START, END  # Failure Analysis Sub-graph class FailureAnalysisState(TypedDict):     cleaned_logs: List[Log]     failures: List[Log]     fa_summary: str     processed_logs: List[str]  class FailureAnalysisOutputState(TypedDict):     fa_summary: str     processed_logs: List[str]  def get_failures(state):     \"\"\" Get logs that contain a failure \"\"\"     cleaned_logs = state[\"cleaned_logs\"]     failures = [log for log in cleaned_logs if \"grade\" in log]     return {\"failures\": failures}  def generate_summary(state):     \"\"\" Generate summary of failures \"\"\"     failures = state[\"failures\"]     # Add fxn: fa_summary = summarize(failures)     fa_summary = \"Poor quality retrieval of Chroma documentation.\"     return {\"fa_summary\": fa_summary, \"processed_logs\": [f\"failure-analysis-on-log-{failure['id']}\" for failure in failures]}  fa_builder = StateGraph(state_schema=FailureAnalysisState,output_schema=FailureAnalysisOutputState) fa_builder.add_node(\"get_failures\", get_failures) fa_builder.add_node(\"generate_summary\", generate_summary) fa_builder.add_edge(START, \"get_failures\") fa_builder.add_edge(\"get_failures\", \"generate_summary\") fa_builder.add_edge(\"generate_summary\", END)  graph = fa_builder.compile() display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[156]: Copied! <pre># Summarization subgraph\nclass QuestionSummarizationState(TypedDict):\n    cleaned_logs: List[Log]\n    qs_summary: str\n    report: str\n    processed_logs: List[str]\n\nclass QuestionSummarizationOutputState(TypedDict):\n    report: str\n    processed_logs: List[str]\n\ndef generate_summary(state):\n    cleaned_logs = state[\"cleaned_logs\"]\n    # Add fxn: summary = summarize(generate_summary)\n    summary = \"Questions focused on usage of ChatOllama and Chroma vector store.\"\n    return {\"qs_summary\": summary, \"processed_logs\": [f\"summary-on-log-{log['id']}\" for log in cleaned_logs]}\n\ndef send_to_slack(state):\n    qs_summary = state[\"qs_summary\"]\n    # Add fxn: report = report_generation(qs_summary)\n    report = \"foo bar baz\"\n    return {\"report\": report}\n\nqs_builder = StateGraph(QuestionSummarizationState,output_schema=QuestionSummarizationOutputState)\nqs_builder.add_node(\"generate_summary\", generate_summary)\nqs_builder.add_node(\"send_to_slack\", send_to_slack)\nqs_builder.add_edge(START, \"generate_summary\")\nqs_builder.add_edge(\"generate_summary\", \"send_to_slack\")\nqs_builder.add_edge(\"send_to_slack\", END)\n\ngraph = qs_builder.compile()\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> # Summarization subgraph class QuestionSummarizationState(TypedDict):     cleaned_logs: List[Log]     qs_summary: str     report: str     processed_logs: List[str]  class QuestionSummarizationOutputState(TypedDict):     report: str     processed_logs: List[str]  def generate_summary(state):     cleaned_logs = state[\"cleaned_logs\"]     # Add fxn: summary = summarize(generate_summary)     summary = \"Questions focused on usage of ChatOllama and Chroma vector store.\"     return {\"qs_summary\": summary, \"processed_logs\": [f\"summary-on-log-{log['id']}\" for log in cleaned_logs]}  def send_to_slack(state):     qs_summary = state[\"qs_summary\"]     # Add fxn: report = report_generation(qs_summary)     report = \"foo bar baz\"     return {\"report\": report}  qs_builder = StateGraph(QuestionSummarizationState,output_schema=QuestionSummarizationOutputState) qs_builder.add_node(\"generate_summary\", generate_summary) qs_builder.add_node(\"send_to_slack\", send_to_slack) qs_builder.add_edge(START, \"generate_summary\") qs_builder.add_edge(\"generate_summary\", \"send_to_slack\") qs_builder.add_edge(\"send_to_slack\", END)  graph = qs_builder.compile() display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[157]: Copied! <pre># Entry Graph\nclass EntryGraphState(TypedDict):\n    raw_logs: List[Log]\n    cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs\n    fa_summary: str # This will only be generated in the FA sub-graph\n    report: str # This will only be generated in the QS sub-graph\n    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs\n</pre> # Entry Graph class EntryGraphState(TypedDict):     raw_logs: List[Log]     cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs     fa_summary: str # This will only be generated in the FA sub-graph     report: str # This will only be generated in the QS sub-graph     processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs In\u00a0[158]: Copied! <pre># Entry Graph\nclass EntryGraphState(TypedDict):\n    raw_logs: List[Log]\n    cleaned_logs: List[Log]\n    fa_summary: str # This will only be generated in the FA sub-graph\n    report: str # This will only be generated in the QS sub-graph\n    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs\n\ndef clean_logs(state):\n    # Get logs\n    raw_logs = state[\"raw_logs\"]\n    # Data cleaning raw_logs -&gt; docs \n    cleaned_logs = raw_logs\n    return {\"cleaned_logs\": cleaned_logs}\n\nentry_builder = StateGraph(EntryGraphState)\nentry_builder.add_node(\"clean_logs\", clean_logs)\nentry_builder.add_node(\"question_summarization\", qs_builder.compile())\nentry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n\nentry_builder.add_edge(START, \"clean_logs\")\nentry_builder.add_edge(\"clean_logs\", \"failure_analysis\")\nentry_builder.add_edge(\"clean_logs\", \"question_summarization\")\nentry_builder.add_edge(\"failure_analysis\", END)\nentry_builder.add_edge(\"question_summarization\", END)\n\ngraph = entry_builder.compile()\n\nfrom IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n</pre> # Entry Graph class EntryGraphState(TypedDict):     raw_logs: List[Log]     cleaned_logs: List[Log]     fa_summary: str # This will only be generated in the FA sub-graph     report: str # This will only be generated in the QS sub-graph     processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs  def clean_logs(state):     # Get logs     raw_logs = state[\"raw_logs\"]     # Data cleaning raw_logs -&gt; docs      cleaned_logs = raw_logs     return {\"cleaned_logs\": cleaned_logs}  entry_builder = StateGraph(EntryGraphState) entry_builder.add_node(\"clean_logs\", clean_logs) entry_builder.add_node(\"question_summarization\", qs_builder.compile()) entry_builder.add_node(\"failure_analysis\", fa_builder.compile())  entry_builder.add_edge(START, \"clean_logs\") entry_builder.add_edge(\"clean_logs\", \"failure_analysis\") entry_builder.add_edge(\"clean_logs\", \"question_summarization\") entry_builder.add_edge(\"failure_analysis\", END) entry_builder.add_edge(\"question_summarization\", END)  graph = entry_builder.compile()  from IPython.display import Image, display  # Setting xray to 1 will show the internal structure of the nested graph display(Image(graph.get_graph(xray=1).draw_mermaid_png())) In\u00a0[159]: Copied! <pre># Dummy logs\nquestion_answer = Log(\n    id=\"1\",\n    question=\"How can I import ChatOllama?\",\n    answer=\"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\",\n)\n\nquestion_answer_feedback = Log(\n    id=\"2\",\n    question=\"How can I use Chroma vector store?\",\n    answer=\"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\",\n    grade=0,\n    grader=\"Document Relevance Recall\",\n    feedback=\"The retrieved documents discuss vector stores in general, but not Chroma specifically\",\n)\n\nraw_logs = [question_answer,question_answer_feedback]\ngraph.invoke({\"raw_logs\": raw_logs})\n</pre> # Dummy logs question_answer = Log(     id=\"1\",     question=\"How can I import ChatOllama?\",     answer=\"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\", )  question_answer_feedback = Log(     id=\"2\",     question=\"How can I use Chroma vector store?\",     answer=\"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\",     grade=0,     grader=\"Document Relevance Recall\",     feedback=\"The retrieved documents discuss vector stores in general, but not Chroma specifically\", )  raw_logs = [question_answer,question_answer_feedback] graph.invoke({\"raw_logs\": raw_logs}) Out[159]: <pre>{'raw_logs': [{'id': '1',\n   'question': 'How can I import ChatOllama?',\n   'answer': \"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\"},\n  {'id': '2',\n   'question': 'How can I use Chroma vector store?',\n   'answer': 'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).',\n   'grade': 0,\n   'grader': 'Document Relevance Recall',\n   'feedback': 'The retrieved documents discuss vector stores in general, but not Chroma specifically'}],\n 'cleaned_logs': [{'id': '1',\n   'question': 'How can I import ChatOllama?',\n   'answer': \"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\"},\n  {'id': '2',\n   'question': 'How can I use Chroma vector store?',\n   'answer': 'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).',\n   'grade': 0,\n   'grader': 'Document Relevance Recall',\n   'feedback': 'The retrieved documents discuss vector stores in general, but not Chroma specifically'}],\n 'fa_summary': 'Poor quality retrieval of Chroma documentation.',\n 'report': 'foo bar baz',\n 'processed_logs': ['failure-analysis-on-log-2',\n  'summary-on-log-1',\n  'summary-on-log-2']}</pre> In\u00a0[145]: Copied! <pre>from langchain_openai import ChatOpenAI\n\n# Prompts we will use\nsubjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\"\njoke_prompt = \"\"\"Generate a joke about {subject}\"\"\"\nbest_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"\n\n# LLM\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n</pre> from langchain_openai import ChatOpenAI  # Prompts we will use subjects_prompt = \"\"\"Generate a list of 3 sub-topics that are all related to this overall topic: {topic}.\"\"\" joke_prompt = \"\"\"Generate a joke about {subject}\"\"\" best_joke_prompt = \"\"\"Below are a bunch of jokes about {topic}. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n  {jokes}\"\"\"  # LLM model = ChatOpenAI(model=\"gpt-4o\", temperature=0)  In\u00a0[146]: Copied! <pre>import operator\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel\n\nclass Subjects(BaseModel):\n    subjects: list[str]\n\nclass BestJoke(BaseModel):\n    id: int\n    \nclass OverallState(TypedDict):\n    topic: str\n    subjects: list\n    jokes: Annotated[list, operator.add]\n    best_selected_joke: str\n</pre> import operator from typing import Annotated from typing_extensions import TypedDict from pydantic import BaseModel  class Subjects(BaseModel):     subjects: list[str]  class BestJoke(BaseModel):     id: int      class OverallState(TypedDict):     topic: str     subjects: list     jokes: Annotated[list, operator.add]     best_selected_joke: str In\u00a0[147]: Copied! <pre>def generate_topics(state: OverallState):\n    prompt = subjects_prompt.format(topic=state[\"topic\"])\n    response = model.with_structured_output(Subjects).invoke(prompt)\n    return {\"subjects\": response.subjects}\n</pre> def generate_topics(state: OverallState):     prompt = subjects_prompt.format(topic=state[\"topic\"])     response = model.with_structured_output(Subjects).invoke(prompt)     return {\"subjects\": response.subjects} In\u00a0[148]: Copied! <pre>from langgraph.types import Send\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n</pre> from langgraph.types import Send def continue_to_jokes(state: OverallState):     return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]] In\u00a0[149]: Copied! <pre>class JokeState(TypedDict):\n    subject: str\n\nclass Joke(BaseModel):\n    joke: str\n\ndef generate_joke(state: JokeState):\n    prompt = joke_prompt.format(subject=state[\"subject\"])\n    response = model.with_structured_output(Joke).invoke(prompt)\n    return {\"jokes\": [response.joke]}\n</pre> class JokeState(TypedDict):     subject: str  class Joke(BaseModel):     joke: str  def generate_joke(state: JokeState):     prompt = joke_prompt.format(subject=state[\"subject\"])     response = model.with_structured_output(Joke).invoke(prompt)     return {\"jokes\": [response.joke]} In\u00a0[151]: Copied! <pre>def best_joke(state: OverallState):\n    jokes = \"\\n\\n\".join(state[\"jokes\"])\n    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n    response = model.with_structured_output(BestJoke).invoke(prompt)\n    return {\"best_selected_joke\": state[\"jokes\"][response.id]}\n</pre> def best_joke(state: OverallState):     jokes = \"\\n\\n\".join(state[\"jokes\"])     prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)     response = model.with_structured_output(BestJoke).invoke(prompt)     return {\"best_selected_joke\": state[\"jokes\"][response.id]} In\u00a0[152]: Copied! <pre>from IPython.display import Image\nfrom langgraph.graph import END, StateGraph, START\n\n# Construct the graph: here we put everything together to construct our graph\ngraph = StateGraph(OverallState)\ngraph.add_node(\"generate_topics\", generate_topics)\ngraph.add_node(\"generate_joke\", generate_joke)\ngraph.add_node(\"best_joke\", best_joke)\ngraph.add_edge(START, \"generate_topics\")\ngraph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\ngraph.add_edge(\"generate_joke\", \"best_joke\")\ngraph.add_edge(\"best_joke\", END)\n\n# Compile the graph\napp = graph.compile()\nImage(app.get_graph().draw_mermaid_png())\n</pre> from IPython.display import Image from langgraph.graph import END, StateGraph, START  # Construct the graph: here we put everything together to construct our graph graph = StateGraph(OverallState) graph.add_node(\"generate_topics\", generate_topics) graph.add_node(\"generate_joke\", generate_joke) graph.add_node(\"best_joke\", best_joke) graph.add_edge(START, \"generate_topics\") graph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"]) graph.add_edge(\"generate_joke\", \"best_joke\") graph.add_edge(\"best_joke\", END)  # Compile the graph app = graph.compile() Image(app.get_graph().draw_mermaid_png()) Out[152]: In\u00a0[153]: Copied! <pre># Call the graph: here we call it to generate a list of jokes\nfor s in app.stream({\"topic\": \"animals\"}):\n    print(s)\n</pre> # Call the graph: here we call it to generate a list of jokes for s in app.stream({\"topic\": \"animals\"}):     print(s) <pre>{'generate_topics': {'subjects': ['Animal Behavior and Communication', 'Conservation and Endangered Species', 'Animal Habitats and Ecosystems']}}\n{'generate_joke': {'jokes': ['Why did the parrot bring a ladder to the conversation?\\n\\nBecause it wanted to take the dialogue to new heights!']}}\n{'generate_joke': {'jokes': ['Why did the squirrel bring a suitcase to the forest?\\n\\nBecause it heard the ecosystem was \"tree-mendous\" and wanted to branch out!']}}\n{'generate_joke': {'jokes': ['Why did the endangered species start a band?\\n\\nBecause they wanted to make some noise before they went extinct!']}}\n{'best_joke': {'best_selected_joke': 'Why did the parrot bring a ladder to the conversation?\\n\\nBecause it wanted to take the dialogue to new heights!'}}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/langgraph/#basics","title":"Basics\u00b6","text":""},{"location":"notebooks/langgraph/#simple-graph","title":"Simple-Graph\u00b6","text":""},{"location":"notebooks/langgraph/#state","title":"State\u00b6","text":""},{"location":"notebooks/langgraph/#node","title":"Node\u00b6","text":""},{"location":"notebooks/langgraph/#edge","title":"Edge\u00b6","text":""},{"location":"notebooks/langgraph/#graph-construction","title":"Graph Construction\u00b6","text":""},{"location":"notebooks/langgraph/#graph-invoke","title":"Graph Invoke\u00b6","text":""},{"location":"notebooks/langgraph/#chain","title":"Chain\u00b6","text":""},{"location":"notebooks/langgraph/#messages","title":"Messages\u00b6","text":""},{"location":"notebooks/langgraph/#tools","title":"Tools\u00b6","text":""},{"location":"notebooks/langgraph/#reducers","title":"Reducers\u00b6","text":""},{"location":"notebooks/langgraph/#router","title":"Router\u00b6","text":""},{"location":"notebooks/langgraph/#agent","title":"Agent\u00b6","text":""},{"location":"notebooks/langgraph/#agent-with-memory","title":"Agent with memory\u00b6","text":""},{"location":"notebooks/langgraph/#state-schema","title":"State Schema\u00b6","text":""},{"location":"notebooks/langgraph/#typedict","title":"TypeDict\u00b6","text":""},{"location":"notebooks/langgraph/#dataclass","title":"Dataclass\u00b6","text":""},{"location":"notebooks/langgraph/#pydantic","title":"PyDantic\u00b6","text":""},{"location":"notebooks/langgraph/#state-reducers","title":"State Reducers\u00b6","text":""},{"location":"notebooks/langgraph/#default-overwriting-state","title":"Default overwriting state\u00b6","text":""},{"location":"notebooks/langgraph/#branching","title":"Branching\u00b6","text":""},{"location":"notebooks/langgraph/#reducers","title":"Reducers\u00b6","text":""},{"location":"notebooks/langgraph/#custom-reducers","title":"Custom Reducers\u00b6","text":""},{"location":"notebooks/langgraph/#messages","title":"Messages\u00b6","text":""},{"location":"notebooks/langgraph/#re-writing","title":"Re-writing\u00b6","text":""},{"location":"notebooks/langgraph/#removal","title":"Removal\u00b6","text":""},{"location":"notebooks/langgraph/#multiple-schema","title":"Multiple Schema\u00b6","text":""},{"location":"notebooks/langgraph/#private-schema","title":"Private Schema\u00b6","text":""},{"location":"notebooks/langgraph/#inputoutput-schema","title":"Input/Output Schema\u00b6","text":""},{"location":"notebooks/langgraph/#trim-and-filter-messages","title":"Trim and Filter Messages\u00b6","text":""},{"location":"notebooks/langgraph/#reducers","title":"Reducers\u00b6","text":""},{"location":"notebooks/langgraph/#filter-messages","title":"Filter Messages\u00b6","text":""},{"location":"notebooks/langgraph/#trim-messages","title":"Trim Messages\u00b6","text":""},{"location":"notebooks/langgraph/#chat-w-summarizing-messages-and-memory","title":"Chat w/ Summarizing Messages and Memory\u00b6","text":""},{"location":"notebooks/langgraph/#chatbot-w-summarizing-messages-and-external-memory","title":"Chatbot w/ Summarizing Messages and External Memory\u00b6","text":""},{"location":"notebooks/langgraph/#persisting-state","title":"Persisting State\u00b6","text":""},{"location":"notebooks/langgraph/#streaming","title":"Streaming\u00b6","text":""},{"location":"notebooks/langgraph/#streaming-full-state","title":"Streaming Full State\u00b6","text":""},{"location":"notebooks/langgraph/#breakpoint","title":"Breakpoint\u00b6","text":""},{"location":"notebooks/langgraph/#breakpoint-for-human-approval","title":"Breakpoint for human approval\u00b6","text":""},{"location":"notebooks/langgraph/#editing-state-and-human-feedback","title":"Editing State and Human Feedback\u00b6","text":""},{"location":"notebooks/langgraph/#editing-state","title":"Editing State\u00b6","text":""},{"location":"notebooks/langgraph/#dynamic-feedback","title":"Dynamic Feedback\u00b6","text":""},{"location":"notebooks/langgraph/#time-travel","title":"Time Travel\u00b6","text":""},{"location":"notebooks/langgraph/#browsing-history","title":"Browsing History\u00b6","text":""},{"location":"notebooks/langgraph/#replaying","title":"Replaying\u00b6","text":""},{"location":"notebooks/langgraph/#forking","title":"Forking\u00b6","text":""},{"location":"notebooks/langgraph/#parallelization","title":"Parallelization\u00b6","text":""},{"location":"notebooks/langgraph/#setting-the-order-of-state-updates","title":"Setting the order of state updates\u00b6","text":""},{"location":"notebooks/langgraph/#working-with-llms","title":"Working with LLMs\u00b6","text":""},{"location":"notebooks/langgraph/#sub-graph","title":"Sub-graph\u00b6","text":""},{"location":"notebooks/langgraph/#map-reduce","title":"Map-Reduce\u00b6","text":""},{"location":"notebooks/langgraph/#state","title":"State\u00b6","text":""},{"location":"notebooks/langgraph/#map","title":"Map\u00b6","text":""},{"location":"notebooks/langgraph/#reduce","title":"Reduce\u00b6","text":""},{"location":"notebooks/langgraph/#research-assistant","title":"Research Assistant\u00b6","text":""},{"location":"notebooks/pydantic/","title":"Pydantic Basics","text":"In\u00a0[9]: Copied! <pre>from pydantic import BaseModel, Field, ValidationError, EmailStr\nfrom typing import Optional\nfrom datetime import date\nimport json\n</pre> from pydantic import BaseModel, Field, ValidationError, EmailStr from typing import Optional from datetime import date import json In\u00a0[10]: Copied! <pre>class UserInput(BaseModel):\n    name: str\n    email: EmailStr\n    query: str\n    order_id: Optional[int] = Field(default=None, description=\"Order ID\", ge=1, le=1000000)\n    purchase_date: Optional[date] = Field(default=None, description=\"Date of purchase transactions\")\n</pre> class UserInput(BaseModel):     name: str     email: EmailStr     query: str     order_id: Optional[int] = Field(default=None, description=\"Order ID\", ge=1, le=1000000)     purchase_date: Optional[date] = Field(default=None, description=\"Date of purchase transactions\")   In\u00a0[13]: Copied! <pre>def validate_user_input(input_data):\n    try:\n        # Attempt to create a UserInput model instance from user input data\n        user_input = UserInput(**input_data)\n        print(f\"\u2705 Valid user input created:\")\n        print(f\"{user_input.model_dump_json(indent=2)}\")\n        return user_input\n    except ValidationError as e:\n        # Capture and display validation errors in a readable format\n        print(f\"\u274c Validation error occurred:\")\n        for error in e.errors():\n            print(f\"  - {error['loc'][0]}: {error['msg']}\")\n        return None\n</pre> def validate_user_input(input_data):     try:         # Attempt to create a UserInput model instance from user input data         user_input = UserInput(**input_data)         print(f\"\u2705 Valid user input created:\")         print(f\"{user_input.model_dump_json(indent=2)}\")         return user_input     except ValidationError as e:         # Capture and display validation errors in a readable format         print(f\"\u274c Validation error occurred:\")         for error in e.errors():             print(f\"  - {error['loc'][0]}: {error['msg']}\")         return None In\u00a0[11]: Copied! <pre>user_input = UserInput(name=\"Rohit Kumar\",\n    email=\"rohitkumar@example.com\",\n    query=\"What is the capital of India?\")\n\nprint(user_input)\n</pre> user_input = UserInput(name=\"Rohit Kumar\",     email=\"rohitkumar@example.com\",     query=\"What is the capital of India?\")  print(user_input) <pre>name='Rohit Kumar' email='rohitkumar@example.com' query='What is the capital of India?' order_id=None purchase_date=None\n</pre> In\u00a0[12]: Copied! <pre># Puttting invalid email format\nuser_input = UserInput(name=\"Rohit Kumar\",\n    email=\"rohitkumar_example.com\",\n    query=\"What is the capital of India?\")\n\nprint(user_input)\n</pre> # Puttting invalid email format user_input = UserInput(name=\"Rohit Kumar\",     email=\"rohitkumar_example.com\",     query=\"What is the capital of India?\")  print(user_input) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[12], line 2\n      1 # Puttting invalid email format\n----&gt; 2 user_input = UserInput(name=\"Rohit Kumar\",\n      3     email=\"rohitkumar_example.com\",\n      4     query=\"What is the capital of India?\")\n      6 print(user_input)\n\nFile ~/Desktop/rokmr/notes/.venv/lib/python3.13/site-packages/pydantic/main.py:253, in BaseModel.__init__(self, **data)\n    251 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    252 __tracebackhide__ = True\n--&gt; 253 validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    254 if self is not validated_self:\n    255     warnings.warn(\n    256         'A custom validator is returning a value other than `self`.\\n'\n    257         \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n    258         'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n    259         stacklevel=2,\n    260     )\n\nValidationError: 1 validation error for UserInput\nemail\n  value is not a valid email address: An email address must have an @-sign. [type=value_error, input_value='rohitkumar_example.com', input_type=str]</pre> In\u00a0[14]: Copied! <pre>input_data = {\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": f\"\"\"I bought a laptop carrying case and it turned out to be \n             the wrong size. I need to return it.\"\"\",\n    \"order_id\": 12345,\n    \"purchase_date\": date(2025, 12, 31)\n}\n\n# Validate the user input data\nuser_input = validate_user_input(input_data)\n</pre> input_data = {     \"name\": \"Rohit Kumar\",     \"email\": \"rohitkumar@example.com\",     \"query\": f\"\"\"I bought a laptop carrying case and it turned out to be               the wrong size. I need to return it.\"\"\",     \"order_id\": 12345,     \"purchase_date\": date(2025, 12, 31) }  # Validate the user input data user_input = validate_user_input(input_data) <pre>\u2705 Valid user input created:\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I bought a laptop carrying case and it turned out to be \\n             the wrong size. I need to return it.\",\n  \"order_id\": 12345,\n  \"purchase_date\": \"2025-12-31\"\n}\n</pre> In\u00a0[16]: Copied! <pre># It will ignore the system_message as it is not a part of the UserInput model\ninput_data = {\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": f\"\"\"I bought a laptop carrying case and it turned out to be \n             the wrong size. I need to return it.\"\"\",\n    \"order_id\": 12345,\n    \"system_message\": \"logging status regarding order processing...\",\n    \"purchase_date\": date(2025, 12, 31)\n}\n\n# Validate the user input data\nuser_input = validate_user_input(input_data)\n</pre> # It will ignore the system_message as it is not a part of the UserInput model input_data = {     \"name\": \"Rohit Kumar\",     \"email\": \"rohitkumar@example.com\",     \"query\": f\"\"\"I bought a laptop carrying case and it turned out to be               the wrong size. I need to return it.\"\"\",     \"order_id\": 12345,     \"system_message\": \"logging status regarding order processing...\",     \"purchase_date\": date(2025, 12, 31) }  # Validate the user input data user_input = validate_user_input(input_data) <pre>\u2705 Valid user input created:\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I bought a laptop carrying case and it turned out to be \\n             the wrong size. I need to return it.\",\n  \"order_id\": 12345,\n  \"purchase_date\": \"2025-12-31\"\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre># Date as string which is automatic handled by pydantic\ninput_data = {\n    \"name\": \"Joe User\",\n    \"email\": \"joe.user@example.com\",\n    \"query\": f\"\"\"I bought a laptop carrying case and it turned out to be \n             the wrong size. I need to return it.\"\"\",\n    \"order_id\": 12345,\n    \"purchase_date\": \"2025-12-31\"\n}\n\nuser_input = validate_user_input(input_data)\n</pre> # Date as string which is automatic handled by pydantic input_data = {     \"name\": \"Joe User\",     \"email\": \"joe.user@example.com\",     \"query\": f\"\"\"I bought a laptop carrying case and it turned out to be               the wrong size. I need to return it.\"\"\",     \"order_id\": 12345,     \"purchase_date\": \"2025-12-31\" }  user_input = validate_user_input(input_data) <pre>\u2705 Valid user input created:\n{\n  \"name\": \"Joe User\",\n  \"email\": \"joe.user@example.com\",\n  \"query\": \"I bought a laptop carrying case and it turned out to be \\n             the wrong size. I need to return it.\",\n  \"order_id\": 12345,\n  \"purchase_date\": \"2025-12-31\"\n}\n</pre> In\u00a0[19]: Copied! <pre>print(user_input)\n</pre>  print(user_input) <pre>name='Joe User' email='joe.user@example.com' query='I bought a laptop carrying case and it turned out to be \\n             the wrong size. I need to return it.' order_id=12345 purchase_date=datetime.date(2025, 12, 31)\n</pre> In\u00a0[22]: Copied! <pre>from pydantic import BaseModel, Field, ValidationError, EmailStr\nfrom typing import List, Literal,Optional\nfrom datetime import date\nfrom dotenv import load_dotenv\nimport openai\nimport json\n</pre> from pydantic import BaseModel, Field, ValidationError, EmailStr from typing import List, Literal,Optional from datetime import date from dotenv import load_dotenv import openai import json  In\u00a0[24]: Copied! <pre>load_dotenv()\nclient = openai.OpenAI()\n</pre> load_dotenv() client = openai.OpenAI() In\u00a0[23]: Copied! <pre>user_input_json = '''\n{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_number\": 12345,\n    \"purchase_date\": null\n}\n'''\n</pre> user_input_json = ''' {     \"name\": \"Rohit Kumar\",     \"email\": \"rohitkumar@example.com\",     \"query\": \"I forgot my password.\",     \"order_number\": 12345,     \"purchase_date\": null } ''' In\u00a0[25]: Copied! <pre>user_input = UserInput.model_validate_json(user_input_json)\n</pre> user_input = UserInput.model_validate_json(user_input_json) In\u00a0[26]: Copied! <pre>class CustomerQuery(UserInput):\n    priority: str = Field(..., description=\"Priority level: low, medium, high\")\n    category: Literal['refund_request', 'information_request', 'other'] = Field(..., description=\"Query category\")\n    is_complaint: bool = Field(..., description=\"Whether this is a complaint\")\n    tags: List[str] = Field(..., description=\"Relevant keyword tags\")\n</pre>  class CustomerQuery(UserInput):     priority: str = Field(..., description=\"Priority level: low, medium, high\")     category: Literal['refund_request', 'information_request', 'other'] = Field(..., description=\"Query category\")     is_complaint: bool = Field(..., description=\"Whether this is a complaint\")     tags: List[str] = Field(..., description=\"Relevant keyword tags\") In\u00a0[27]: Copied! <pre># Create a prompt with generic example data to guide LLM.\nexample_response_structure = f\"\"\"{{\n    name=\"Example User\",\n    email=\"user@example.com\",\n    query=\"I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.\",\n    order_id=12345,\n    purchase_date=\"2025-12-31\",\n    priority=\"medium\",\n    category=\"refund_request\",\n    is_complaint=True,\n    tags=[\"monitor\", \"support\", \"exchange\"] \n}}\"\"\"\n</pre> # Create a prompt with generic example data to guide LLM. example_response_structure = f\"\"\"{{     name=\"Example User\",     email=\"user@example.com\",     query=\"I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.\",     order_id=12345,     purchase_date=\"2025-12-31\",     priority=\"medium\",     category=\"refund_request\",     is_complaint=True,     tags=[\"monitor\", \"support\", \"exchange\"]  }}\"\"\" In\u00a0[28]: Copied! <pre># Create prompt with user data and expected JSON structure\nprompt = f\"\"\"\nPlease analyze this user query\\n {user_input.model_dump_json(indent=2)}:\n\nReturn your analysis as a JSON object matching this exact structure \nand data types:\n{example_response_structure}\n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON object.\n\"\"\"\n\nprint(prompt)\n</pre> # Create prompt with user data and expected JSON structure prompt = f\"\"\" Please analyze this user query\\n {user_input.model_dump_json(indent=2)}:  Return your analysis as a JSON object matching this exact structure  and data types: {example_response_structure}  Respond ONLY with valid JSON. Do not include any explanations or  other text or formatting before or after the JSON object. \"\"\"  print(prompt) <pre>\nPlease analyze this user query\n {\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null\n}:\n\nReturn your analysis as a JSON object matching this exact structure \nand data types:\n{\n    name=\"Example User\",\n    email=\"user@example.com\",\n    query=\"I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.\",\n    order_id=12345,\n    purchase_date=\"2025-12-31\",\n    priority=\"medium\",\n    category=\"refund_request\",\n    is_complaint=True,\n    tags=[\"monitor\", \"support\", \"exchange\"] \n}\n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON object.\n\n</pre> In\u00a0[29]: Copied! <pre># Define a function to call the LLM\ndef call_llm(prompt, model=\"gpt-4o\"):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n</pre> # Define a function to call the LLM def call_llm(prompt, model=\"gpt-4o\"):     response = client.chat.completions.create(         model=model,         messages=[{\"role\": \"user\", \"content\": prompt}]     )     return response.choices[0].message.content In\u00a0[30]: Copied! <pre># Get response from LLM\nresponse_content = call_llm(prompt)\nprint(response_content)\n</pre> # Get response from LLM response_content = call_llm(prompt) print(response_content) <pre>```json\n{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_id\": null,\n    \"purchase_date\": null,\n    \"priority\": \"low\",\n    \"category\": \"account_issue\",\n    \"is_complaint\": False,\n    \"tags\": [\"password\", \"support\", \"account\"]\n}\n```\n</pre> In\u00a0[31]: Copied! <pre># Attempt to parse the response into CustomerQuery model\nvalid_data = CustomerQuery.model_validate_json(response_content)\n</pre> # Attempt to parse the response into CustomerQuery model valid_data = CustomerQuery.model_validate_json(response_content) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[31], line 2\n      1 # Attempt to parse the response into CustomerQuery model\n----&gt; 2 valid_data = CustomerQuery.model_validate_json(response_content)\n\nFile ~/Desktop/rokmr/notes/.venv/lib/python3.13/site-packages/pydantic/main.py:746, in BaseModel.model_validate_json(cls, json_data, strict, context, by_alias, by_name)\n    740 if by_alias is False and by_name is not True:\n    741     raise PydanticUserError(\n    742         'At least one of `by_alias` or `by_name` must be set to True.',\n    743         code='validate-by-alias-and-name-false',\n    744     )\n--&gt; 746 return cls.__pydantic_validator__.validate_json(\n    747     json_data, strict=strict, context=context, by_alias=by_alias, by_name=by_name\n    748 )\n\nValidationError: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid</pre> In\u00a0[33]: Copied! <pre># Define a function to validate an LLM response\ndef validate_with_model(data_model, llm_response):\n    try:\n        validated_data = data_model.model_validate_json(llm_response)\n        print(\"data validation successful!\")\n        print(validated_data.model_dump_json(indent=2))\n        return validated_data, None\n    except ValidationError as e:\n        print(f\"error validating data: {e}\")\n        error_message = (\n            f\"This response generated a validation error: {e}.\"\n        )\n        return None, error_message\n</pre> # Define a function to validate an LLM response def validate_with_model(data_model, llm_response):     try:         validated_data = data_model.model_validate_json(llm_response)         print(\"data validation successful!\")         print(validated_data.model_dump_json(indent=2))         return validated_data, None     except ValidationError as e:         print(f\"error validating data: {e}\")         error_message = (             f\"This response generated a validation error: {e}.\"         )         return None, error_message In\u00a0[34]: Copied! <pre># Test your validation function with the LLM response\nvalidated_data, validation_error = validate_with_model(\n    CustomerQuery, response_content\n)\n</pre> # Test your validation function with the LLM response validated_data, validation_error = validate_with_model(     CustomerQuery, response_content ) <pre>error validating data: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</pre> In\u00a0[35]: Copied! <pre># Define a function to create a retry prompt with error feedback\ndef create_retry_prompt(\n    original_prompt, original_response, error_message\n):\n    retry_prompt = f\"\"\"\nThis is a request to fix an error in the structure of an llm_response.\nHere is the original request:\n&lt;original_prompt&gt;\n{original_prompt}\n&lt;/original_prompt&gt;\n\nHere is the original llm_response:\n&lt;llm_response&gt;\n{original_response}\n&lt;/llm_response&gt;\n\nThis response generated an error: \n&lt;error_message&gt;\n{error_message}\n&lt;/error_message&gt;\n\nCompare the error message and the llm_response and identify what \nneeds to be fixed or removed\nin the llm_response to resolve this error. \n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON string.\n\"\"\"\n    return retry_prompt\n</pre> # Define a function to create a retry prompt with error feedback def create_retry_prompt(     original_prompt, original_response, error_message ):     retry_prompt = f\"\"\" This is a request to fix an error in the structure of an llm_response. Here is the original request:  {original_prompt}   Here is the original llm_response:  {original_response}   This response generated an error:   {error_message}   Compare the error message and the llm_response and identify what  needs to be fixed or removed in the llm_response to resolve this error.   Respond ONLY with valid JSON. Do not include any explanations or  other text or formatting before or after the JSON string. \"\"\"     return retry_prompt In\u00a0[36]: Copied! <pre># Create a retry prompt for validation errors\nvalidation_retry_prompt = create_retry_prompt(\n    original_prompt=prompt,\n    original_response=response_content,\n    error_message=validation_error\n)\n\nprint(validation_retry_prompt)\n</pre> # Create a retry prompt for validation errors validation_retry_prompt = create_retry_prompt(     original_prompt=prompt,     original_response=response_content,     error_message=validation_error )  print(validation_retry_prompt) <pre>\nThis is a request to fix an error in the structure of an llm_response.\nHere is the original request:\n&lt;original_prompt&gt;\n\nPlease analyze this user query\n {\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null\n}:\n\nReturn your analysis as a JSON object matching this exact structure \nand data types:\n{\n    name=\"Example User\",\n    email=\"user@example.com\",\n    query=\"I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.\",\n    order_id=12345,\n    purchase_date=\"2025-12-31\",\n    priority=\"medium\",\n    category=\"refund_request\",\n    is_complaint=True,\n    tags=[\"monitor\", \"support\", \"exchange\"] \n}\n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON object.\n\n&lt;/original_prompt&gt;\n\nHere is the original llm_response:\n&lt;llm_response&gt;\n```json\n{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_id\": null,\n    \"purchase_date\": null,\n    \"priority\": \"low\",\n    \"category\": \"account_issue\",\n    \"is_complaint\": False,\n    \"tags\": [\"password\", \"support\", \"account\"]\n}\n```\n&lt;/llm_response&gt;\n\nThis response generated an error: \n&lt;error_message&gt;\nThis response generated a validation error: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid.\n&lt;/error_message&gt;\n\nCompare the error message and the llm_response and identify what \nneeds to be fixed or removed\nin the llm_response to resolve this error. \n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON string.\n\n</pre> In\u00a0[37]: Copied! <pre># Call the LLM with the validation retry prompt\nvalidation_retry_response = call_llm(validation_retry_prompt)\nprint(validation_retry_response)\n</pre> # Call the LLM with the validation retry prompt validation_retry_response = call_llm(validation_retry_prompt) print(validation_retry_response) <pre>```json\n{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_id\": null,\n    \"purchase_date\": null,\n    \"priority\": \"low\",\n    \"category\": \"account_issue\",\n    \"is_complaint\": false,\n    \"tags\": [\"password\", \"support\", \"account\"]\n}\n```\n</pre> In\u00a0[38]: Copied! <pre># Attempt to validate retry response from LLM\nvalidated_data, validation_error = validate_with_model(\n    CustomerQuery, validation_retry_response\n)\n</pre> # Attempt to validate retry response from LLM validated_data, validation_error = validate_with_model(     CustomerQuery, validation_retry_response ) <pre>error validating data: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n</pre> In\u00a0[39]: Copied! <pre># Create a second retry prompt for validation errors\nsecond_validation_retry_prompt = create_retry_prompt(\n    original_prompt=validation_retry_prompt,\n    original_response=validation_retry_response,\n    error_message=validation_error\n)\n\nprint(second_validation_retry_prompt)\n</pre> # Create a second retry prompt for validation errors second_validation_retry_prompt = create_retry_prompt(     original_prompt=validation_retry_prompt,     original_response=validation_retry_response,     error_message=validation_error )  print(second_validation_retry_prompt) <pre>\nThis is a request to fix an error in the structure of an llm_response.\nHere is the original request:\n&lt;original_prompt&gt;\n\nThis is a request to fix an error in the structure of an llm_response.\nHere is the original request:\n&lt;original_prompt&gt;\n\nPlease analyze this user query\n {\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null\n}:\n\nReturn your analysis as a JSON object matching this exact structure \nand data types:\n{\n    name=\"Example User\",\n    email=\"user@example.com\",\n    query=\"I ordered a new computer monitor and it arrived with the screen cracked. I need to exchange it for a new one.\",\n    order_id=12345,\n    purchase_date=\"2025-12-31\",\n    priority=\"medium\",\n    category=\"refund_request\",\n    is_complaint=True,\n    tags=[\"monitor\", \"support\", \"exchange\"] \n}\n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON object.\n\n&lt;/original_prompt&gt;\n\nHere is the original llm_response:\n&lt;llm_response&gt;\n```json\n{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_id\": null,\n    \"purchase_date\": null,\n    \"priority\": \"low\",\n    \"category\": \"account_issue\",\n    \"is_complaint\": False,\n    \"tags\": [\"password\", \"support\", \"account\"]\n}\n```\n&lt;/llm_response&gt;\n\nThis response generated an error: \n&lt;error_message&gt;\nThis response generated a validation error: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid.\n&lt;/error_message&gt;\n\nCompare the error message and the llm_response and identify what \nneeds to be fixed or removed\nin the llm_response to resolve this error. \n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON string.\n\n&lt;/original_prompt&gt;\n\nHere is the original llm_response:\n&lt;llm_response&gt;\n```json\n{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_id\": null,\n    \"purchase_date\": null,\n    \"priority\": \"low\",\n    \"category\": \"account_issue\",\n    \"is_complaint\": false,\n    \"tags\": [\"password\", \"support\", \"account\"]\n}\n```\n&lt;/llm_response&gt;\n\nThis response generated an error: \n&lt;error_message&gt;\nThis response generated a validation error: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid.\n&lt;/error_message&gt;\n\nCompare the error message and the llm_response and identify what \nneeds to be fixed or removed\nin the llm_response to resolve this error. \n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON string.\n\n</pre> In\u00a0[40]: Copied! <pre># Call the LLM with the second validation retry prompt\nsecond_validation_retry_response = call_llm(\n    second_validation_retry_prompt\n)\nprint(second_validation_retry_response)\n</pre> # Call the LLM with the second validation retry prompt second_validation_retry_response = call_llm(     second_validation_retry_prompt ) print(second_validation_retry_response) <pre>{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_id\": null,\n    \"purchase_date\": null,\n    \"priority\": \"low\",\n    \"category\": \"account_issue\",\n    \"is_complaint\": false,\n    \"tags\": [\"password\", \"support\", \"account\"]\n}\n</pre> In\u00a0[41]: Copied! <pre># Define a function to automatically retry an LLM call multiple times\ndef validate_llm_response(\n    prompt, data_model, n_retry=5, model=\"gpt-4o\"\n):\n    # Initial LLM call\n    response_content = call_llm(prompt, model=model)\n    current_prompt = prompt\n\n    # Try to validate with the model\n    # attempt: 0=initial, 1=first retry, ...\n    for attempt in range(n_retry + 1):\n\n        validated_data, validation_error = validate_with_model(\n            data_model, response_content\n        )\n\n        if validation_error:\n            if attempt &lt; n_retry:\n                print(f\"retry {attempt} of {n_retry} failed, trying again...\")\n            else:\n                print(f\"Max retries reached. Last error: {validation_error}\")\n                return None, (\n                    f\"Max retries reached. Last error: {validation_error}\"\n                )\n\n            validation_retry_prompt = create_retry_prompt(\n                original_prompt=current_prompt,\n                original_response=response_content,\n                error_message=validation_error\n            )\n            response_content = call_llm(\n                validation_retry_prompt, model=model\n            )\n            current_prompt = validation_retry_prompt\n            continue\n\n        # If you get here, both parsing and validation succeeded\n        return validated_data, None\n</pre> # Define a function to automatically retry an LLM call multiple times def validate_llm_response(     prompt, data_model, n_retry=5, model=\"gpt-4o\" ):     # Initial LLM call     response_content = call_llm(prompt, model=model)     current_prompt = prompt      # Try to validate with the model     # attempt: 0=initial, 1=first retry, ...     for attempt in range(n_retry + 1):          validated_data, validation_error = validate_with_model(             data_model, response_content         )          if validation_error:             if attempt &lt; n_retry:                 print(f\"retry {attempt} of {n_retry} failed, trying again...\")             else:                 print(f\"Max retries reached. Last error: {validation_error}\")                 return None, (                     f\"Max retries reached. Last error: {validation_error}\"                 )              validation_retry_prompt = create_retry_prompt(                 original_prompt=current_prompt,                 original_response=response_content,                 error_message=validation_error             )             response_content = call_llm(                 validation_retry_prompt, model=model             )             current_prompt = validation_retry_prompt             continue          # If you get here, both parsing and validation succeeded         return validated_data, None In\u00a0[42]: Copied! <pre># Test your complete solution with the original prompt\nvalidated_data, error = validate_llm_response(\n    prompt, CustomerQuery\n)\n</pre> # Test your complete solution with the original prompt validated_data, error = validate_llm_response(     prompt, CustomerQuery ) <pre>error validating data: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nretry 0 of 5 failed, trying again...\nerror validating data: 1 validation error for CustomerQuery\ncategory\n  Input should be 'refund_request', 'information_request' or 'other' [type=literal_error, input_value='account_issue', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/literal_error\nretry 1 of 5 failed, trying again...\nerror validating data: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n    \"name\": ...rt\", \"account\"]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nretry 2 of 5 failed, trying again...\ndata validation successful!\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null,\n  \"priority\": \"low\",\n  \"category\": \"other\",\n  \"is_complaint\": false,\n  \"tags\": [\n    \"password\",\n    \"support\",\n    \"account\"\n  ]\n}\n</pre> In\u00a0[43]: Copied! <pre># Investigate the model_json_schema for CustomerQuery\ndata_model_schema = json.dumps(\n    CustomerQuery.model_json_schema(), indent=2\n)\nprint(data_model_schema)\n</pre> # Investigate the model_json_schema for CustomerQuery data_model_schema = json.dumps(     CustomerQuery.model_json_schema(), indent=2 ) print(data_model_schema) <pre>{\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"email\": {\n      \"format\": \"email\",\n      \"title\": \"Email\",\n      \"type\": \"string\"\n    },\n    \"query\": {\n      \"title\": \"Query\",\n      \"type\": \"string\"\n    },\n    \"order_id\": {\n      \"anyOf\": [\n        {\n          \"maximum\": 1000000,\n          \"minimum\": 1,\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Order ID\",\n      \"title\": \"Order Id\"\n    },\n    \"purchase_date\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Date of purchase transactions\",\n      \"title\": \"Purchase Date\"\n    },\n    \"priority\": {\n      \"description\": \"Priority level: low, medium, high\",\n      \"title\": \"Priority\",\n      \"type\": \"string\"\n    },\n    \"category\": {\n      \"description\": \"Query category\",\n      \"enum\": [\n        \"refund_request\",\n        \"information_request\",\n        \"other\"\n      ],\n      \"title\": \"Category\",\n      \"type\": \"string\"\n    },\n    \"is_complaint\": {\n      \"description\": \"Whether this is a complaint\",\n      \"title\": \"Is Complaint\",\n      \"type\": \"boolean\"\n    },\n    \"tags\": {\n      \"description\": \"Relevant keyword tags\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Tags\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"name\",\n    \"email\",\n    \"query\",\n    \"priority\",\n    \"category\",\n    \"is_complaint\",\n    \"tags\"\n  ],\n  \"title\": \"CustomerQuery\",\n  \"type\": \"object\"\n}\n</pre> In\u00a0[46]: Copied! <pre>user_input.model_dump_json(indent=2)\n</pre> user_input.model_dump_json(indent=2) Out[46]: <pre>'{\\n  \"name\": \"Rohit Kumar\",\\n  \"email\": \"rohitkumar@example.com\",\\n  \"query\": \"I forgot my password.\",\\n  \"order_id\": null,\\n  \"purchase_date\": null\\n}'</pre> In\u00a0[44]: Copied! <pre># model_json_schema() is has the rich information about the model\n# Create new prompt with user input and model_json_schema\nprompt = f\"\"\"\nPlease analyze this user query\\n {user_input.model_dump_json(indent=2)}:\n\nReturn your analysis as a JSON object matching the following schema:\n{data_model_schema}\n\nRespond ONLY with valid JSON. Do not include any explanations or \nother text or formatting before or after the JSON object.\n\"\"\"\n</pre> # model_json_schema() is has the rich information about the model # Create new prompt with user input and model_json_schema prompt = f\"\"\" Please analyze this user query\\n {user_input.model_dump_json(indent=2)}:  Return your analysis as a JSON object matching the following schema: {data_model_schema}  Respond ONLY with valid JSON. Do not include any explanations or  other text or formatting before or after the JSON object. \"\"\" In\u00a0[45]: Copied! <pre># Run your validate_llm_response function with the new prompt\nfinal_analysis, error = validate_llm_response(\n    prompt, CustomerQuery\n)\n</pre> # Run your validate_llm_response function with the new prompt final_analysis, error = validate_llm_response(     prompt, CustomerQuery ) <pre>error validating data: 1 validation error for CustomerQuery\n  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='```json\\n{\\n  \"name\": \"R...   \"login\"\\n  ]\\n}\\n```', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\nretry 0 of 5 failed, trying again...\ndata validation successful!\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null,\n  \"priority\": \"high\",\n  \"category\": \"information_request\",\n  \"is_complaint\": false,\n  \"tags\": [\n    \"password\",\n    \"account_access\",\n    \"login\"\n  ]\n}\n</pre> In\u00a0[48]: Copied! <pre>import instructor\nimport anthropic\n</pre> import instructor import anthropic In\u00a0[49]: Copied! <pre>print(user_input_json)\n</pre> print(user_input_json) <pre>\n{\n    \"name\": \"Rohit Kumar\",\n    \"email\": \"rohitkumar@example.com\",\n    \"query\": \"I forgot my password.\",\n    \"order_number\": 12345,\n    \"purchase_date\": null\n}\n\n</pre> In\u00a0[50]: Copied! <pre># Validate using UserInput model\n\nuser_input = UserInput.model_validate_json(user_input_json)\n\nprint(user_input)\n</pre> # Validate using UserInput model  user_input = UserInput.model_validate_json(user_input_json)  print(user_input) <pre>name='Rohit Kumar' email='rohitkumar@example.com' query='I forgot my password.' order_id=None purchase_date=None\n</pre> In\u00a0[56]: Copied! <pre>prompt = (\n    f\"Analyze the following customer query {user_input} \"\n    f\"and provide a structured response.\"\n)\n</pre> prompt = (     f\"Analyze the following customer query {user_input} \"     f\"and provide a structured response.\" ) In\u00a0[\u00a0]: Copied! <pre>load_dotenv()\n\n# Using instructor\nanthropic_client = instructor.from_anthropic(anthropic.Anthropic())\n\nresponse = anthropic_client.messages.create(\n    model=\"claude-3-7-sonnet-latest\",  \n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": prompt\n        }\n    ],\n    response_model=CustomerQuery  \n)\n</pre> load_dotenv()  # Using instructor anthropic_client = instructor.from_anthropic(anthropic.Anthropic())  response = anthropic_client.messages.create(     model=\"claude-3-7-sonnet-latest\",       max_tokens=1024,     messages=[         {             \"role\": \"user\",              \"content\": prompt         }     ],     response_model=CustomerQuery   ) In\u00a0[58]: Copied! <pre>print(type(response))\nprint(response.model_dump_json(indent=2))\n</pre> print(type(response)) print(response.model_dump_json(indent=2)) <pre>&lt;class '__main__.CustomerQuery'&gt;\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null,\n  \"priority\": \"medium\",\n  \"category\": \"information_request\",\n  \"is_complaint\": false,\n  \"tags\": [\n    \"password\",\n    \"account\",\n    \"login\"\n  ]\n}\n</pre> In\u00a0[60]: Copied! <pre># Without using instructor\nfrom openai import OpenAI\nopenai_client = OpenAI()\nresponse = openai_client.beta.chat.completions.parse(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    response_format=CustomerQuery\n)\nresponse_content = response.choices[0].message.content\nprint(type(response_content))\nprint(response_content)\n</pre> # Without using instructor from openai import OpenAI openai_client = OpenAI() response = openai_client.beta.chat.completions.parse(     model=\"gpt-4o\",     messages=[{\"role\": \"user\", \"content\": prompt}],     response_format=CustomerQuery ) response_content = response.choices[0].message.content print(type(response_content)) print(response_content) <pre>&lt;class 'str'&gt;\n{\"name\":\"Rohit Kumar\",\"email\":\"rohitkumar@example.com\",\"query\":\"I forgot my password.\",\"order_id\":null,\"purchase_date\":null,\"priority\":\"medium\",\"category\":\"information_request\",\"is_complaint\":false,\"tags\":[\"account\",\"password\",\"login\"]}\n</pre> In\u00a0[61]: Copied! <pre># Above is valid json but not necessary datamodel\n</pre> # Above is valid json but not necessary datamodel In\u00a0[62]: Copied! <pre># Validate the repsonse you got from the LLM\nvalid_data = CustomerQuery.model_validate_json(\n    response_content\n)\nprint(type(valid_data))\nprint(valid_data.model_dump_json(indent=2))\n</pre> # Validate the repsonse you got from the LLM valid_data = CustomerQuery.model_validate_json(     response_content ) print(type(valid_data)) print(valid_data.model_dump_json(indent=2)) <pre>&lt;class '__main__.CustomerQuery'&gt;\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null,\n  \"priority\": \"medium\",\n  \"category\": \"information_request\",\n  \"is_complaint\": false,\n  \"tags\": [\n    \"account\",\n    \"password\",\n    \"login\"\n  ]\n}\n</pre> In\u00a0[63]: Copied! <pre># Try the responses API from OpenAI\nresponse = openai_client.responses.parse(\n    model=\"gpt-4o\",\n    input=[{\"role\": \"user\", \"content\": prompt}],\n    text_format=CustomerQuery\n)\n\nprint(type(response))\n</pre> # Try the responses API from OpenAI response = openai_client.responses.parse(     model=\"gpt-4o\",     input=[{\"role\": \"user\", \"content\": prompt}],     text_format=CustomerQuery )  print(type(response)) <pre>&lt;class 'openai.types.responses.parsed_response.ParsedResponse[CustomerQuery]'&gt;\n</pre> In\u00a0[64]: Copied! <pre># Investigate class inheritance structure of the OpenAI response\ndef print_class_inheritence(llm_response):\n    for cls in type(llm_response).mro():\n        print(f\"{cls.__module__}.{cls.__name__}\")\n\nprint_class_inheritence(response)\n</pre> # Investigate class inheritance structure of the OpenAI response def print_class_inheritence(llm_response):     for cls in type(llm_response).mro():         print(f\"{cls.__module__}.{cls.__name__}\")  print_class_inheritence(response) <pre>openai.types.responses.parsed_response.ParsedResponse[CustomerQuery]\nopenai.types.responses.parsed_response.ParsedResponse\nopenai.types.responses.response.Response\nopenai._models.GenericModel\nopenai._compat.GenericModel\nopenai.BaseModel\npydantic.main.BaseModel\ntyping.Generic\nbuiltins.object\n</pre> In\u00a0[65]: Copied! <pre># Print the response type and content \nprint(type(response.output_parsed))\nprint(response.output_parsed.model_dump_json(indent=2))\n</pre> # Print the response type and content  print(type(response.output_parsed)) print(response.output_parsed.model_dump_json(indent=2)) <pre>&lt;class '__main__.CustomerQuery'&gt;\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null,\n  \"priority\": \"medium\",\n  \"category\": \"information_request\",\n  \"is_complaint\": false,\n  \"tags\": [\n    \"password\",\n    \"account_access\",\n    \"login_issue\"\n  ]\n}\n</pre> In\u00a0[70]: Copied! <pre># Try out the Pydantic AI package for defining an agent and getting a structured response\nfrom pydantic_ai import Agent\nimport nest_asyncio\nnest_asyncio.apply()\n\nagent = Agent(\n    model=\"google-gla:gemini-2.0-flash\",\n    output_type=CustomerQuery,\n)\n\nresponse = agent.run_sync(prompt)\n</pre> # Try out the Pydantic AI package for defining an agent and getting a structured response from pydantic_ai import Agent import nest_asyncio nest_asyncio.apply()  agent = Agent(     model=\"google-gla:gemini-2.0-flash\",     output_type=CustomerQuery, )  response = agent.run_sync(prompt) In\u00a0[71]: Copied! <pre># Print out the repsonse type and content\nprint(type(response.output))\nprint(response.output.model_dump_json(indent=2))\n</pre> # Print out the repsonse type and content print(type(response.output)) print(response.output.model_dump_json(indent=2)) <pre>&lt;class '__main__.CustomerQuery'&gt;\n{\n  \"name\": \"Rohit Kumar\",\n  \"email\": \"rohitkumar@example.com\",\n  \"query\": \"I forgot my password.\",\n  \"order_id\": null,\n  \"purchase_date\": null,\n  \"priority\": \"low\",\n  \"category\": \"information_request\",\n  \"is_complaint\": false,\n  \"tags\": [\n    \"password reset\"\n  ]\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/pydantic/#pydantic-basics","title":"Pydantic Basics\u00b6","text":""},{"location":"notebooks/pydantic/#prompting-for-structured-output-and-setting-up-retrying-logic","title":"Prompting for structured output and setting up retrying logic\u00b6","text":""},{"location":"notebooks/pydantic/#using-pydantic-models-for-structured-llm-output","title":"Using Pydantic Models for Structured LLM Output\u00b6","text":""},{"location":"notebooks/ragagent/","title":"Preprocess Documents","text":"In\u00a0[1]: Copied! <pre>%%capture --no-stderr\n%pip install -U --quiet langgraph \"langchain[openai]\" langchain-community langchain-text-splitters\n</pre> %%capture --no-stderr %pip install -U --quiet langgraph \"langchain[openai]\" langchain-community langchain-text-splitters In\u00a0[1]: Copied! <pre>import getpass\nimport os\n\n\ndef _set_env(key: str):\n    if key not in os.environ:\n        os.environ[key] = getpass.getpass(f\"{key}:\")\n\n\n_set_env(\"OPENAI_API_KEY\")\n</pre> import getpass import os   def _set_env(key: str):     if key not in os.environ:         os.environ[key] = getpass.getpass(f\"{key}:\")   _set_env(\"OPENAI_API_KEY\") In\u00a0[11]: Copied! <pre># !pip install beautifulsoup4\n</pre> # !pip install beautifulsoup4 In\u00a0[2]: Copied! <pre>from langchain_community.document_loaders import WebBaseLoader\n\nurls = [\n    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls]\n</pre> from langchain_community.document_loaders import WebBaseLoader  urls = [     \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",     \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",     \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\", ]  docs = [WebBaseLoader(url).load() for url in urls] <pre>/Users/rohitkumar/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\n</pre> In\u00a0[3]: Copied! <pre>docs[0][0].page_content.strip()[:1000]\n</pre> docs[0][0].page_content.strip()[:1000] Out[3]: <pre>\"Reward Hacking in Reinforcement Learning | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Reward Hacking in Reinforcement Learning\\n    \\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBackground\\n\\nReward Function in RL\\n\\nSpurious Correlation\\n\\n\\nLet\u2019s Define Reward Hacking\\n\\nList of Examples\\n\\nReward hacking examples in RL tasks\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nData Analysis of RLHF\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to ac\"</pre> In\u00a0[4]: Copied! <pre>from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=50\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n</pre> from langchain_text_splitters import RecursiveCharacterTextSplitter  docs_list = [item for sublist in docs for item in sublist]  text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(     chunk_size=100, chunk_overlap=50 ) doc_splits = text_splitter.split_documents(docs_list) In\u00a0[5]: Copied! <pre>doc_splits[0].page_content.strip()\n</pre> doc_splits[0].page_content.strip() Out[5]: <pre>\"Reward Hacking in Reinforcement Learning | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\"</pre> In\u00a0[6]: Copied! <pre>from langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvectorstore = InMemoryVectorStore.from_documents(\n    documents=doc_splits, embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n</pre> from langchain_core.vectorstores import InMemoryVectorStore from langchain_openai import OpenAIEmbeddings  vectorstore = InMemoryVectorStore.from_documents(     documents=doc_splits, embedding=OpenAIEmbeddings() ) retriever = vectorstore.as_retriever() In\u00a0[7]: Copied! <pre>from langchain.tools.retriever import create_retriever_tool\n\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"retrieve_blog_posts\",\n    \"Search and return information about Lilian Weng blog posts.\",\n)\n</pre> from langchain.tools.retriever import create_retriever_tool  retriever_tool = create_retriever_tool(     retriever,     \"retrieve_blog_posts\",     \"Search and return information about Lilian Weng blog posts.\", ) In\u00a0[8]: Copied! <pre>retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n</pre> retriever_tool.invoke({\"query\": \"types of reward hacking\"}) Out[8]: <pre>'Detecting Reward Hacking#\\n\\nIn-Context Reward Hacking#\\n\\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\\n\\nWhy does Reward Hacking Exist?#'</pre> In\u00a0[9]: Copied! <pre>from langgraph.graph import MessagesState\nfrom langchain.chat_models import init_chat_model\n\nresponse_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n\n\ndef generate_query_or_respond(state: MessagesState):\n    \"\"\"Call the model to generate a response based on the current state. Given\n    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n    \"\"\"\n    response = (\n        response_model\n        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n    )\n    return {\"messages\": [response]}\n</pre> from langgraph.graph import MessagesState from langchain.chat_models import init_chat_model  response_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)   def generate_query_or_respond(state: MessagesState):     \"\"\"Call the model to generate a response based on the current state. Given     the question, it will decide to retrieve using the retriever tool, or simply respond to the user.     \"\"\"     response = (         response_model         .bind_tools([retriever_tool]).invoke(state[\"messages\"])     )     return {\"messages\": [response]} In\u00a0[10]: Copied! <pre>input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\ngenerate_query_or_respond(input)[\"messages\"][-1].pretty_print()\n</pre> input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]} generate_query_or_respond(input)[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\n\nHello! How can I help you today?\n</pre> In\u00a0[11]: Copied! <pre>input = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n        }\n    ]\n}\ngenerate_query_or_respond(input)[\"messages\"][-1].pretty_print()\n</pre> input = {     \"messages\": [         {             \"role\": \"user\",             \"content\": \"What does Lilian Weng say about types of reward hacking?\",         }     ] } generate_query_or_respond(input)[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\nTool Calls:\n  retrieve_blog_posts (call_l53UTwlVnc3QrI7JDKw6lrfH)\n Call ID: call_l53UTwlVnc3QrI7JDKw6lrfH\n  Args:\n    query: types of reward hacking\n</pre> In\u00a0[12]: Copied! <pre>from pydantic import BaseModel, Field\nfrom typing import Literal\n\nGRADE_PROMPT = (\n    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n    \"Here is the user question: {question} \\n\"\n    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n)\n\n\nclass GradeDocuments(BaseModel):\n    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n    )\n\n\ngrader_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n\n\ndef grade_documents(\n    state: MessagesState,\n) -&gt; Literal[\"generate_answer\", \"rewrite_question\"]:\n    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n    question = state[\"messages\"][0].content\n    context = state[\"messages\"][-1].content\n\n    prompt = GRADE_PROMPT.format(question=question, context=context)\n    response = (\n        grader_model\n        .with_structured_output(GradeDocuments).invoke(\n            [{\"role\": \"user\", \"content\": prompt}]\n        )\n    )\n    score = response.binary_score\n\n    if score == \"yes\":\n        return \"generate_answer\"\n    else:\n        return \"rewrite_question\"\n</pre> from pydantic import BaseModel, Field from typing import Literal  GRADE_PROMPT = (     \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"     \"Here is the retrieved document: \\n\\n {context} \\n\\n\"     \"Here is the user question: {question} \\n\"     \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"     \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\" )   class GradeDocuments(BaseModel):     \"\"\"Grade documents using a binary score for relevance check.\"\"\"      binary_score: str = Field(         description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"     )   grader_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)   def grade_documents(     state: MessagesState, ) -&gt; Literal[\"generate_answer\", \"rewrite_question\"]:     \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"     question = state[\"messages\"][0].content     context = state[\"messages\"][-1].content      prompt = GRADE_PROMPT.format(question=question, context=context)     response = (         grader_model         .with_structured_output(GradeDocuments).invoke(             [{\"role\": \"user\", \"content\": prompt}]         )     )     score = response.binary_score      if score == \"yes\":         return \"generate_answer\"     else:         return \"rewrite_question\" In\u00a0[13]: Copied! <pre>from langchain_core.messages import convert_to_messages\n\ninput = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n        ]\n    )\n}\ngrade_documents(input)\n</pre> from langchain_core.messages import convert_to_messages  input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": \"What does Lilian Weng say about types of reward hacking?\",             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_blog_posts\",                         \"args\": {\"query\": \"types of reward hacking\"},                     }                 ],             },             {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},         ]     ) } grade_documents(input) Out[13]: <pre>'rewrite_question'</pre> In\u00a0[14]: Copied! <pre>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n                \"tool_call_id\": \"1\",\n            },\n        ]\n    )\n}\ngrade_documents(input)\n</pre> input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": \"What does Lilian Weng say about types of reward hacking?\",             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_blog_posts\",                         \"args\": {\"query\": \"types of reward hacking\"},                     }                 ],             },             {                 \"role\": \"tool\",                 \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",                 \"tool_call_id\": \"1\",             },         ]     ) } grade_documents(input) Out[14]: <pre>'rewrite_question'</pre> In\u00a0[15]: Copied! <pre>REWRITE_PROMPT = (\n    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n    \"Here is the initial question:\"\n    \"\\n ------- \\n\"\n    \"{question}\"\n    \"\\n ------- \\n\"\n    \"Formulate an improved question:\"\n)\n\n\ndef rewrite_question(state: MessagesState):\n    \"\"\"Rewrite the original user question.\"\"\"\n    messages = state[\"messages\"]\n    question = messages[0].content\n    prompt = REWRITE_PROMPT.format(question=question)\n    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}\n</pre> REWRITE_PROMPT = (     \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"     \"Here is the initial question:\"     \"\\n ------- \\n\"     \"{question}\"     \"\\n ------- \\n\"     \"Formulate an improved question:\" )   def rewrite_question(state: MessagesState):     \"\"\"Rewrite the original user question.\"\"\"     messages = state[\"messages\"]     question = messages[0].content     prompt = REWRITE_PROMPT.format(question=question)     response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])     return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]} In\u00a0[16]: Copied! <pre>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n        ]\n    )\n}\n\nresponse = rewrite_question(input)\nprint(response[\"messages\"][-1][\"content\"])\n</pre> input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": \"What does Lilian Weng say about types of reward hacking?\",             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_blog_posts\",                         \"args\": {\"query\": \"types of reward hacking\"},                     }                 ],             },             {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},         ]     ) }  response = rewrite_question(input) print(response[\"messages\"][-1][\"content\"]) <pre>Certainly! Here\u2019s an improved version of the question:\n\nWhat are the different types of reward hacking described by Lilian Weng, and how does she characterize them?\n</pre> In\u00a0[17]: Copied! <pre>GENERATE_PROMPT = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer the question. \"\n    \"If you don't know the answer, just say that you don't know. \"\n    \"Use three sentences maximum and keep the answer concise.\\n\"\n    \"Question: {question} \\n\"\n    \"Context: {context}\"\n)\n\n\ndef generate_answer(state: MessagesState):\n    \"\"\"Generate an answer.\"\"\"\n    question = state[\"messages\"][0].content\n    context = state[\"messages\"][-1].content\n    prompt = GENERATE_PROMPT.format(question=question, context=context)\n    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n    return {\"messages\": [response]}\n</pre> GENERATE_PROMPT = (     \"You are an assistant for question-answering tasks. \"     \"Use the following pieces of retrieved context to answer the question. \"     \"If you don't know the answer, just say that you don't know. \"     \"Use three sentences maximum and keep the answer concise.\\n\"     \"Question: {question} \\n\"     \"Context: {context}\" )   def generate_answer(state: MessagesState):     \"\"\"Generate an answer.\"\"\"     question = state[\"messages\"][0].content     context = state[\"messages\"][-1].content     prompt = GENERATE_PROMPT.format(question=question, context=context)     response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])     return {\"messages\": [response]} In\u00a0[18]: Copied! <pre>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n                \"tool_call_id\": \"1\",\n            },\n        ]\n    )\n}\n\nresponse = generate_answer(input)\nresponse[\"messages\"][-1].pretty_print()\n</pre> input = {     \"messages\": convert_to_messages(         [             {                 \"role\": \"user\",                 \"content\": \"What does Lilian Weng say about types of reward hacking?\",             },             {                 \"role\": \"assistant\",                 \"content\": \"\",                 \"tool_calls\": [                     {                         \"id\": \"1\",                         \"name\": \"retrieve_blog_posts\",                         \"args\": {\"query\": \"types of reward hacking\"},                     }                 ],             },             {                 \"role\": \"tool\",                 \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",                 \"tool_call_id\": \"1\",             },         ]     ) }  response = generate_answer(input) response[\"messages\"][-1].pretty_print() <pre>================================== Ai Message ==================================\n\nLilian Weng says that reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering. These categories describe different ways in which an agent might exploit flaws in the reward system. Environment or goal misspecification involves unintended behaviors due to poorly specified objectives, while reward tampering involves the agent manipulating the reward signal itself.\n</pre> In\u00a0[19]: Copied! <pre>from langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.prebuilt import tools_condition\n\nworkflow = StateGraph(MessagesState)\n\n# Define the nodes we will cycle between\nworkflow.add_node(generate_query_or_respond)\nworkflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\nworkflow.add_node(rewrite_question)\nworkflow.add_node(generate_answer)\n\nworkflow.add_edge(START, \"generate_query_or_respond\")\n\n# Decide whether to retrieve\nworkflow.add_conditional_edges(\n    \"generate_query_or_respond\",\n    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n    tools_condition,\n    {\n        # Translate the condition outputs to nodes in our graph\n        \"tools\": \"retrieve\",\n        END: END,\n    },\n)\n\n# Edges taken after the `action` node is called.\nworkflow.add_conditional_edges(\n    \"retrieve\",\n    # Assess agent decision\n    grade_documents,\n)\nworkflow.add_edge(\"generate_answer\", END)\nworkflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n\n# Compile\ngraph = workflow.compile()\n</pre> from langgraph.graph import StateGraph, START, END from langgraph.prebuilt import ToolNode from langgraph.prebuilt import tools_condition  workflow = StateGraph(MessagesState)  # Define the nodes we will cycle between workflow.add_node(generate_query_or_respond) workflow.add_node(\"retrieve\", ToolNode([retriever_tool])) workflow.add_node(rewrite_question) workflow.add_node(generate_answer)  workflow.add_edge(START, \"generate_query_or_respond\")  # Decide whether to retrieve workflow.add_conditional_edges(     \"generate_query_or_respond\",     # Assess LLM decision (call `retriever_tool` tool or respond to the user)     tools_condition,     {         # Translate the condition outputs to nodes in our graph         \"tools\": \"retrieve\",         END: END,     }, )  # Edges taken after the `action` node is called. workflow.add_conditional_edges(     \"retrieve\",     # Assess agent decision     grade_documents, ) workflow.add_edge(\"generate_answer\", END) workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")  # Compile graph = workflow.compile() In\u00a0[20]: Copied! <pre>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</pre> from IPython.display import Image, display  display(Image(graph.get_graph().draw_mermaid_png())) In\u00a0[21]: Copied! <pre>for chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            }\n        ]\n    }\n):\n    for node, update in chunk.items():\n        print(\"Update from node\", node)\n        update[\"messages\"][-1].pretty_print()\n        print(\"\\n\\n\")\n</pre> for chunk in graph.stream(     {         \"messages\": [             {                 \"role\": \"user\",                 \"content\": \"What does Lilian Weng say about types of reward hacking?\",             }         ]     } ):     for node, update in chunk.items():         print(\"Update from node\", node)         update[\"messages\"][-1].pretty_print()         print(\"\\n\\n\") <pre>Update from node generate_query_or_respond\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_blog_posts (call_9iSTtBymiBSlrsyA7pFvNRH4)\n Call ID: call_9iSTtBymiBSlrsyA7pFvNRH4\n  Args:\n    query: types of reward hacking\n\n\n\nUpdate from node retrieve\n================================= Tool Message =================================\nName: retrieve_blog_posts\n\nDetecting Reward Hacking#\n\nIn-Context Reward Hacking#\n\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n\nWhy does Reward Hacking Exist?#\n\n\n\nUpdate from node generate_answer\n================================== Ai Message ==================================\n\nLilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both categories. Some work defines reward tampering separately, but Weng includes it under the umbrella of reward hacking.\n\n\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/ragagent/#preprocess-documents","title":"Preprocess Documents\u00b6","text":""},{"location":"notebooks/ragagent/#create-a-retrieval-tools","title":"Create a Retrieval Tools\u00b6","text":""},{"location":"notebooks/ragagent/#generate-query","title":"Generate query\u00b6","text":""},{"location":"notebooks/ragagent/#grade-documents","title":"Grade Documents\u00b6","text":""}]}